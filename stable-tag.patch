diff --git a/codex-rs/Cargo.lock b/codex-rs/Cargo.lock
index 64c83b0f..61a6e37f 100644
--- a/codex-rs/Cargo.lock
+++ b/codex-rs/Cargo.lock
@@ -178,7 +178,7 @@ checksum = "b0674a1ddeecb70197781e945de4b3b8ffb61fa939a5597bcf48503737663100"
 
 [[package]]
 name = "app_test_support"
-version = "0.0.0"
+version = "0.53.0"
 dependencies = [
  "anyhow",
  "assert_cmd",
@@ -822,7 +822,7 @@ checksum = "e9b18233253483ce2f65329a24072ec414db782531bdbb7d0bbc4bd2ce6b7e21"
 
 [[package]]
 name = "codex-ansi-escape"
-version = "0.0.0"
+version = "0.53.0"
 dependencies = [
  "ansi-to-tui",
  "ratatui",
@@ -831,7 +831,7 @@ dependencies = [
 
 [[package]]
 name = "codex-app-server"
-version = "0.0.0"
+version = "0.53.0"
 dependencies = [
  "anyhow",
  "app_test_support",
@@ -866,7 +866,7 @@ dependencies = [
 
 [[package]]
 name = "codex-app-server-protocol"
-version = "0.0.0"
+version = "0.53.0"
 dependencies = [
  "anyhow",
  "clap",
@@ -883,7 +883,7 @@ dependencies = [
 
 [[package]]
 name = "codex-apply-patch"
-version = "0.0.0"
+version = "0.53.0"
 dependencies = [
  "anyhow",
  "assert_cmd",
@@ -898,7 +898,7 @@ dependencies = [
 
 [[package]]
 name = "codex-arg0"
-version = "0.0.0"
+version = "0.53.0"
 dependencies = [
  "anyhow",
  "codex-apply-patch",
@@ -911,7 +911,7 @@ dependencies = [
 
 [[package]]
 name = "codex-async-utils"
-version = "0.0.0"
+version = "0.53.0"
 dependencies = [
  "async-trait",
  "pretty_assertions",
@@ -935,7 +935,7 @@ dependencies = [
 
 [[package]]
 name = "codex-backend-openapi-models"
-version = "0.0.0"
+version = "0.53.0"
 dependencies = [
  "serde",
  "serde_json",
@@ -943,8 +943,12 @@ dependencies = [
 ]
 
 [[package]]
-name = "codex-chatgpt"
+name = "codex-build-info"
 version = "0.0.0"
+
+[[package]]
+name = "codex-chatgpt"
+version = "0.53.0"
 dependencies = [
  "anyhow",
  "clap",
@@ -959,7 +963,7 @@ dependencies = [
 
 [[package]]
 name = "codex-cli"
-version = "0.0.0"
+version = "0.53.0"
 dependencies = [
  "anyhow",
  "assert_cmd",
@@ -996,7 +1000,7 @@ dependencies = [
 
 [[package]]
 name = "codex-cloud-tasks"
-version = "0.0.0"
+version = "0.53.0"
 dependencies = [
  "anyhow",
  "async-trait",
@@ -1022,7 +1026,7 @@ dependencies = [
 
 [[package]]
 name = "codex-cloud-tasks-client"
-version = "0.0.0"
+version = "0.53.0"
 dependencies = [
  "anyhow",
  "async-trait",
@@ -1037,19 +1041,22 @@ dependencies = [
 
 [[package]]
 name = "codex-common"
-version = "0.0.0"
+version = "0.53.0"
 dependencies = [
  "clap",
  "codex-app-server-protocol",
+ "codex-build-info",
  "codex-core",
  "codex-protocol",
+ "reqwest",
  "serde",
+ "serde_json",
  "toml",
 ]
 
 [[package]]
 name = "codex-core"
-version = "0.0.0"
+version = "0.53.0"
 dependencies = [
  "anyhow",
  "askama",
@@ -1128,7 +1135,7 @@ dependencies = [
 
 [[package]]
 name = "codex-exec"
-version = "0.0.0"
+version = "0.53.0"
 dependencies = [
  "anyhow",
  "assert_cmd",
@@ -1161,7 +1168,7 @@ dependencies = [
 
 [[package]]
 name = "codex-execpolicy"
-version = "0.0.0"
+version = "0.53.0"
 dependencies = [
  "allocative",
  "anyhow",
@@ -1181,7 +1188,7 @@ dependencies = [
 
 [[package]]
 name = "codex-feedback"
-version = "0.0.0"
+version = "0.53.0"
 dependencies = [
  "anyhow",
  "codex-protocol",
@@ -1192,7 +1199,7 @@ dependencies = [
 
 [[package]]
 name = "codex-file-search"
-version = "0.0.0"
+version = "0.53.0"
 dependencies = [
  "anyhow",
  "clap",
@@ -1205,7 +1212,7 @@ dependencies = [
 
 [[package]]
 name = "codex-git"
-version = "0.0.0"
+version = "0.53.0"
 dependencies = [
  "assert_matches",
  "once_cell",
@@ -1221,7 +1228,7 @@ dependencies = [
 
 [[package]]
 name = "codex-keyring-store"
-version = "0.0.0"
+version = "0.53.0"
 dependencies = [
  "keyring",
  "tracing",
@@ -1229,7 +1236,7 @@ dependencies = [
 
 [[package]]
 name = "codex-linux-sandbox"
-version = "0.0.0"
+version = "0.53.0"
 dependencies = [
  "clap",
  "codex-core",
@@ -1240,9 +1247,19 @@ dependencies = [
  "tokio",
 ]
 
+[[package]]
+name = "codex-litellm-model-session-telemetry"
+version = "0.1.0"
+dependencies = [
+ "chrono",
+ "once_cell",
+ "parking_lot",
+ "serde",
+]
+
 [[package]]
 name = "codex-login"
-version = "0.0.0"
+version = "0.53.0"
 dependencies = [
  "anyhow",
  "base64",
@@ -1266,7 +1283,7 @@ dependencies = [
 
 [[package]]
 name = "codex-mcp-server"
-version = "0.0.0"
+version = "0.53.0"
 dependencies = [
  "anyhow",
  "assert_cmd",
@@ -1293,7 +1310,7 @@ dependencies = [
 
 [[package]]
 name = "codex-ollama"
-version = "0.0.0"
+version = "0.53.0"
 dependencies = [
  "assert_matches",
  "async-stream",
@@ -1309,7 +1326,7 @@ dependencies = [
 
 [[package]]
 name = "codex-otel"
-version = "0.0.0"
+version = "0.53.0"
 dependencies = [
  "chrono",
  "codex-app-server-protocol",
@@ -1330,14 +1347,14 @@ dependencies = [
 
 [[package]]
 name = "codex-process-hardening"
-version = "0.0.0"
+version = "0.53.0"
 dependencies = [
  "libc",
 ]
 
 [[package]]
 name = "codex-protocol"
-version = "0.0.0"
+version = "0.53.0"
 dependencies = [
  "anyhow",
  "base64",
@@ -1363,7 +1380,7 @@ dependencies = [
 
 [[package]]
 name = "codex-protocol-ts"
-version = "0.0.0"
+version = "0.53.0"
 dependencies = [
  "anyhow",
  "clap",
@@ -1373,7 +1390,7 @@ dependencies = [
 
 [[package]]
 name = "codex-responses-api-proxy"
-version = "0.0.0"
+version = "0.53.0"
 dependencies = [
  "anyhow",
  "clap",
@@ -1389,7 +1406,7 @@ dependencies = [
 
 [[package]]
 name = "codex-rmcp-client"
-version = "0.0.0"
+version = "0.53.0"
 dependencies = [
  "anyhow",
  "axum",
@@ -1418,7 +1435,7 @@ dependencies = [
 
 [[package]]
 name = "codex-stdio-to-uds"
-version = "0.0.0"
+version = "0.53.0"
 dependencies = [
  "anyhow",
  "assert_cmd",
@@ -1429,7 +1446,7 @@ dependencies = [
 
 [[package]]
 name = "codex-tui"
-version = "0.0.0"
+version = "0.53.0"
 dependencies = [
  "anyhow",
  "arboard",
@@ -1445,6 +1462,7 @@ dependencies = [
  "codex-core",
  "codex-feedback",
  "codex-file-search",
+ "codex-litellm-model-session-telemetry",
  "codex-login",
  "codex-ollama",
  "codex-protocol",
@@ -1491,7 +1509,7 @@ dependencies = [
 
 [[package]]
 name = "codex-utils-cache"
-version = "0.0.0"
+version = "0.53.0"
 dependencies = [
  "lru",
  "sha1",
@@ -1500,7 +1518,7 @@ dependencies = [
 
 [[package]]
 name = "codex-utils-image"
-version = "0.0.0"
+version = "0.53.0"
 dependencies = [
  "base64",
  "codex-utils-cache",
@@ -1512,7 +1530,7 @@ dependencies = [
 
 [[package]]
 name = "codex-utils-json-to-toml"
-version = "0.0.0"
+version = "0.53.0"
 dependencies = [
  "pretty_assertions",
  "serde_json",
@@ -1521,7 +1539,7 @@ dependencies = [
 
 [[package]]
 name = "codex-utils-pty"
-version = "0.0.0"
+version = "0.53.0"
 dependencies = [
  "anyhow",
  "portable-pty",
@@ -1530,7 +1548,7 @@ dependencies = [
 
 [[package]]
 name = "codex-utils-readiness"
-version = "0.0.0"
+version = "0.53.0"
 dependencies = [
  "assert_matches",
  "async-trait",
@@ -1541,11 +1559,11 @@ dependencies = [
 
 [[package]]
 name = "codex-utils-string"
-version = "0.0.0"
+version = "0.53.0"
 
 [[package]]
 name = "codex-utils-tokenizer"
-version = "0.0.0"
+version = "0.53.0"
 dependencies = [
  "anyhow",
  "pretty_assertions",
@@ -1680,7 +1698,7 @@ checksum = "773648b94d0e5d620f64f280777445740e61fe701025087ec8b57f45c791888b"
 
 [[package]]
 name = "core_test_support"
-version = "0.0.0"
+version = "0.53.0"
 dependencies = [
  "anyhow",
  "assert_cmd",
@@ -3653,7 +3671,7 @@ checksum = "47e1ffaa40ddd1f3ed91f717a33c8c0ee23fff369e3aa8772b9605cc1d22f4c3"
 
 [[package]]
 name = "mcp-types"
-version = "0.0.0"
+version = "0.53.0"
 dependencies = [
  "schemars 0.8.22",
  "serde",
@@ -3663,7 +3681,7 @@ dependencies = [
 
 [[package]]
 name = "mcp_test_support"
-version = "0.0.0"
+version = "0.53.0"
 dependencies = [
  "anyhow",
  "assert_cmd",
diff --git a/codex-rs/Cargo.toml b/codex-rs/Cargo.toml
index 06fe7cba..eb1c6147 100644
--- a/codex-rs/Cargo.toml
+++ b/codex-rs/Cargo.toml
@@ -2,6 +2,8 @@
 members = [
     "backend-client",
     "ansi-escape",
+    "build-info",
+    "codex-litellm-model-session-telemetry",
     "async-utils",
     "app-server",
     "app-server-protocol",
@@ -60,6 +62,8 @@ codex-apply-patch = { path = "apply-patch" }
 codex-arg0 = { path = "arg0" }
 codex-async-utils = { path = "async-utils" }
 codex-backend-client = { path = "backend-client" }
+codex-build-info = { path = "build-info" }
+codex-litellm-model-session-telemetry = { path = "codex-litellm-model-session-telemetry" }
 codex-chatgpt = { path = "chatgpt" }
 codex-common = { path = "common" }
 codex-core = { path = "core" }
diff --git a/codex-rs/app-server/src/models.rs b/codex-rs/app-server/src/models.rs
index be47cbc2..36d3f09f 100644
--- a/codex-rs/app-server/src/models.rs
+++ b/codex-rs/app-server/src/models.rs
@@ -16,7 +16,7 @@ fn model_from_preset(preset: ModelPreset) -> Model {
         id: preset.id.to_string(),
         model: preset.model.to_string(),
         display_name: preset.display_name.to_string(),
-        description: preset.description.to_string(),
+        description: preset.description.clone().unwrap_or_default(),
         supported_reasoning_efforts: reasoning_efforts_from_preset(
             preset.supported_reasoning_efforts,
         ),
diff --git a/codex-rs/build-info/Cargo.toml b/codex-rs/build-info/Cargo.toml
new file mode 100644
index 00000000..3ba70fe6
--- /dev/null
+++ b/codex-rs/build-info/Cargo.toml
@@ -0,0 +1,6 @@
+[package]
+name = "codex-build-info"
+version = "0.0.0"
+edition = "2021"
+
+[dependencies]
diff --git a/codex-rs/build-info/build.rs b/codex-rs/build-info/build.rs
new file mode 100644
index 00000000..5766b67b
--- /dev/null
+++ b/codex-rs/build-info/build.rs
@@ -0,0 +1,27 @@
+use std::process::Command;
+
+fn resolve_commit() -> String {
+    Command::new("git")
+        .args(["rev-parse", "--short", "HEAD"])
+        .output()
+        .ok()
+        .and_then(|output| {
+            if output.status.success() {
+                let trimmed = String::from_utf8_lossy(&output.stdout).trim().to_string();
+                if trimmed.is_empty() {
+                    None
+                } else {
+                    Some(trimmed)
+                }
+            } else {
+                None
+            }
+        })
+        .unwrap_or_else(|| "unknown".to_string())
+}
+
+fn main() {
+    let commit = resolve_commit();
+    println!("cargo:rustc-env=CODEX_LITELLM_COMMIT={commit}");
+    println!("cargo:rustc-env=CODEX_UPSTREAM_COMMIT={commit}");
+}
diff --git a/codex-rs/build-info/src/lib.rs b/codex-rs/build-info/src/lib.rs
new file mode 100644
index 00000000..db1f6841
--- /dev/null
+++ b/codex-rs/build-info/src/lib.rs
@@ -0,0 +1,20 @@
+pub const CODEX_UPSTREAM_COMMIT: &str = env!("CODEX_UPSTREAM_COMMIT");
+pub const CODEX_LITELLM_COMMIT: &str = env!("CODEX_LITELLM_COMMIT");
+pub const VERSION_WITH_COMMIT: &str = concat!(
+    env!("CARGO_PKG_VERSION"),
+    "+",
+    env!("CODEX_UPSTREAM_COMMIT"),
+    "+lit",
+    env!("CODEX_LITELLM_COMMIT")
+);
+
+pub fn decorate_version(base_version: &str) -> String {
+    format!(
+        "{base_version}+{}+lit{}",
+        CODEX_UPSTREAM_COMMIT, CODEX_LITELLM_COMMIT
+    )
+}
+
+pub fn version_with_commit() -> String {
+    VERSION_WITH_COMMIT.to_string()
+}
diff --git a/codex-rs/common/Cargo.toml b/codex-rs/common/Cargo.toml
index d8f30cc0..2c667b84 100644
--- a/codex-rs/common/Cargo.toml
+++ b/codex-rs/common/Cargo.toml
@@ -11,11 +11,14 @@ clap = { workspace = true, features = ["derive", "wrap_help"], optional = true }
 codex-core = { workspace = true }
 codex-protocol = { workspace = true }
 codex-app-server-protocol = { workspace = true }
+codex-build-info = { workspace = true }
 serde = { workspace = true, optional = true }
+serde_json = { workspace = true, optional = true }
+reqwest = { workspace = true, features = ["blocking", "json"], optional = true }
 toml = { workspace = true, optional = true }
 
 [features]
 # Separate feature so that `clap` is not a mandatory dependency.
-cli = ["clap", "serde", "toml"]
+cli = ["clap", "serde", "serde_json", "toml", "reqwest"]
 elapsed = []
 sandbox_summary = []
diff --git a/codex-rs/common/src/lib.rs b/codex-rs/common/src/lib.rs
index 276bfca0..c445cec5 100644
--- a/codex-rs/common/src/lib.rs
+++ b/codex-rs/common/src/lib.rs
@@ -30,6 +30,7 @@ pub use sandbox_summary::summarize_sandbox_policy;
 mod config_summary;
 
 pub use config_summary::create_config_summary_entries;
+pub mod litellm;
 // Shared fuzzy matcher (used by TUI selection popups and other UI filtering)
 pub mod fuzzy_match;
 // Shared model presets used by TUI and MCP server
diff --git a/codex-rs/common/src/litellm.rs b/codex-rs/common/src/litellm.rs
new file mode 100644
index 00000000..2c105720
--- /dev/null
+++ b/codex-rs/common/src/litellm.rs
@@ -0,0 +1,77 @@
+use codex_build_info::{decorate_version as decorate_version_inner, version_with_commit};
+use codex_core::config::{
+    LiteLlmProviderUpdate, ensure_litellm_baseline, read_litellm_provider_state,
+    write_litellm_provider_state,
+};
+use std::env;
+use std::io;
+use std::io::IsTerminal;
+use std::io::Write as _;
+use std::path::Path;
+
+pub const LITELLM_BASE_URL_ENV: &str = "LITELLM_BASE_URL";
+pub const LITELLM_API_KEY_ENV: &str = "LITELLM_API_KEY";
+
+pub fn decorate_version(base_version: &str) -> String {
+    decorate_version_inner(base_version)
+}
+
+pub fn version_with_commit_string() -> String {
+    version_with_commit()
+}
+
+pub fn ensure_litellm_credentials(codex_home: &Path) -> io::Result<()> {
+    ensure_litellm_baseline(codex_home)?;
+    let state = read_litellm_provider_state(codex_home)?;
+    let mut update = LiteLlmProviderUpdate::default();
+
+    if state.base_url.is_none() {
+        let base_url = prompt_or_env("LiteLLM endpoint URL", LITELLM_BASE_URL_ENV, false)?;
+        update.base_url = Some(base_url);
+    }
+
+    if state.api_key.is_none() {
+        let api_key = prompt_or_env("LiteLLM API key", LITELLM_API_KEY_ENV, true)?;
+        update.api_key = Some(api_key);
+    }
+
+    write_litellm_provider_state(codex_home, update)
+}
+
+fn prompt_or_env(label: &str, env_var: &str, secret: bool) -> io::Result<String> {
+    if let Ok(value) = env::var(env_var) {
+        let trimmed = value.trim();
+        if !trimmed.is_empty() {
+            eprintln!("{label} sourced from ${env_var}.");
+            return Ok(trimmed.to_string());
+        }
+    }
+
+    if !io::stdin().is_terminal() {
+        return Err(io::Error::new(
+            io::ErrorKind::InvalidInput,
+            format!("{label} is not configured and ${env_var} is unset."),
+        ));
+    }
+
+    prompt_for_value(label, secret)
+}
+
+fn prompt_for_value(label: &str, secret: bool) -> io::Result<String> {
+    loop {
+        if secret {
+            eprint!("{label}: ");
+        } else {
+            eprint!("{label}: ");
+        }
+        io::stderr().flush()?;
+        let mut buffer = String::new();
+        io::stdin().read_line(&mut buffer)?;
+        let value = buffer.trim().to_string();
+        if value.is_empty() {
+            eprintln!("{label} cannot be empty.");
+            continue;
+        }
+        return Ok(value);
+    }
+}
diff --git a/codex-rs/common/src/model_presets.rs b/codex-rs/common/src/model_presets.rs
index b6d06438..23fa9789 100644
--- a/codex-rs/common/src/model_presets.rs
+++ b/codex-rs/common/src/model_presets.rs
@@ -1,5 +1,9 @@
 use codex_app_server_protocol::AuthMode;
 use codex_core::protocol_config_types::ReasoningEffort;
+use reqwest::blocking::Client;
+use serde::Deserialize;
+use serde_json::Value;
+use std::collections::HashSet;
 
 /// A reasoning effort option that can be surfaced for a model.
 #[derive(Debug, Clone, Copy)]
@@ -11,16 +15,16 @@ pub struct ReasoningEffortPreset {
 }
 
 /// Metadata describing a Codex-supported model.
-#[derive(Debug, Clone, Copy)]
+#[derive(Debug, Clone)]
 pub struct ModelPreset {
     /// Stable identifier for the preset.
-    pub id: &'static str,
+    pub id: String,
     /// Model slug (e.g., "gpt-5").
-    pub model: &'static str,
+    pub model: String,
     /// Display name shown in UIs.
-    pub display_name: &'static str,
+    pub display_name: String,
     /// Short human description shown in UIs.
-    pub description: &'static str,
+    pub description: Option<String>,
     /// Reasoning effort applied when none is explicitly chosen.
     pub default_reasoning_effort: ReasoningEffort,
     /// Supported reasoning effort options.
@@ -29,68 +33,264 @@ pub struct ModelPreset {
     pub is_default: bool,
 }
 
-const PRESETS: &[ModelPreset] = &[
-    ModelPreset {
-        id: "gpt-5-codex",
-        model: "gpt-5-codex",
-        display_name: "gpt-5-codex",
-        description: "Optimized for coding tasks with many tools.",
-        default_reasoning_effort: ReasoningEffort::Medium,
-        supported_reasoning_efforts: &[
-            ReasoningEffortPreset {
-                effort: ReasoningEffort::Low,
-                description: "Fastest responses with limited reasoning",
-            },
-            ReasoningEffortPreset {
-                effort: ReasoningEffort::Medium,
-                description: "Dynamically adjusts reasoning based on the task",
-            },
-            ReasoningEffortPreset {
-                effort: ReasoningEffort::High,
-                description: "Maximizes reasoning depth for complex or ambiguous problems",
-            },
-        ],
-        is_default: true,
+const STANDARD_REASONING_PRESETS: &[ReasoningEffortPreset] = &[
+    ReasoningEffortPreset {
+        effort: ReasoningEffort::Low,
+        description: "Fastest responses with limited reasoning",
     },
-    ModelPreset {
-        id: "gpt-5",
-        model: "gpt-5",
-        display_name: "gpt-5",
-        description: "Broad world knowledge with strong general reasoning.",
-        default_reasoning_effort: ReasoningEffort::Medium,
-        supported_reasoning_efforts: &[
-            ReasoningEffortPreset {
-                effort: ReasoningEffort::Minimal,
-                description: "Fastest responses with little reasoning",
-            },
-            ReasoningEffortPreset {
-                effort: ReasoningEffort::Low,
-                description: "Balances speed with some reasoning; useful for straightforward queries and short explanations",
-            },
-            ReasoningEffortPreset {
-                effort: ReasoningEffort::Medium,
-                description: "Provides a solid balance of reasoning depth and latency for general-purpose tasks",
-            },
-            ReasoningEffortPreset {
-                effort: ReasoningEffort::High,
-                description: "Maximizes reasoning depth for complex or ambiguous problems",
-            },
-        ],
-        is_default: false,
+    ReasoningEffortPreset {
+        effort: ReasoningEffort::Medium,
+        description: "Dynamically adjusts reasoning based on the task",
+    },
+    ReasoningEffortPreset {
+        effort: ReasoningEffort::High,
+        description: "Maximizes reasoning depth for complex or ambiguous problems",
+    },
+];
+
+const GPT5_REASONING_PRESETS: &[ReasoningEffortPreset] = &[
+    ReasoningEffortPreset {
+        effort: ReasoningEffort::Minimal,
+        description: "Fastest responses with little reasoning",
+    },
+    ReasoningEffortPreset {
+        effort: ReasoningEffort::Low,
+        description: "Balances speed with some reasoning; useful for straightforward queries and short explanations",
+    },
+    ReasoningEffortPreset {
+        effort: ReasoningEffort::Medium,
+        description: "Provides a solid balance of reasoning depth and latency for general-purpose tasks",
+    },
+    ReasoningEffortPreset {
+        effort: ReasoningEffort::High,
+        description: "Maximizes reasoning depth for complex or ambiguous problems",
     },
 ];
 
 pub fn builtin_model_presets(_auth_mode: Option<AuthMode>) -> Vec<ModelPreset> {
-    PRESETS.to_vec()
+    vec![
+        ModelPreset {
+            id: "gpt-5-codex".to_string(),
+            model: "gpt-5-codex".to_string(),
+            display_name: "gpt-5-codex".to_string(),
+            description: Some("Optimized for coding tasks with many tools.".to_string()),
+            default_reasoning_effort: ReasoningEffort::Medium,
+            supported_reasoning_efforts: STANDARD_REASONING_PRESETS,
+            is_default: true,
+        },
+        ModelPreset {
+            id: "gpt-5".to_string(),
+            model: "gpt-5".to_string(),
+            display_name: "gpt-5".to_string(),
+            description: Some("Broad world knowledge with strong general reasoning.".to_string()),
+            default_reasoning_effort: ReasoningEffort::Medium,
+            supported_reasoning_efforts: GPT5_REASONING_PRESETS,
+            is_default: false,
+        },
+    ]
+}
+
+pub fn make_litellm_model_preset(model: &str, is_default: bool) -> ModelPreset {
+    ModelPreset {
+        id: model.to_string(),
+        model: model.to_string(),
+        display_name: model.to_string(),
+        description: None,
+        default_reasoning_effort: ReasoningEffort::Medium,
+        supported_reasoning_efforts: STANDARD_REASONING_PRESETS,
+        is_default,
+    }
+}
+
+pub const LITELLM_PROVIDER_ID: &str = "litellm";
+pub const LEGACY_LITELLM_PROVIDER_ID: &str = "litellm-direct";
+const LITELLM_DEFAULT_MODEL: &str = "gpt-oss-120b-litellm";
+
+pub fn is_litellm_provider_id(provider_id: &str) -> bool {
+    matches!(
+        provider_id,
+        LITELLM_PROVIDER_ID | LEGACY_LITELLM_PROVIDER_ID
+    )
+}
+
+pub fn presets_for_provider(
+    provider_id: &str,
+    current_model: &str,
+    auth_mode: Option<AuthMode>,
+) -> Vec<ModelPreset> {
+    if matches!(
+        provider_id,
+        LITELLM_PROVIDER_ID | LEGACY_LITELLM_PROVIDER_ID
+    ) {
+        let mut presets = Vec::new();
+        presets.push(make_litellm_model_preset(current_model, true));
+        if current_model != LITELLM_DEFAULT_MODEL {
+            presets.push(make_litellm_model_preset(LITELLM_DEFAULT_MODEL, false));
+        }
+        presets
+    } else {
+        builtin_model_presets(auth_mode)
+    }
+}
+
+#[derive(Debug, Deserialize)]
+struct LiteLlmModelEntry {
+    #[serde(default)]
+    id: Option<String>,
+    #[serde(default)]
+    model: Option<String>,
+    #[serde(default)]
+    description: Option<String>,
+    #[serde(default)]
+    display_name: Option<String>,
+}
+
+pub fn fetch_litellm_model_presets(
+    base_url: &str,
+    api_key: Option<&str>,
+    current_model: &str,
+) -> Result<Vec<ModelPreset>, String> {
+    let base = base_url.to_string();
+    let key = api_key.map(|k| k.to_string());
+    let current = current_model.to_string();
+
+    std::thread::spawn(move || {
+        fetch_litellm_model_presets_blocking(&base, key.as_deref(), &current)
+    })
+    .join()
+    .map_err(|_| "LiteLLM /models worker panicked".to_string())?
+}
+
+fn fetch_litellm_model_presets_blocking(
+    base_url: &str,
+    api_key: Option<&str>,
+    current_model: &str,
+) -> Result<Vec<ModelPreset>, String> {
+    let trimmed = base_url.trim_end_matches('/');
+    let url = format!("{trimmed}/models");
+
+    let client = Client::builder()
+        .user_agent("codex-litellm")
+        .build()
+        .map_err(|err| format!("failed to create HTTP client: {err}"))?;
+
+    let mut request = client.get(&url);
+    if let Some(token) = api_key {
+        request = request.bearer_auth(token);
+    }
+
+    let response = request
+        .send()
+        .map_err(|err| format!("LiteLLM /models request failed: {err}"))?;
+
+    if !response.status().is_success() {
+        return Err(format!(
+            "LiteLLM /models responded with status {}",
+            response.status()
+        ));
+    }
+
+    let payload: Value = response
+        .json()
+        .map_err(|err| format!("failed to decode LiteLLM /models response: {err}"))?;
+
+    let mut presets = extract_litellm_models(payload)?;
+    if presets.is_empty() {
+        return Err("LiteLLM /models returned an empty list".to_string());
+    }
+
+    if !presets.iter().any(|preset| preset.model == current_model) {
+        presets.insert(0, make_litellm_model_preset(current_model, true));
+    }
+    for preset in presets.iter_mut() {
+        preset.is_default = preset.model == current_model;
+    }
+
+    Ok(presets)
+}
+
+fn extract_litellm_models(payload: Value) -> Result<Vec<ModelPreset>, String> {
+    let array_value = payload
+        .get("data")
+        .or_else(|| payload.get("models"))
+        .or_else(|| payload.get("result"))
+        .cloned()
+        .unwrap_or(payload);
+
+    let items = match array_value {
+        Value::Array(values) => values,
+        other => return Err(format!("unexpected LiteLLM /models payload: {other:?}")),
+    };
+
+    let mut presets = Vec::new();
+    let mut seen: HashSet<String> = HashSet::new();
+    for item in items {
+        match item {
+            Value::String(slug) => {
+                if seen.insert(slug.clone()) {
+                    presets.push(make_litellm_model_preset(&slug, false));
+                }
+            }
+            Value::Object(map) => {
+                let entry: LiteLlmModelEntry = serde_json::from_value(Value::Object(map))
+                    .map_err(|err| format!("failed to parse LiteLLM model entry: {err}"))?;
+                if let Some(slug) = entry.id.or(entry.model) {
+                    if seen.insert(slug.clone()) {
+                        let mut preset = make_litellm_model_preset(&slug, false);
+                        if let Some(name) = entry.display_name.clone() {
+                            preset.display_name = name;
+                        }
+                        preset.description = entry.description;
+                        presets.push(preset);
+                    }
+                }
+            }
+            _ => {}
+        }
+    }
+    Ok(presets)
 }
 
 #[cfg(test)]
 mod tests {
     use super::*;
+    use serde_json::json;
 
     #[test]
     fn only_one_default_model_is_configured() {
-        let default_models = PRESETS.iter().filter(|preset| preset.is_default).count();
-        assert!(default_models == 1);
+        let default_models = builtin_model_presets(None)
+            .into_iter()
+            .filter(|preset| preset.is_default)
+            .count();
+        assert_eq!(default_models, 1);
+    }
+
+    #[test]
+    fn litellm_presets_include_current_model_first() {
+        let current = "or/minimax-m2:free";
+        let presets = presets_for_provider(LITELLM_PROVIDER_ID, current, None);
+        assert!(!presets.is_empty());
+        assert_eq!(presets[0].model, current);
+        assert!(presets[0].is_default);
+
+        let legacy = presets_for_provider(LEGACY_LITELLM_PROVIDER_ID, current, None);
+        assert!(!legacy.is_empty());
+        assert_eq!(legacy[0].model, current);
+        assert!(legacy[0].is_default);
+    }
+
+    #[test]
+    fn extract_models_handles_common_shapes() {
+        let payload = json!({
+            "data": [
+                { "id": "or/minimax-m2:free", "description": "MiniMax Lite" },
+                { "model": "vertex/gpt-oss-20b", "display_name": "GPT OSS 20B" },
+                "vertex/gpt-oss-120b"
+            ]
+        });
+        let presets = extract_litellm_models(payload).expect("parse models");
+        assert_eq!(presets.len(), 3);
+        assert_eq!(presets[0].model, "or/minimax-m2:free");
+        assert_eq!(presets[1].display_name, "GPT OSS 20B".to_string());
     }
 }
diff --git a/codex-rs/core/src/config/mod.rs b/codex-rs/core/src/config/mod.rs
index 1ee48d30..b17466d7 100644
--- a/codex-rs/core/src/config/mod.rs
+++ b/codex-rs/core/src/config/mod.rs
@@ -53,6 +53,9 @@ use std::path::PathBuf;
 use crate::config::profile::ConfigProfile;
 use toml::Value as TomlValue;
 use toml_edit::DocumentMut;
+use toml_edit::Item as TomlItem;
+use toml_edit::Table as TomlTable;
+use toml_edit::{Array as TomlArray, value};
 
 pub mod edit;
 pub mod profile;
@@ -71,6 +74,295 @@ pub const GPT_5_CODEX_MEDIUM_MODEL: &str = "gpt-5-codex";
 pub(crate) const PROJECT_DOC_MAX_BYTES: usize = 32 * 1024; // 32 KiB
 
 pub(crate) const CONFIG_TOML_FILE: &str = "config.toml";
+pub(crate) const LITELLM_PROVIDER_ID: &str = "litellm";
+const LEGACY_LITELLM_PROVIDER_ID: &str = "litellm-direct";
+pub(crate) const LITELLM_DEFAULT_MODEL: &str = "gpt-oss-120b-litellm";
+const LITELLM_PROVIDER_NAME: &str = "LiteLLM Direct";
+const LITELLM_DEFAULT_MAX_TOKENS: i64 = 128_000;
+const LITELLM_DEFAULT_SANDBOX_MODE: &str = "workspace-write";
+const LITELLM_DEFAULT_APPROVAL_POLICY: &str = "never";
+const LITELLM_DEFAULT_REQUEST_MAX_RETRIES: i64 = 4;
+const LITELLM_DEFAULT_STREAM_MAX_RETRIES: i64 = 5;
+const LITELLM_DEFAULT_STREAM_IDLE_TIMEOUT_MS: i64 = 300_000;
+
+#[derive(Debug, Clone, Default)]
+pub struct LiteLlmProviderUpdate {
+    pub base_url: Option<String>,
+    pub api_key: Option<String>,
+}
+
+impl LiteLlmProviderUpdate {
+    fn is_empty(&self) -> bool {
+        self.base_url.is_none() && self.api_key.is_none()
+    }
+}
+
+#[derive(Debug, Clone)]
+pub struct LiteLlmProviderState {
+    pub base_url: Option<String>,
+    pub api_key: Option<String>,
+}
+
+pub fn ensure_litellm_baseline(codex_home: &Path) -> std::io::Result<()> {
+    std::fs::create_dir_all(codex_home)?;
+    let config_path = codex_home.join(CONFIG_TOML_FILE);
+    let mut doc = load_config_document(&config_path)?;
+    let mut changed = migrate_legacy_litellm_provider(&mut doc);
+
+    if !doc.as_table().contains_key("model_provider") {
+        doc["model_provider"] = value(LITELLM_PROVIDER_ID);
+        changed = true;
+    }
+
+    if !doc.as_table().contains_key("model") {
+        doc["model"] = value(LITELLM_DEFAULT_MODEL);
+        changed = true;
+    }
+
+    if !doc.as_table().contains_key("max_tokens") {
+        doc["max_tokens"] = value(LITELLM_DEFAULT_MAX_TOKENS);
+        changed = true;
+    }
+
+    if !doc.as_table().contains_key("sandbox_mode") {
+        doc["sandbox_mode"] = value(LITELLM_DEFAULT_SANDBOX_MODE);
+        changed = true;
+    }
+
+    if !doc.as_table().contains_key("approval_policy") {
+        doc["approval_policy"] = value(LITELLM_DEFAULT_APPROVAL_POLICY);
+        changed = true;
+    }
+
+    let provider_table = ensure_litellm_provider_table(&mut doc);
+    if !provider_table.contains_key("name") {
+        provider_table["name"] = value(LITELLM_PROVIDER_NAME);
+        changed = true;
+    }
+    if !provider_table.contains_key("wire_api") {
+        provider_table["wire_api"] = value("chat");
+        changed = true;
+    }
+    if !provider_table.contains_key("requires_openai_auth") {
+        provider_table["requires_openai_auth"] = value(false);
+        changed = true;
+    }
+    if !provider_table.contains_key("request_max_retries") {
+        provider_table["request_max_retries"] = value(LITELLM_DEFAULT_REQUEST_MAX_RETRIES);
+        changed = true;
+    }
+    if !provider_table.contains_key("stream_max_retries") {
+        provider_table["stream_max_retries"] = value(LITELLM_DEFAULT_STREAM_MAX_RETRIES);
+        changed = true;
+    }
+    if !provider_table.contains_key("stream_idle_timeout_ms") {
+        provider_table["stream_idle_timeout_ms"] = value(LITELLM_DEFAULT_STREAM_IDLE_TIMEOUT_MS);
+        changed = true;
+    }
+
+    let chat_params = provider_table
+        .entry("chat_completion_parameters")
+        .or_insert(TomlItem::Table(TomlTable::new()));
+    if let Some(params_table) = chat_params.as_table_mut() {
+        params_table.set_implicit(false);
+        if !params_table.contains_key("modalities") {
+            let mut arr = TomlArray::new();
+            arr.push("text");
+            params_table["modalities"] = TomlItem::Value(arr.into());
+            changed = true;
+        }
+        let response_format = params_table
+            .entry("response_format")
+            .or_insert(TomlItem::Table(TomlTable::new()));
+        if let Some(response_table) = response_format.as_table_mut() {
+            response_table.set_implicit(false);
+            if !response_table.contains_key("type") {
+                response_table["type"] = value("text");
+                changed = true;
+            }
+        }
+    }
+
+    if changed {
+        save_config_document(&config_path, &doc)?;
+    }
+
+    Ok(())
+}
+
+pub fn read_litellm_provider_state(codex_home: &Path) -> std::io::Result<LiteLlmProviderState> {
+    ensure_litellm_baseline(codex_home)?;
+    let config_path = codex_home.join(CONFIG_TOML_FILE);
+    let doc = load_config_document(&config_path)?;
+    let provider_table = ensure_litellm_provider_table_ref(&doc);
+    let base_url = provider_table
+        .and_then(|table| table.get("base_url"))
+        .and_then(TomlItem::as_str)
+        .map(|value| value.trim().to_string())
+        .filter(|value| !value.is_empty());
+    let api_key = provider_table
+        .and_then(|table| table.get("experimental_bearer_token"))
+        .and_then(TomlItem::as_str)
+        .map(|value| value.trim().to_string())
+        .filter(|value| !value.is_empty());
+
+    Ok(LiteLlmProviderState { base_url, api_key })
+}
+
+pub fn write_litellm_provider_state(
+    codex_home: &Path,
+    update: LiteLlmProviderUpdate,
+) -> std::io::Result<()> {
+    if update.is_empty() {
+        return Ok(());
+    }
+
+    ensure_litellm_baseline(codex_home)?;
+    let config_path = codex_home.join(CONFIG_TOML_FILE);
+    let mut doc = load_config_document(&config_path)?;
+    let provider_table = ensure_litellm_provider_table(&mut doc);
+    let mut changed = false;
+
+    if let Some(base_url) = update.base_url {
+        if base_url.is_empty() {
+            provider_table.remove("base_url");
+        } else {
+            provider_table["base_url"] = value(base_url);
+        }
+        changed = true;
+    }
+
+    if let Some(api_key) = update.api_key {
+        if api_key.is_empty() {
+            provider_table.remove("experimental_bearer_token");
+        } else {
+            provider_table["experimental_bearer_token"] = value(api_key);
+        }
+        changed = true;
+    }
+
+    if changed {
+        save_config_document(&config_path, &doc)?;
+    }
+
+    Ok(())
+}
+
+pub fn set_default_model(codex_home: &Path, model: &str) -> std::io::Result<()> {
+    ensure_litellm_baseline(codex_home)?;
+    let config_path = codex_home.join(CONFIG_TOML_FILE);
+    let mut doc = load_config_document(&config_path)?;
+    doc["model"] = value(model);
+    save_config_document(&config_path, &doc)?;
+    Ok(())
+}
+
+pub fn clear_litellm_credentials(codex_home: &Path) -> std::io::Result<bool> {
+    let config_path = codex_home.join(CONFIG_TOML_FILE);
+    let mut doc = load_config_document(&config_path)?;
+    let provider_exists = match ensure_litellm_provider_table_mut(&mut doc) {
+        Some(table) => table,
+        None => return Ok(false),
+    };
+
+    let mut changed = false;
+    if provider_exists.remove("base_url").is_some() {
+        changed = true;
+    }
+    if provider_exists
+        .remove("experimental_bearer_token")
+        .is_some()
+    {
+        changed = true;
+    }
+
+    if changed {
+        save_config_document(&config_path, &doc)?;
+    }
+
+    Ok(changed)
+}
+
+fn migrate_legacy_litellm_provider(doc: &mut DocumentMut) -> bool {
+    let mut changed = false;
+
+    if doc
+        .as_table()
+        .get("model_provider")
+        .and_then(TomlItem::as_str)
+        == Some(LEGACY_LITELLM_PROVIDER_ID)
+    {
+        doc["model_provider"] = value(LITELLM_PROVIDER_ID);
+        changed = true;
+    }
+
+    if let Some(providers) = doc
+        .as_table_mut()
+        .get_mut("model_providers")
+        .and_then(TomlItem::as_table_mut)
+    {
+        if providers.contains_key(LEGACY_LITELLM_PROVIDER_ID) {
+            if !providers.contains_key(LITELLM_PROVIDER_ID) {
+                if let Some(existing) = providers.get(LEGACY_LITELLM_PROVIDER_ID).cloned() {
+                    providers.insert(LITELLM_PROVIDER_ID, existing);
+                }
+            }
+            providers.remove(LEGACY_LITELLM_PROVIDER_ID);
+            changed = true;
+        }
+    }
+
+    changed
+}
+
+fn ensure_litellm_provider_table(doc: &mut DocumentMut) -> &mut TomlTable {
+    let providers_item = doc
+        .as_table_mut()
+        .entry("model_providers")
+        .or_insert(TomlItem::Table(TomlTable::new()));
+    let providers_table = providers_item.as_table_mut().expect("table");
+    providers_table.set_implicit(false);
+    let provider_item = providers_table
+        .entry(LITELLM_PROVIDER_ID)
+        .or_insert(TomlItem::Table(TomlTable::new()));
+    let provider_table = provider_item.as_table_mut().expect("table");
+    provider_table.set_implicit(false);
+    provider_table
+}
+
+fn ensure_litellm_provider_table_ref(doc: &DocumentMut) -> Option<&TomlTable> {
+    doc.as_table()
+        .get("model_providers")
+        .and_then(TomlItem::as_table)
+        .and_then(|providers| providers.get(LITELLM_PROVIDER_ID))
+        .and_then(TomlItem::as_table)
+}
+
+fn ensure_litellm_provider_table_mut(doc: &mut DocumentMut) -> Option<&mut TomlTable> {
+    doc.as_table_mut()
+        .get_mut("model_providers")
+        .and_then(TomlItem::as_table_mut)
+        .and_then(|providers| providers.get_mut(LITELLM_PROVIDER_ID))
+        .and_then(TomlItem::as_table_mut)
+}
+
+fn load_config_document(path: &Path) -> std::io::Result<DocumentMut> {
+    match std::fs::read_to_string(path) {
+        Ok(contents) => match contents.parse::<DocumentMut>() {
+            Ok(doc) => Ok(doc),
+            Err(err) => {
+                tracing::warn!("Failed to parse {}: {err}", path.display());
+                Ok(DocumentMut::new())
+            }
+        },
+        Err(err) if err.kind() == ErrorKind::NotFound => Ok(DocumentMut::new()),
+        Err(err) => Err(err),
+    }
+}
+
+fn save_config_document(path: &Path, doc: &DocumentMut) -> std::io::Result<()> {
+    std::fs::write(path, doc.to_string())
+}
 
 /// Application configuration loaded from disk and merged with overrides.
 #[derive(Debug, Clone, PartialEq)]
@@ -98,6 +390,9 @@ pub struct Config {
     /// Info needed to make an API request to the model.
     pub model_provider: ModelProviderInfo,
 
+    /// Returns true when the bundled LiteLLM provider still needs a base URL.
+    pub litellm_setup_required: bool,
+
     /// Approval policy for executing commands.
     pub approval_policy: AskForApproval,
 
@@ -976,9 +1271,10 @@ impl Config {
         let mut model_providers = built_in_model_providers();
         // Merge user-defined providers into the built-in list.
         for (key, provider) in cfg.model_providers.into_iter() {
-            model_providers.entry(key).or_insert(provider);
+            model_providers.insert(key, provider);
         }
 
+        // Ensure LiteLLM deployments using the fallback proxy maintain stream format compatibility.
         let model_provider_id = model_provider
             .or(config_profile.model_provider)
             .or(cfg.model_provider)
@@ -993,6 +1289,20 @@ impl Config {
             })?
             .clone();
 
+        let base_url_missing = model_provider
+            .base_url
+            .as_ref()
+            .map(|value| value.trim().is_empty())
+            .unwrap_or(true);
+        let token_missing = model_provider
+            .experimental_bearer_token
+            .as_ref()
+            .map(|value| value.trim().is_empty())
+            .unwrap_or(true);
+        let litellm_setup_required = (model_provider_id == "litellm"
+            || model_provider_id == LITELLM_PROVIDER_ID)
+            && (base_url_missing || token_missing);
+
         let shell_environment_policy = cfg.shell_environment_policy.into();
 
         let history = cfg.history.unwrap_or_default();
@@ -1096,6 +1406,7 @@ impl Config {
             model_auto_compact_token_limit,
             model_provider_id,
             model_provider,
+            litellm_setup_required,
             cwd: resolved_cwd,
             approval_policy,
             sandbox_policy,
@@ -2870,6 +3181,7 @@ model_verbosity = "high"
                 model_auto_compact_token_limit: Some(180_000),
                 model_provider_id: "openai".to_string(),
                 model_provider: fixture.openai_provider.clone(),
+                litellm_setup_required: false,
                 approval_policy: AskForApproval::Never,
                 sandbox_policy: SandboxPolicy::new_read_only_policy(),
                 did_user_set_custom_approval_policy_or_sandbox_mode: true,
@@ -2942,6 +3254,7 @@ model_verbosity = "high"
             model_auto_compact_token_limit: Some(14_746),
             model_provider_id: "openai-chat-completions".to_string(),
             model_provider: fixture.openai_chat_completions_provider.clone(),
+            litellm_setup_required: false,
             approval_policy: AskForApproval::UnlessTrusted,
             sandbox_policy: SandboxPolicy::new_read_only_policy(),
             did_user_set_custom_approval_policy_or_sandbox_mode: true,
@@ -3029,6 +3342,7 @@ model_verbosity = "high"
             model_auto_compact_token_limit: Some(180_000),
             model_provider_id: "openai".to_string(),
             model_provider: fixture.openai_provider.clone(),
+            litellm_setup_required: false,
             approval_policy: AskForApproval::OnFailure,
             sandbox_policy: SandboxPolicy::new_read_only_policy(),
             did_user_set_custom_approval_policy_or_sandbox_mode: true,
@@ -3102,6 +3416,7 @@ model_verbosity = "high"
             model_auto_compact_token_limit: Some(244_800),
             model_provider_id: "openai".to_string(),
             model_provider: fixture.openai_provider.clone(),
+            litellm_setup_required: false,
             approval_policy: AskForApproval::OnFailure,
             sandbox_policy: SandboxPolicy::new_read_only_policy(),
             did_user_set_custom_approval_policy_or_sandbox_mode: true,
diff --git a/codex-rs/core/src/config_loader/mod.rs b/codex-rs/core/src/config_loader/mod.rs
index 6b55b015..311cb2b6 100644
--- a/codex-rs/core/src/config_loader/mod.rs
+++ b/codex-rs/core/src/config_loader/mod.rs
@@ -1,6 +1,6 @@
 mod macos;
 
-use crate::config::CONFIG_TOML_FILE;
+use crate::config::{CONFIG_TOML_FILE, ensure_litellm_baseline};
 use macos::load_managed_admin_config_layer;
 use std::io;
 use std::path::Path;
@@ -70,6 +70,8 @@ async fn load_config_layers_internal(
     codex_home: &Path,
     overrides: LoaderOverrides,
 ) -> io::Result<LoadedConfigLayers> {
+    ensure_litellm_baseline(codex_home)?;
+
     #[cfg(target_os = "macos")]
     let LoaderOverrides {
         managed_config_path,
diff --git a/codex-rs/tui/Cargo.toml b/codex-rs/tui/Cargo.toml
index f087202c..22a13a0c 100644
--- a/codex-rs/tui/Cargo.toml
+++ b/codex-rs/tui/Cargo.toml
@@ -40,6 +40,7 @@ codex-ollama = { workspace = true }
 codex-protocol = { workspace = true }
 codex-app-server-protocol = { workspace = true }
 codex-feedback = { workspace = true }
+codex-litellm-model-session-telemetry = { workspace = true }
 color-eyre = { workspace = true }
 crossterm = { workspace = true, features = ["bracketed-paste", "event-stream"] }
 diffy = { workspace = true }
diff --git a/codex-rs/tui/src/app.rs b/codex-rs/tui/src/app.rs
index 3c06113c..2adf5a55 100644
--- a/codex-rs/tui/src/app.rs
+++ b/codex-rs/tui/src/app.rs
@@ -14,6 +14,7 @@ use crate::tui;
 use crate::tui::TuiEvent;
 use crate::updates::UpdateAction;
 use codex_ansi_escape::ansi_escape_line;
+use codex_common::model_presets::ModelPreset;
 use codex_core::AuthManager;
 use codex_core::ConversationManager;
 use codex_core::config::Config;
@@ -78,6 +79,7 @@ pub(crate) struct App {
     pub(crate) feedback: codex_feedback::CodexFeedback,
     /// Set when the user confirms an update; propagated on exit.
     pub(crate) pending_update_action: Option<UpdateAction>,
+    model_presets: Vec<ModelPreset>,
 }
 
 impl App {
@@ -91,6 +93,9 @@ impl App {
         initial_images: Vec<PathBuf>,
         resume_selection: ResumeSelection,
         feedback: codex_feedback::CodexFeedback,
+        model_presets: Vec<ModelPreset>,
+        auto_open_model_selector: bool,
+        litellm_model_missing: bool,
     ) -> Result<AppExitInfo> {
         use tokio_stream::StreamExt;
         let (app_event_tx, mut app_event_rx) = unbounded_channel();
@@ -103,6 +108,12 @@ impl App {
 
         let enhanced_keys_supported = tui.enhanced_keys_supported();
 
+        let should_auto_open_selector = auto_open_model_selector
+            && matches!(
+                resume_selection,
+                ResumeSelection::StartFresh | ResumeSelection::Exit
+            );
+
         let chat_widget = match resume_selection {
             ResumeSelection::StartFresh | ResumeSelection::Exit => {
                 let init = crate::chatwidget::ChatWidgetInit {
@@ -114,6 +125,9 @@ impl App {
                     enhanced_keys_supported,
                     auth_manager: auth_manager.clone(),
                     feedback: feedback.clone(),
+                    model_presets: model_presets.clone(),
+                    auto_open_model_selector: should_auto_open_selector,
+                    litellm_model_missing,
                 };
                 ChatWidget::new(init, conversation_manager.clone())
             }
@@ -137,6 +151,9 @@ impl App {
                     enhanced_keys_supported,
                     auth_manager: auth_manager.clone(),
                     feedback: feedback.clone(),
+                    model_presets: model_presets.clone(),
+                    auto_open_model_selector: false,
+                    litellm_model_missing,
                 };
                 ChatWidget::new_from_existing(
                     init,
@@ -167,6 +184,7 @@ impl App {
             backtrack: BacktrackState::default(),
             feedback: feedback.clone(),
             pending_update_action: None,
+            model_presets,
         };
 
         #[cfg(not(debug_assertions))]
@@ -257,6 +275,9 @@ impl App {
                     enhanced_keys_supported: self.enhanced_keys_supported,
                     auth_manager: self.auth_manager.clone(),
                     feedback: self.feedback.clone(),
+                    model_presets: self.model_presets.clone(),
+                    auto_open_model_selector: false,
+                    litellm_model_missing: false,
                 };
                 self.chat_widget = ChatWidget::new(init, self.server.clone());
                 tui.frame_requester().schedule_frame();
diff --git a/codex-rs/tui/src/app_backtrack.rs b/codex-rs/tui/src/app_backtrack.rs
index e2ded3d3..ee7218c9 100644
--- a/codex-rs/tui/src/app_backtrack.rs
+++ b/codex-rs/tui/src/app_backtrack.rs
@@ -347,6 +347,9 @@ impl App {
             enhanced_keys_supported: self.enhanced_keys_supported,
             auth_manager: self.auth_manager.clone(),
             feedback: self.feedback.clone(),
+            model_presets: Vec::new(),
+            auto_open_model_selector: false,
+            litellm_model_missing: false,
         };
         self.chat_widget =
             crate::chatwidget::ChatWidget::new_from_existing(init, conv, session_configured);
diff --git a/codex-rs/tui/src/bottom_pane/list_selection_view.rs b/codex-rs/tui/src/bottom_pane/list_selection_view.rs
index 44d7b264..a5988a8e 100644
--- a/codex-rs/tui/src/bottom_pane/list_selection_view.rs
+++ b/codex-rs/tui/src/bottom_pane/list_selection_view.rs
@@ -364,13 +364,32 @@ impl Renderable for ListSelectionView {
         let rows = self.build_rows();
         let rows_height =
             measure_rows_height(&rows, &self.state, MAX_POPUP_ROWS, content_area.width);
+
+        let inset_area = content_area.inset(Insets::vh(1, 2));
+        let mut remaining = inset_area.height;
+
+        let header_len = header_height.min(remaining);
+        remaining = remaining.saturating_sub(header_len);
+
+        let spacer_len = remaining.min(1);
+        remaining = remaining.saturating_sub(spacer_len);
+
+        let search_len = if self.is_searchable {
+            remaining.min(1)
+        } else {
+            0
+        };
+        remaining = remaining.saturating_sub(search_len);
+
+        let rows_len = rows_height.min(remaining);
+
         let [header_area, _, search_area, list_area] = Layout::vertical([
-            Constraint::Max(header_height),
-            Constraint::Max(1),
-            Constraint::Length(if self.is_searchable { 1 } else { 0 }),
-            Constraint::Length(rows_height),
+            Constraint::Length(header_len),
+            Constraint::Length(spacer_len),
+            Constraint::Length(search_len),
+            Constraint::Fill(1),
         ])
-        .areas(content_area.inset(Insets::vh(1, 2)));
+        .areas(inset_area);
 
         if header_area.height < header_height {
             let [header_area, elision_area] =
@@ -385,31 +404,35 @@ impl Renderable for ListSelectionView {
         }
 
         if self.is_searchable {
-            Line::from(self.search_query.clone()).render(search_area, buf);
-            let query_span: Span<'static> = if self.search_query.is_empty() {
-                self.search_placeholder
-                    .as_ref()
-                    .map(|placeholder| placeholder.clone().dim())
-                    .unwrap_or_else(|| "".into())
-            } else {
-                self.search_query.clone().into()
-            };
-            Line::from(query_span).render(search_area, buf);
+            if search_area.height > 0 {
+                Line::from(self.search_query.clone()).render(search_area, buf);
+                let query_span: Span<'static> = if self.search_query.is_empty() {
+                    self.search_placeholder
+                        .as_ref()
+                        .map(|placeholder| placeholder.clone().dim())
+                        .unwrap_or_else(|| "".into())
+                } else {
+                    self.search_query.clone().into()
+                };
+                Line::from(query_span).render(search_area, buf);
+            }
         }
 
-        if list_area.height > 0 {
-            let list_area = Rect {
-                x: list_area.x - 2,
+        let usable_rows = rows_len.min(list_area.height);
+
+        if usable_rows > 0 {
+            let render_area = Rect {
+                x: list_area.x,
                 y: list_area.y,
-                width: list_area.width + 2,
-                height: list_area.height,
+                width: list_area.width,
+                height: usable_rows,
             };
             render_rows(
-                list_area,
+                render_area,
                 buf,
                 &rows,
                 &self.state,
-                list_area.height as usize,
+                usable_rows as usize,
                 "no matches",
             );
         }
diff --git a/codex-rs/tui/src/chatwidget.rs b/codex-rs/tui/src/chatwidget.rs
index 3dfa1b36..668168c7 100644
--- a/codex-rs/tui/src/chatwidget.rs
+++ b/codex-rs/tui/src/chatwidget.rs
@@ -102,22 +102,23 @@ use self::interrupts::InterruptManager;
 mod agent;
 use self::agent::spawn_agent;
 use self::agent::spawn_agent_from_existing;
-mod session_header;
-use self::session_header::SessionHeader;
 use crate::streaming::controller::StreamController;
 use std::path::Path;
 
 use chrono::Local;
 use codex_common::approval_presets::ApprovalPreset;
 use codex_common::approval_presets::builtin_approval_presets;
-use codex_common::model_presets::ModelPreset;
-use codex_common::model_presets::builtin_model_presets;
+use codex_common::model_presets::{
+    ModelPreset, builtin_model_presets, fetch_litellm_model_presets, is_litellm_provider_id,
+};
 use codex_core::AuthManager;
 use codex_core::ConversationManager;
+use codex_core::config::read_litellm_provider_state;
 use codex_core::protocol::AskForApproval;
 use codex_core::protocol::SandboxPolicy;
 use codex_core::protocol_config_types::ReasoningEffort as ReasoningEffortConfig;
 use codex_file_search::FileMatch;
+use codex_litellm_model_session_telemetry as session_telemetry;
 use codex_protocol::plan_tool::UpdatePlanArgs;
 use strum::IntoEnumIterator;
 
@@ -227,6 +228,9 @@ pub(crate) struct ChatWidgetInit {
     pub(crate) enhanced_keys_supported: bool,
     pub(crate) auth_manager: Arc<AuthManager>,
     pub(crate) feedback: codex_feedback::CodexFeedback,
+    pub(crate) model_presets: Vec<ModelPreset>,
+    pub(crate) auto_open_model_selector: bool,
+    pub(crate) litellm_model_missing: bool,
 }
 
 pub(crate) struct ChatWidget {
@@ -236,7 +240,6 @@ pub(crate) struct ChatWidget {
     active_cell: Option<Box<dyn HistoryCell>>,
     config: Config,
     auth_manager: Arc<AuthManager>,
-    session_header: SessionHeader,
     initial_user_message: Option<UserMessage>,
     token_info: Option<TokenUsageInfo>,
     rate_limit_snapshot: Option<RateLimitSnapshotDisplay>,
@@ -276,6 +279,11 @@ pub(crate) struct ChatWidget {
     feedback: codex_feedback::CodexFeedback,
     // Current session rollout path (if known)
     current_rollout_path: Option<PathBuf>,
+    model_presets: Vec<ModelPreset>,
+    auto_open_model_selector: bool,
+    litellm_model_missing: bool,
+    litellm_model_missing_notified: bool,
+    last_reasoning_delta: Option<String>,
 }
 
 struct UserMessage {
@@ -314,15 +322,24 @@ impl ChatWidget {
         self.bottom_pane.update_status_header(header);
     }
 
+    fn mark_litellm_model_missing(&mut self) {
+        if self.litellm_model_missing_notified {
+            return;
+        }
+        self.add_error_message("Select a LiteLLM model (run /model)".to_string());
+        self.litellm_model_missing_notified = true;
+    }
+
     // --- Small event handlers ---
     fn on_session_configured(&mut self, event: codex_core::protocol::SessionConfiguredEvent) {
         self.bottom_pane
             .set_history_metadata(event.history_log_id, event.history_entry_count);
+        let session_key = event.session_id.to_string();
         self.conversation_id = Some(event.session_id);
+        session_telemetry::clear_session(Some(&session_key));
         self.current_rollout_path = Some(event.rollout_path.clone());
         let initial_messages = event.initial_messages.clone();
-        let model_for_header = event.model.clone();
-        self.session_header.set_model(&model_for_header);
+        self.config.model = event.model.clone();
         self.add_to_history(history_cell::new_session_info(
             &self.config,
             event,
@@ -333,6 +350,10 @@ impl ChatWidget {
         }
         // Ask codex-core to enumerate custom prompts for this session.
         self.submit_op(Op::ListCustomPrompts);
+        if self.auto_open_model_selector {
+            self.auto_open_model_selector = false;
+            self.open_model_popup();
+        }
         if let Some(user_message) = self.initial_user_message.take() {
             self.submit_user_message(user_message);
         }
@@ -457,6 +478,7 @@ impl ChatWidget {
 
     pub(crate) fn set_token_info(&mut self, info: Option<TokenUsageInfo>) {
         if let Some(info) = info {
+            let last_usage = info.last_token_usage.clone();
             let context_window = info
                 .model_context_window
                 .or(self.config.model_context_window);
@@ -465,6 +487,28 @@ impl ChatWidget {
                     .percent_of_context_window_remaining(window)
             });
             self.bottom_pane.set_context_window_percent(percent);
+
+            if last_usage.total_tokens > 0 {
+                let prompt_tokens = last_usage
+                    .input_tokens
+                    .saturating_sub(last_usage.cached_input_tokens);
+                let reasoning_tokens = last_usage.reasoning_output_tokens;
+                let session_key = self.conversation_id.as_ref().map(|id| id.to_string());
+                let reasoning_label = self
+                    .config
+                    .model_reasoning_effort
+                    .map(|effort| effort.to_string());
+                session_telemetry::record_turn(
+                    session_key.as_deref(),
+                    &self.config.model,
+                    reasoning_label.as_deref(),
+                    prompt_tokens,
+                    last_usage.output_tokens,
+                    reasoning_tokens,
+                    last_usage.total_tokens,
+                );
+            }
+
             self.token_info = Some(info);
         }
     }
@@ -979,12 +1023,15 @@ impl ChatWidget {
             enhanced_keys_supported,
             auth_manager,
             feedback,
+            model_presets,
+            auto_open_model_selector,
+            litellm_model_missing,
         } = common;
         let mut rng = rand::rng();
         let placeholder = EXAMPLE_PROMPTS[rng.random_range(0..EXAMPLE_PROMPTS.len())].to_string();
         let codex_op_tx = spawn_agent(config.clone(), app_event_tx.clone(), conversation_manager);
 
-        Self {
+        let mut this = Self {
             app_event_tx: app_event_tx.clone(),
             frame_requester: frame_requester.clone(),
             codex_op_tx,
@@ -999,7 +1046,6 @@ impl ChatWidget {
             active_cell: None,
             config: config.clone(),
             auth_manager,
-            session_header: SessionHeader::new(config.model),
             initial_user_message: create_initial_user_message(
                 initial_prompt.unwrap_or_default(),
                 initial_images,
@@ -1025,7 +1071,18 @@ impl ChatWidget {
             last_rendered_width: std::cell::Cell::new(None),
             feedback,
             current_rollout_path: None,
+            model_presets,
+            auto_open_model_selector,
+            litellm_model_missing,
+            litellm_model_missing_notified: false,
+            last_reasoning_delta: None,
+        };
+
+        if this.litellm_model_missing {
+            this.mark_litellm_model_missing();
         }
+
+        this
     }
 
     /// Create a ChatWidget attached to an existing conversation (e.g., a fork).
@@ -1043,6 +1100,9 @@ impl ChatWidget {
             enhanced_keys_supported,
             auth_manager,
             feedback,
+            model_presets,
+            auto_open_model_selector,
+            litellm_model_missing,
         } = common;
         let mut rng = rand::rng();
         let placeholder = EXAMPLE_PROMPTS[rng.random_range(0..EXAMPLE_PROMPTS.len())].to_string();
@@ -1050,7 +1110,7 @@ impl ChatWidget {
         let codex_op_tx =
             spawn_agent_from_existing(conversation, session_configured, app_event_tx.clone());
 
-        Self {
+        let mut this = Self {
             app_event_tx: app_event_tx.clone(),
             frame_requester: frame_requester.clone(),
             codex_op_tx,
@@ -1065,7 +1125,6 @@ impl ChatWidget {
             active_cell: None,
             config: config.clone(),
             auth_manager,
-            session_header: SessionHeader::new(config.model),
             initial_user_message: create_initial_user_message(
                 initial_prompt.unwrap_or_default(),
                 initial_images,
@@ -1091,7 +1150,18 @@ impl ChatWidget {
             last_rendered_width: std::cell::Cell::new(None),
             feedback,
             current_rollout_path: None,
+            model_presets,
+            auto_open_model_selector,
+            litellm_model_missing,
+            litellm_model_missing_notified: false,
+            last_reasoning_delta: None,
+        };
+
+        if this.litellm_model_missing {
+            this.mark_litellm_model_missing();
         }
+
+        this
     }
 
     pub fn desired_height(&self, width: u16) -> u16 {
@@ -1680,20 +1750,58 @@ impl ChatWidget {
     pub(crate) fn open_model_popup(&mut self) {
         let current_model = self.config.model.clone();
         let auth_mode = self.auth_manager.auth().map(|auth| auth.mode);
-        let presets: Vec<ModelPreset> = builtin_model_presets(auth_mode);
-
-        let mut items: Vec<SelectionItem> = Vec::new();
-        for preset in presets.into_iter() {
-            let description = if preset.description.is_empty() {
-                None
+        let mut presets: Vec<ModelPreset> =
+            if is_litellm_provider_id(&self.config.model_provider_id) {
+                match read_litellm_provider_state(&self.config.codex_home)
+                    .map_err(|err| err.to_string())
+                    .and_then(|state| {
+                        let base_url = state
+                            .base_url
+                            .ok_or_else(|| "LiteLLM endpoint URL missing in config".to_string())?;
+                        fetch_litellm_model_presets(
+                            &base_url,
+                            state.api_key.as_deref(),
+                            &current_model,
+                        )
+                    }) {
+                    Ok(list) => list,
+                    Err(err) => {
+                        self.add_error_message(format!(
+                            "Unable to fetch LiteLLM model list ({err}). Falling back to defaults."
+                        ));
+                        builtin_model_presets(auth_mode)
+                    }
+                }
+            } else if self.model_presets.is_empty() {
+                builtin_model_presets(auth_mode)
             } else {
-                Some(preset.description.to_string())
+                self.model_presets.clone()
             };
+
+        if let Some(default_index) = presets
+            .iter()
+            .position(|preset| preset.model == current_model)
+        {
+            for (idx, preset) in presets.iter_mut().enumerate() {
+                preset.is_default = idx == default_index;
+            }
+        } else if !presets.is_empty() {
+            presets[0].is_default = true;
+        }
+
+        self.model_presets = presets.clone();
+
+        let mut items: Vec<SelectionItem> = Vec::new();
+        for preset in presets.iter() {
+            let description = preset
+                .description
+                .as_ref()
+                .and_then(|text| (!text.is_empty()).then(|| text.to_string()));
             let is_current = preset.model == current_model;
-            let preset_for_action = preset;
+            let preset_for_action = preset.clone();
             let actions: Vec<SelectionAction> = vec![Box::new(move |tx| {
                 tx.send(AppEvent::OpenReasoningPopup {
-                    model: preset_for_action,
+                    model: preset_for_action.clone(),
                 });
             })];
             items.push(SelectionItem {
@@ -2059,8 +2167,11 @@ impl ChatWidget {
 
     /// Set the model in the widget's config copy.
     pub(crate) fn set_model(&mut self, model: &str) {
-        self.session_header.set_model(model);
         self.config.model = model.to_string();
+        for preset in self.model_presets.iter_mut() {
+            preset.is_default = preset.model == model;
+        }
+        self.request_redraw();
     }
 
     pub(crate) fn add_info_message(&mut self, message: String, hint: Option<String>) {
@@ -2358,7 +2469,7 @@ impl ChatWidget {
 
 impl WidgetRef for &ChatWidget {
     fn render_ref(&self, area: Rect, buf: &mut Buffer) {
-        let [_, active_cell_area, bottom_pane_area] = self.layout_areas(area);
+        let [_header_area, active_cell_area, bottom_pane_area] = self.layout_areas(area);
         (&self.bottom_pane).render(bottom_pane_area, buf);
         if !active_cell_area.is_empty()
             && let Some(cell) = &self.active_cell
diff --git a/codex-rs/tui/src/chatwidget/session_header.rs b/codex-rs/tui/src/chatwidget/session_header.rs
deleted file mode 100644
index 32e31b66..00000000
--- a/codex-rs/tui/src/chatwidget/session_header.rs
+++ /dev/null
@@ -1,16 +0,0 @@
-pub(crate) struct SessionHeader {
-    model: String,
-}
-
-impl SessionHeader {
-    pub(crate) fn new(model: String) -> Self {
-        Self { model }
-    }
-
-    /// Updates the header's model text.
-    pub(crate) fn set_model(&mut self, model: &str) {
-        if self.model != model {
-            self.model = model.to_string();
-        }
-    }
-}
diff --git a/codex-rs/tui/src/chatwidget/tests.rs b/codex-rs/tui/src/chatwidget/tests.rs
index 6c9b78e3..711a5073 100644
--- a/codex-rs/tui/src/chatwidget/tests.rs
+++ b/codex-rs/tui/src/chatwidget/tests.rs
@@ -5,6 +5,7 @@ use crate::test_backend::VT100Backend;
 use crate::tui::FrameRequester;
 use assert_matches::assert_matches;
 use codex_common::approval_presets::builtin_approval_presets;
+use codex_common::model_presets::builtin_model_presets;
 use codex_core::AuthManager;
 use codex_core::CodexAuth;
 use codex_core::config::Config;
@@ -245,6 +246,9 @@ async fn helpers_are_available_and_do_not_panic() {
         enhanced_keys_supported: false,
         auth_manager,
         feedback: codex_feedback::CodexFeedback::new(),
+        model_presets: builtin_model_presets(None),
+        auto_open_model_selector: false,
+        litellm_model_missing: false,
     };
     let mut w = ChatWidget::new(init, conversation_manager);
     // Basic construction sanity.
@@ -301,6 +305,11 @@ fn make_chatwidget_manual() -> (
         last_rendered_width: std::cell::Cell::new(None),
         feedback: codex_feedback::CodexFeedback::new(),
         current_rollout_path: None,
+        model_presets: builtin_model_presets(None),
+        auto_open_model_selector: false,
+        litellm_model_missing: false,
+        litellm_model_missing_notified: false,
+        last_reasoning_delta: None,
     };
     (widget, rx, op_rx)
 }
diff --git a/codex-rs/tui/src/history_cell.rs b/codex-rs/tui/src/history_cell.rs
index 73d61dc4..18172ce3 100644
--- a/codex-rs/tui/src/history_cell.rs
+++ b/codex-rs/tui/src/history_cell.rs
@@ -556,14 +556,6 @@ pub(crate) fn new_session_info(
         rollout_path: _,
     } = event;
     if is_first_event {
-        // Header box rendered as history (so it appears at the very top)
-        let header = SessionHeaderHistoryCell::new(
-            model,
-            reasoning_effort,
-            config.cwd.clone(),
-            crate::version::CODEX_CLI_VERSION,
-        );
-
         // Help lines below the header (new copy and list)
         let help_lines: Vec<Line<'static>> = vec![
             "  To get started, describe a task or try one of these commands:"
@@ -597,6 +589,13 @@ pub(crate) fn new_session_info(
             ]),
         ];
 
+        let header = SessionHeaderHistoryCell::new(
+            model,
+            reasoning_effort,
+            config.cwd.clone(),
+            codex_common::litellm::decorate_version(crate::version::CODEX_CLI_VERSION),
+        );
+
         CompositeHistoryCell {
             parts: vec![
                 Box::new(header),
@@ -623,7 +622,7 @@ pub(crate) fn new_user_prompt(message: String) -> UserHistoryCell {
 
 #[derive(Debug)]
 struct SessionHeaderHistoryCell {
-    version: &'static str,
+    version: String,
     model: String,
     reasoning_effort: Option<ReasoningEffortConfig>,
     directory: PathBuf,
@@ -634,7 +633,7 @@ impl SessionHeaderHistoryCell {
         model: String,
         reasoning_effort: Option<ReasoningEffortConfig>,
         directory: PathBuf,
-        version: &'static str,
+        version: String,
     ) -> Self {
         Self {
             version,
diff --git a/codex-rs/tui/src/lib.rs b/codex-rs/tui/src/lib.rs
index eb5d3ae9..d7f59c64 100644
--- a/codex-rs/tui/src/lib.rs
+++ b/codex-rs/tui/src/lib.rs
@@ -7,6 +7,7 @@ use additional_dirs::add_dir_warning_message;
 use app::App;
 pub use app::AppExitInfo;
 use codex_app_server_protocol::AuthMode;
+use codex_common::model_presets::ModelPreset;
 use codex_core::AuthManager;
 use codex_core::BUILT_IN_OSS_MODEL_PROVIDER_ID;
 use codex_core::CodexAuth;
@@ -332,6 +333,9 @@ async fn run_ratatui_app(
         should_show_windows_wsl_screen,
     );
 
+    let mut litellm_credentials_updated = false;
+    let mut litellm_model_selected = false;
+
     let config = if should_show_onboarding {
         let onboarding_result = run_onboarding_app(
             OnboardingScreenArgs {
@@ -358,12 +362,17 @@ async fn run_ratatui_app(
                 update_action: None,
             });
         }
+        litellm_credentials_updated = onboarding_result.litellm_credentials_updated;
+        litellm_model_selected = onboarding_result.litellm_model_selected;
         // if the user acknowledged windows or made an explicit decision ato trust the directory, reload the config accordingly
+        let reload_for_litellm = onboarding_result.litellm_credentials_updated
+            || onboarding_result.litellm_model_selected;
         if should_show_windows_wsl_screen
             || onboarding_result
                 .directory_trust_decision
                 .map(|d| d == TrustDirectorySelection::Trust)
                 .unwrap_or(false)
+            || reload_for_litellm
         {
             load_config_or_exit(cli_kv_overrides, overrides).await
         } else {
@@ -373,6 +382,16 @@ async fn run_ratatui_app(
         initial_config
     };
 
+    let auth_mode = auth_manager.auth().map(|auth| auth.mode);
+    let model_presets: Vec<ModelPreset> = codex_common::model_presets::presets_for_provider(
+        &config.model_provider_id,
+        &config.model,
+        auth_mode,
+    );
+    let auto_open_model_selector =
+        !litellm_model_selected && (litellm_credentials_updated || config.litellm_setup_required);
+    let litellm_model_missing = false;
+
     // Determine resume behavior: explicit id, then resume last, then picker.
     let resume_selection = if let Some(id_str) = cli.resume_session_id.as_deref() {
         match find_conversation_path_by_id_str(&config.codex_home, id_str).await? {
@@ -448,6 +467,9 @@ async fn run_ratatui_app(
         images,
         resume_selection,
         feedback,
+        model_presets,
+        auto_open_model_selector,
+        litellm_model_missing,
     )
     .await;
 
@@ -544,6 +566,9 @@ fn should_show_onboarding(
 fn should_show_login_screen(login_status: LoginStatus, config: &Config) -> bool {
     // Only show the login screen for providers that actually require OpenAI auth
     // (OpenAI or equivalents). For OSS/other providers, skip login entirely.
+    if config.litellm_setup_required {
+        return true;
+    }
     if !config.model_provider.requires_openai_auth {
         return false;
     }
diff --git a/codex-rs/tui/src/onboarding/auth.rs b/codex-rs/tui/src/onboarding/auth.rs
index 56527ac8..c4408915 100644
--- a/codex-rs/tui/src/onboarding/auth.rs
+++ b/codex-rs/tui/src/onboarding/auth.rs
@@ -1,15 +1,18 @@
 #![allow(clippy::unwrap_used)]
 
+use codex_common::litellm::{LITELLM_API_KEY_ENV, LITELLM_BASE_URL_ENV};
 use codex_core::AuthManager;
 use codex_core::auth::AuthCredentialsStoreMode;
 use codex_core::auth::CLIENT_ID;
-use codex_core::auth::login_with_api_key;
-use codex_core::auth::read_openai_api_key_from_env;
+use codex_core::config::{
+    LiteLlmProviderUpdate, read_litellm_provider_state, write_litellm_provider_state,
+};
 use codex_login::ServerOptions;
 use codex_login::ShutdownHandle;
 use codex_login::run_login_server;
 use crossterm::event::KeyCode;
 use crossterm::event::KeyEvent;
+use crossterm::event::KeyEventKind;
 use crossterm::event::KeyModifiers;
 use ratatui::buffer::Buffer;
 use ratatui::layout::Constraint;
@@ -30,6 +33,7 @@ use ratatui::widgets::Wrap;
 
 use codex_app_server_protocol::AuthMode;
 use codex_protocol::config_types::ForcedLoginMethod;
+use std::env;
 use std::sync::RwLock;
 
 use crate::LoginStatus;
@@ -45,19 +49,32 @@ use super::onboarding_screen::StepState;
 #[derive(Clone)]
 pub(crate) enum SignInState {
     PickMode,
+    #[allow(dead_code)]
     ChatGptContinueInBrowser(ContinueInBrowserState),
+    #[allow(dead_code)]
     ChatGptSuccessMessage,
+    #[allow(dead_code)]
     ChatGptSuccess,
     ApiKeyEntry(ApiKeyInputState),
     ApiKeyConfigured,
 }
 
-const API_KEY_DISABLED_MESSAGE: &str = "API key login is disabled.";
+const API_KEY_DISABLED_MESSAGE: &str = "LiteLLM configuration is disabled.";
 
 #[derive(Clone, Default)]
 pub(crate) struct ApiKeyInputState {
-    value: String,
-    prepopulated_from_env: bool,
+    base_url: String,
+    api_key: String,
+    stage: ApiKeyEntryStage,
+    base_url_prepopulated: bool,
+    api_key_prepopulated: bool,
+}
+
+#[derive(Clone, Copy, Default, PartialEq, Eq)]
+enum ApiKeyEntryStage {
+    #[default]
+    BaseUrl,
+    ApiKey,
 }
 
 #[derive(Clone)]
@@ -77,54 +94,26 @@ impl Drop for ContinueInBrowserState {
 
 impl KeyboardHandler for AuthModeWidget {
     fn handle_key_event(&mut self, key_event: KeyEvent) {
+        if key_event.kind != KeyEventKind::Press && key_event.kind != KeyEventKind::Repeat {
+            return;
+        }
         if self.handle_api_key_entry_key_event(&key_event) {
             return;
         }
 
         match key_event.code {
-            KeyCode::Up | KeyCode::Char('k') => {
-                if self.is_chatgpt_login_allowed() {
-                    self.highlighted_mode = AuthMode::ChatGPT;
-                }
-            }
-            KeyCode::Down | KeyCode::Char('j') => {
+            KeyCode::Up | KeyCode::Char('k') | KeyCode::Down | KeyCode::Char('j') => {
                 if self.is_api_login_allowed() {
                     self.highlighted_mode = AuthMode::ApiKey;
                 }
             }
-            KeyCode::Char('1') => {
-                if self.is_chatgpt_login_allowed() {
-                    self.start_chatgpt_login();
-                }
-            }
-            KeyCode::Char('2') => {
+            KeyCode::Char('1') | KeyCode::Char('2') | KeyCode::Enter => {
                 if self.is_api_login_allowed() {
                     self.start_api_key_entry();
                 } else {
                     self.disallow_api_login();
                 }
             }
-            KeyCode::Enter => {
-                let sign_in_state = { (*self.sign_in_state.read().unwrap()).clone() };
-                match sign_in_state {
-                    SignInState::PickMode => match self.highlighted_mode {
-                        AuthMode::ChatGPT if self.is_chatgpt_login_allowed() => {
-                            self.start_chatgpt_login();
-                        }
-                        AuthMode::ApiKey if self.is_api_login_allowed() => {
-                            self.start_api_key_entry();
-                        }
-                        AuthMode::ChatGPT => {}
-                        AuthMode::ApiKey => {
-                            self.disallow_api_login();
-                        }
-                    },
-                    SignInState::ChatGptSuccessMessage => {
-                        *self.sign_in_state.write().unwrap() = SignInState::ChatGptSuccess;
-                    }
-                    _ => {}
-                }
-            }
             KeyCode::Esc => {
                 tracing::info!("Esc pressed");
                 let sign_in_state = { (*self.sign_in_state.read().unwrap()).clone() };
@@ -149,11 +138,13 @@ pub(crate) struct AuthModeWidget {
     pub error: Option<String>,
     pub sign_in_state: Arc<RwLock<SignInState>>,
     pub codex_home: PathBuf,
-    pub cli_auth_credentials_store_mode: AuthCredentialsStoreMode,
     pub login_status: LoginStatus,
+    #[allow(dead_code)]
     pub auth_manager: Arc<AuthManager>,
+    #[allow(dead_code)]
     pub forced_chatgpt_workspace_id: Option<String>,
     pub forced_login_method: Option<ForcedLoginMethod>,
+    pub cli_auth_credentials_store_mode: AuthCredentialsStoreMode,
 }
 
 impl AuthModeWidget {
@@ -161,12 +152,13 @@ impl AuthModeWidget {
         !matches!(self.forced_login_method, Some(ForcedLoginMethod::Chatgpt))
     }
 
+    #[allow(dead_code)]
     fn is_chatgpt_login_allowed(&self) -> bool {
         !matches!(self.forced_login_method, Some(ForcedLoginMethod::Api))
     }
 
     fn disallow_api_login(&mut self) {
-        self.highlighted_mode = AuthMode::ChatGPT;
+        self.highlighted_mode = AuthMode::ApiKey;
         self.error = Some(API_KEY_DISABLED_MESSAGE.to_string());
         *self.sign_in_state.write().unwrap() = SignInState::PickMode;
         self.request_frame.schedule_frame();
@@ -176,11 +168,11 @@ impl AuthModeWidget {
         let mut lines: Vec<Line> = vec![
             Line::from(vec![
                 "  ".into(),
-                "Sign in with ChatGPT to use Codex as part of your paid plan".into(),
+                "codex-litellm connects directly to your LiteLLM backend.".into(),
             ]),
             Line::from(vec![
                 "  ".into(),
-                "or connect an API key for usage-based billing".into(),
+                "Provide the endpoint URL and API key to configure this workspace.".into(),
             ]),
             "".into(),
         ];
@@ -214,39 +206,23 @@ impl AuthModeWidget {
             vec![line1, line2]
         };
 
-        let chatgpt_description = if self.is_chatgpt_login_allowed() {
-            "Usage included with Plus, Pro, and Team plans"
-        } else {
-            "ChatGPT login is disabled"
-        };
-        lines.extend(create_mode_item(
-            0,
-            AuthMode::ChatGPT,
-            "Sign in with ChatGPT",
-            chatgpt_description,
-        ));
-        lines.push("".into());
         if self.is_api_login_allowed() {
             lines.extend(create_mode_item(
-                1,
+                0,
                 AuthMode::ApiKey,
-                "Provide your own API key",
-                "Pay for what you use",
+                "Configure LiteLLM credentials",
+                "Paste the LiteLLM endpoint URL and API key (env: LITELLM_BASE_URL / LITELLM_API_KEY)",
             ));
             lines.push("".into());
         } else {
             lines.push(
-                "  API key login is disabled by this workspace. Sign in with ChatGPT to continue."
+                "  LiteLLM configuration is disabled for this workspace. Ask your admin to enable API key login."
                     .dim()
                     .into(),
             );
             lines.push("".into());
         }
-        lines.push(
-            // AE: Following styles.md, this should probably be Cyan because it's a user input tip.
-            //     But leaving this for a future cleanup.
-            "  Press Enter to continue".dim().into(),
-        );
+        lines.push("  Press Enter to continue".dim().into());
         if let Some(err) = &self.error {
             lines.push("".into());
             lines.push(err.as_str().red().into());
@@ -326,9 +302,9 @@ impl AuthModeWidget {
 
     fn render_api_key_configured(&self, area: Rect, buf: &mut Buffer) {
         let lines = vec![
-            " API key configured".fg(Color::Green).into(),
+            " LiteLLM credentials configured".fg(Color::Green).into(),
             "".into(),
-            "  Codex will use usage-based billing with your API key.".into(),
+            "  codex-litellm will reuse these settings for future sessions.".into(),
         ];
 
         Paragraph::new(lines)
@@ -344,48 +320,83 @@ impl AuthModeWidget {
         ])
         .areas(area);
 
-        let mut intro_lines: Vec<Line> = vec![
-            Line::from(vec![
-                "> ".into(),
-                "Use your own OpenAI API key for usage-based billing".bold(),
-            ]),
-            "".into(),
-            "  Paste or type your API key below. It will be stored locally in auth.json.".into(),
-            "".into(),
-        ];
-        if state.prepopulated_from_env {
-            intro_lines.push("  Detected OPENAI_API_KEY environment variable.".into());
-            intro_lines.push(
-                "  Paste a different key if you prefer to use another account."
-                    .dim()
-                    .into(),
-            );
-            intro_lines.push("".into());
-        }
+        let (title, placeholder, value, intro_lines) = match state.stage {
+            ApiKeyEntryStage::BaseUrl => {
+                let mut intro_lines: Vec<Line> = vec![
+                    Line::from(vec![
+                        "> ".into(),
+                        "Step 1 of 2: LiteLLM endpoint URL".bold(),
+                    ]),
+                    "".into(),
+                    "  Paste the base URL of your LiteLLM server (for example, https://litellm.example.com/v1)."
+                        .into(),
+                    "".into(),
+                ];
+                if state.base_url_prepopulated {
+                    intro_lines.push(format!("  Prefilled from ${LITELLM_BASE_URL_ENV}.").into());
+                    intro_lines.push("  Press Enter to accept or type to override.".dim().into());
+                    intro_lines.push("".into());
+                }
+                (
+                    "LiteLLM endpoint",
+                    "Paste or type the LiteLLM endpoint URL".to_string(),
+                    state.base_url.clone(),
+                    intro_lines,
+                )
+            }
+            ApiKeyEntryStage::ApiKey => {
+                let mut intro_lines: Vec<Line> = vec![
+                    Line::from(vec!["> ".into(), "Step 2 of 2: LiteLLM API key".bold()]),
+                    "".into(),
+                    "  Paste the API token used by your LiteLLM deployment.".into(),
+                    "  Stored locally in config.toml as a Bearer token."
+                        .dim()
+                        .into(),
+                    "".into(),
+                ];
+                if state.api_key_prepopulated {
+                    intro_lines.push(format!("  Prefilled from ${LITELLM_API_KEY_ENV}.").into());
+                    intro_lines.push("  Press Enter to accept or type to override.".dim().into());
+                    intro_lines.push("".into());
+                }
+                (
+                    "LiteLLM API key",
+                    "Paste or type your LiteLLM API key".to_string(),
+                    state.api_key.clone(),
+                    intro_lines,
+                )
+            }
+        };
         Paragraph::new(intro_lines)
             .wrap(Wrap { trim: false })
             .render(intro_area, buf);
 
-        let content_line: Line = if state.value.is_empty() {
-            vec!["Paste or type your API key".dim()].into()
+        let content_line: Line = if value.trim().is_empty() {
+            vec![placeholder.dim()].into()
         } else {
-            Line::from(state.value.clone())
+            Line::from(value)
         };
         Paragraph::new(content_line)
             .wrap(Wrap { trim: false })
             .block(
                 Block::default()
-                    .title("API key")
+                    .title(title)
                     .borders(Borders::ALL)
                     .border_type(BorderType::Rounded)
                     .border_style(Style::default().fg(Color::Cyan)),
             )
             .render(input_area, buf);
 
-        let mut footer_lines: Vec<Line> = vec![
-            "  Press Enter to save".dim().into(),
-            "  Press Esc to go back".dim().into(),
-        ];
+        let mut footer_lines: Vec<Line> = match state.stage {
+            ApiKeyEntryStage::BaseUrl => vec![
+                "  Press Enter to continue".dim().into(),
+                "  Press Esc to cancel".dim().into(),
+            ],
+            ApiKeyEntryStage::ApiKey => vec![
+                "  Press Enter to save".dim().into(),
+                "  Press Esc to go back".dim().into(),
+            ],
+        };
         if let Some(error) = &self.error {
             footer_lines.push("".into());
             footer_lines.push(error.as_str().red().into());
@@ -396,7 +407,10 @@ impl AuthModeWidget {
     }
 
     fn handle_api_key_entry_key_event(&mut self, key_event: &KeyEvent) -> bool {
-        let mut should_save: Option<String> = None;
+        if key_event.kind != KeyEventKind::Press && key_event.kind != KeyEventKind::Repeat {
+            return false;
+        }
+        let mut should_save: Option<(String, String)> = None;
         let mut should_request_frame = false;
 
         {
@@ -404,25 +418,59 @@ impl AuthModeWidget {
             if let SignInState::ApiKeyEntry(state) = &mut *guard {
                 match key_event.code {
                     KeyCode::Esc => {
-                        *guard = SignInState::PickMode;
+                        match state.stage {
+                            ApiKeyEntryStage::BaseUrl => {
+                                *guard = SignInState::PickMode;
+                            }
+                            ApiKeyEntryStage::ApiKey => {
+                                state.stage = ApiKeyEntryStage::BaseUrl;
+                            }
+                        }
                         self.error = None;
                         should_request_frame = true;
                     }
-                    KeyCode::Enter => {
-                        let trimmed = state.value.trim().to_string();
-                        if trimmed.is_empty() {
-                            self.error = Some("API key cannot be empty".to_string());
-                            should_request_frame = true;
-                        } else {
-                            should_save = Some(trimmed);
+                    KeyCode::Enter => match state.stage {
+                        ApiKeyEntryStage::BaseUrl => {
+                            let trimmed = state.base_url.trim().to_string();
+                            if trimmed.is_empty() {
+                                self.error =
+                                    Some("LiteLLM endpoint URL cannot be empty".to_string());
+                                should_request_frame = true;
+                            } else {
+                                state.base_url = trimmed;
+                                state.stage = ApiKeyEntryStage::ApiKey;
+                                self.error = None;
+                                should_request_frame = true;
+                            }
                         }
-                    }
+                        ApiKeyEntryStage::ApiKey => {
+                            let trimmed = state.api_key.trim().to_string();
+                            if trimmed.is_empty() {
+                                self.error = Some("LiteLLM API key cannot be empty".to_string());
+                                should_request_frame = true;
+                            } else {
+                                should_save = Some((state.base_url.clone(), trimmed));
+                            }
+                        }
+                    },
                     KeyCode::Backspace => {
-                        if state.prepopulated_from_env {
-                            state.value.clear();
-                            state.prepopulated_from_env = false;
-                        } else {
-                            state.value.pop();
+                        match state.stage {
+                            ApiKeyEntryStage::BaseUrl => {
+                                if state.base_url_prepopulated {
+                                    state.base_url.clear();
+                                    state.base_url_prepopulated = false;
+                                } else {
+                                    state.base_url.pop();
+                                }
+                            }
+                            ApiKeyEntryStage::ApiKey => {
+                                if state.api_key_prepopulated {
+                                    state.api_key.clear();
+                                    state.api_key_prepopulated = false;
+                                } else {
+                                    state.api_key.pop();
+                                }
+                            }
                         }
                         self.error = None;
                         should_request_frame = true;
@@ -431,24 +479,34 @@ impl AuthModeWidget {
                         if !key_event.modifiers.contains(KeyModifiers::CONTROL)
                             && !key_event.modifiers.contains(KeyModifiers::ALT) =>
                     {
-                        if state.prepopulated_from_env {
-                            state.value.clear();
-                            state.prepopulated_from_env = false;
+                        match state.stage {
+                            ApiKeyEntryStage::BaseUrl => {
+                                if state.base_url_prepopulated {
+                                    state.base_url.clear();
+                                    state.base_url_prepopulated = false;
+                                }
+                                state.base_url.push(c);
+                            }
+                            ApiKeyEntryStage::ApiKey => {
+                                if state.api_key_prepopulated {
+                                    state.api_key.clear();
+                                    state.api_key_prepopulated = false;
+                                }
+                                state.api_key.push(c);
+                            }
                         }
-                        state.value.push(c);
                         self.error = None;
                         should_request_frame = true;
                     }
                     _ => {}
                 }
-                // handled; let guard drop before potential save
             } else {
                 return false;
             }
         }
 
-        if let Some(api_key) = should_save {
-            self.save_api_key(api_key);
+        if let Some((base_url, api_key)) = should_save {
+            self.save_litellm_credentials(base_url, api_key);
         } else if should_request_frame {
             self.request_frame.schedule_frame();
         }
@@ -463,11 +521,22 @@ impl AuthModeWidget {
 
         let mut guard = self.sign_in_state.write().unwrap();
         if let SignInState::ApiKeyEntry(state) = &mut *guard {
-            if state.prepopulated_from_env {
-                state.value = trimmed.to_string();
-                state.prepopulated_from_env = false;
-            } else {
-                state.value.push_str(trimmed);
+            match state.stage {
+                ApiKeyEntryStage::BaseUrl => {
+                    if state.base_url_prepopulated {
+                        state.base_url.clear();
+                        state.base_url_prepopulated = false;
+                    }
+                    state.base_url.push_str(trimmed);
+                }
+                ApiKeyEntryStage::ApiKey => {
+                    if state.api_key_prepopulated {
+                        state.api_key = trimmed.to_string();
+                        state.api_key_prepopulated = false;
+                    } else {
+                        state.api_key.push_str(trimmed);
+                    }
+                }
             }
             self.error = None;
         } else {
@@ -479,72 +548,100 @@ impl AuthModeWidget {
         true
     }
 
-    fn start_api_key_entry(&mut self) {
+    pub(crate) fn start_api_key_entry(&mut self) {
         if !self.is_api_login_allowed() {
             self.disallow_api_login();
             return;
         }
         self.error = None;
-        let prefill_from_env = read_openai_api_key_from_env();
-        let mut guard = self.sign_in_state.write().unwrap();
-        match &mut *guard {
-            SignInState::ApiKeyEntry(state) => {
-                if state.value.is_empty() {
-                    if let Some(prefill) = prefill_from_env {
-                        state.value = prefill;
-                        state.prepopulated_from_env = true;
-                    } else {
-                        state.prepopulated_from_env = false;
-                    }
-                }
-            }
-            _ => {
-                *guard = SignInState::ApiKeyEntry(ApiKeyInputState {
-                    value: prefill_from_env.clone().unwrap_or_default(),
-                    prepopulated_from_env: prefill_from_env.is_some(),
-                });
+        let base_url_env = read_env_var(LITELLM_BASE_URL_ENV);
+        let api_key_env = read_env_var(LITELLM_API_KEY_ENV);
+
+        let provider_state = match read_litellm_provider_state(&self.codex_home) {
+            Ok(state) => state,
+            Err(err) => {
+                self.error = Some(format!("Failed to read LiteLLM configuration: {err}"));
+                self.request_frame.schedule_frame();
+                return;
             }
-        }
+        };
+
+        let (base_url, base_url_prepopulated) =
+            match provider_state.base_url.filter(|v| !v.trim().is_empty()) {
+                Some(existing) => (existing, false),
+                None => match base_url_env {
+                    Some(env_value) => (env_value, true),
+                    None => (String::new(), false),
+                },
+            };
+
+        let (api_key, api_key_prepopulated) =
+            match provider_state.api_key.filter(|v| !v.trim().is_empty()) {
+                Some(existing) => (strip_bearer(existing), false),
+                None => match api_key_env {
+                    Some(env_value) => (env_value, true),
+                    None => (String::new(), false),
+                },
+            };
+
+        let stage = if base_url.trim().is_empty() {
+            ApiKeyEntryStage::BaseUrl
+        } else {
+            ApiKeyEntryStage::ApiKey
+        };
+
+        let mut guard = self.sign_in_state.write().unwrap();
+        *guard = SignInState::ApiKeyEntry(ApiKeyInputState {
+            base_url,
+            api_key,
+            stage,
+            base_url_prepopulated,
+            api_key_prepopulated,
+        });
         drop(guard);
         self.request_frame.schedule_frame();
     }
 
-    fn save_api_key(&mut self, api_key: String) {
+    fn save_litellm_credentials(&mut self, base_url: String, api_key: String) {
         if !self.is_api_login_allowed() {
             self.disallow_api_login();
             return;
         }
-        match login_with_api_key(
-            &self.codex_home,
-            &api_key,
-            self.cli_auth_credentials_store_mode,
-        ) {
+        let trimmed_url = base_url.trim().to_string();
+        let trimmed_key = api_key.trim().to_string();
+        if trimmed_url.is_empty() {
+            self.error = Some("LiteLLM endpoint URL cannot be empty".to_string());
+            self.request_frame.schedule_frame();
+            return;
+        }
+        if trimmed_key.is_empty() {
+            self.error = Some("LiteLLM API key cannot be empty".to_string());
+            self.request_frame.schedule_frame();
+            return;
+        }
+
+        let update = LiteLlmProviderUpdate {
+            base_url: Some(trimmed_url),
+            api_key: Some(format_bearer(&trimmed_key)),
+        };
+
+        match write_litellm_provider_state(&self.codex_home, update) {
             Ok(()) => {
                 self.error = None;
-                self.login_status = LoginStatus::AuthMode(AuthMode::ApiKey);
+                self.login_status = LoginStatus::NotAuthenticated;
                 self.auth_manager.reload();
                 *self.sign_in_state.write().unwrap() = SignInState::ApiKeyConfigured;
             }
             Err(err) => {
-                self.error = Some(format!("Failed to save API key: {err}"));
-                let mut guard = self.sign_in_state.write().unwrap();
-                if let SignInState::ApiKeyEntry(existing) = &mut *guard {
-                    if existing.value.is_empty() {
-                        existing.value.push_str(&api_key);
-                    }
-                    existing.prepopulated_from_env = false;
-                } else {
-                    *guard = SignInState::ApiKeyEntry(ApiKeyInputState {
-                        value: api_key,
-                        prepopulated_from_env: false,
-                    });
-                }
+                self.error = Some(format!("Failed to write LiteLLM configuration: {err}"));
+                // fall through to reshow the form with existing input
             }
         }
 
         self.request_frame.schedule_frame();
     }
 
+    #[allow(dead_code)]
     fn start_chatgpt_login(&mut self) {
         // If we're already authenticated with ChatGPT, don't start a new login 
         // just proceed to the success message flow.
@@ -587,7 +684,6 @@ impl AuthModeWidget {
                         }
                         _ => {
                             *sign_in_state.write().unwrap() = SignInState::PickMode;
-                            // self.error = Some(e.to_string());
                             request_frame.schedule_frame();
                         }
                     }
@@ -641,14 +737,43 @@ impl WidgetRef for AuthModeWidget {
     }
 }
 
+fn read_env_var(name: &str) -> Option<String> {
+    env::var(name)
+        .ok()
+        .map(|value| value.trim().to_string())
+        .filter(|value| !value.is_empty())
+}
+
+fn strip_bearer(value: String) -> String {
+    let trimmed = value.trim();
+    if let Some(rest) = trimmed.strip_prefix("Bearer") {
+        rest.trim_start().to_string()
+    } else if let Some(rest) = trimmed.strip_prefix("bearer") {
+        rest.trim_start().to_string()
+    } else {
+        trimmed.to_string()
+    }
+}
+
+fn format_bearer(token: &str) -> String {
+    if token.len() >= 6 && token[..6].eq_ignore_ascii_case("bearer") {
+        let rest = token[6..].trim_start();
+        if rest.is_empty() {
+            "Bearer".to_string()
+        } else {
+            format!("Bearer {rest}")
+        }
+    } else {
+        format!("Bearer {token}")
+    }
+}
+
 #[cfg(test)]
 mod tests {
     use super::*;
     use pretty_assertions::assert_eq;
     use tempfile::TempDir;
 
-    use codex_core::auth::AuthCredentialsStoreMode;
-
     fn widget_forced_chatgpt() -> (AuthModeWidget, TempDir) {
         let codex_home = TempDir::new().unwrap();
         let codex_home_path = codex_home.path().to_path_buf();
@@ -658,13 +783,8 @@ mod tests {
             error: None,
             sign_in_state: Arc::new(RwLock::new(SignInState::PickMode)),
             codex_home: codex_home_path.clone(),
-            cli_auth_credentials_store_mode: AuthCredentialsStoreMode::File,
             login_status: LoginStatus::NotAuthenticated,
-            auth_manager: AuthManager::shared(
-                codex_home_path,
-                false,
-                AuthCredentialsStoreMode::File,
-            ),
+            auth_manager: AuthManager::shared(codex_home_path, false),
             forced_chatgpt_workspace_id: None,
             forced_login_method: Some(ForcedLoginMethod::Chatgpt),
         };
@@ -688,7 +808,8 @@ mod tests {
     fn saving_api_key_is_blocked_when_chatgpt_forced() {
         let (mut widget, _tmp) = widget_forced_chatgpt();
 
-        widget.save_api_key("sk-test".to_string());
+        widget
+            .save_litellm_credentials("https://example.com/v1".to_string(), "sk-test".to_string());
 
         assert_eq!(widget.error.as_deref(), Some(API_KEY_DISABLED_MESSAGE));
         assert!(matches!(
diff --git a/codex-rs/tui/src/onboarding/mod.rs b/codex-rs/tui/src/onboarding/mod.rs
index 6c420dae..23d79a96 100644
--- a/codex-rs/tui/src/onboarding/mod.rs
+++ b/codex-rs/tui/src/onboarding/mod.rs
@@ -1,4 +1,5 @@
 mod auth;
+pub mod model;
 pub mod onboarding_screen;
 mod trust_directory;
 pub use trust_directory::TrustDirectorySelection;
diff --git a/codex-rs/tui/src/onboarding/model.rs b/codex-rs/tui/src/onboarding/model.rs
new file mode 100644
index 00000000..5c6762cd
--- /dev/null
+++ b/codex-rs/tui/src/onboarding/model.rs
@@ -0,0 +1,806 @@
+use codex_common::model_presets::{
+    ModelPreset, builtin_model_presets, fetch_litellm_model_presets, is_litellm_provider_id,
+    presets_for_provider,
+};
+use codex_core::config::Config;
+use codex_core::config::edit::{ConfigEdit, apply_blocking};
+use codex_core::config::read_litellm_provider_state;
+use codex_core::protocol_config_types::ReasoningEffort;
+use crossterm::event::{KeyCode, KeyEvent, KeyEventKind};
+use ratatui::buffer::Buffer;
+use ratatui::layout::{Constraint, Layout, Rect};
+use ratatui::style::{Color, Modifier, Style, Stylize};
+use ratatui::text::{Line, Span};
+use ratatui::widgets::{Block, Borders, Clear, Paragraph, Widget, WidgetRef, Wrap};
+use std::cell::Cell;
+
+use super::onboarding_screen::{KeyboardHandler, StepState, StepStateProvider};
+use crate::tui::FrameRequester;
+
+const FALLBACK_LITELLM_MODEL: &str = "gpt-oss-120b-litellm";
+
+#[derive(Clone)]
+pub(crate) struct ModelSelectionResult {
+    pub model: String,
+    pub reasoning_effort: Option<ReasoningEffort>,
+}
+
+#[derive(Clone)]
+struct ReasoningOption {
+    effort: Option<ReasoningEffort>,
+    label: String,
+    description: Option<String>,
+    selected_description: Option<String>,
+}
+
+struct ReasoningStageState {
+    preset_index: usize,
+    options: Vec<ReasoningOption>,
+    selected_index: usize,
+    scroll_offset: Cell<usize>,
+    visible_capacity: Cell<usize>,
+}
+
+impl ReasoningStageState {
+    fn new(preset_index: usize, options: Vec<ReasoningOption>, selected_index: usize) -> Self {
+        Self {
+            preset_index,
+            options,
+            selected_index,
+            scroll_offset: Cell::new(0),
+            visible_capacity: Cell::new(0),
+        }
+    }
+}
+
+enum SelectionStage {
+    ModelList,
+    Reasoning(ReasoningStageState),
+}
+
+enum ModelStepStatus {
+    Hidden,
+    AwaitingCredentials,
+    Loading,
+    Ready,
+    Saving,
+    Complete,
+    Error(String),
+}
+
+pub(crate) struct ModelSelectionWidget {
+    request_frame: FrameRequester,
+    config: Config,
+    presets: Vec<ModelPreset>,
+    selected_index: usize,
+    status: ModelStepStatus,
+    selection_result: Option<ModelSelectionResult>,
+    stage: SelectionStage,
+    scroll_offset: Cell<usize>,
+    visible_capacity: Cell<usize>,
+}
+
+impl ModelSelectionWidget {
+    pub(crate) fn new(request_frame: FrameRequester, config: Config) -> Self {
+        let mut widget = Self {
+            request_frame,
+            config,
+            presets: Vec::new(),
+            selected_index: 0,
+            status: ModelStepStatus::Hidden,
+            selection_result: None,
+            stage: SelectionStage::ModelList,
+            scroll_offset: Cell::new(0),
+            visible_capacity: Cell::new(1),
+        };
+
+        if is_litellm_provider_id(&widget.config.model_provider_id) {
+            if widget.config.litellm_setup_required {
+                widget.status = ModelStepStatus::AwaitingCredentials;
+            } else {
+                widget.load_presets();
+            }
+        }
+
+        widget
+    }
+
+    pub(crate) fn credentials_ready(&mut self) {
+        if matches!(self.status, ModelStepStatus::AwaitingCredentials) {
+            self.load_presets();
+        }
+    }
+
+    pub(crate) fn selection(&self) -> Option<ModelSelectionResult> {
+        self.selection_result.clone()
+    }
+
+    fn load_presets(&mut self) {
+        self.status = ModelStepStatus::Loading;
+        #[cfg(debug_assertions)]
+        tracing::debug!(
+            provider = %self.config.model_provider_id,
+            "model_onboarding.load_presets:start"
+        );
+        self.request_frame.schedule_frame();
+
+        match self.fetch_presets() {
+            Ok(presets) if !presets.is_empty() => {
+                self.apply_loaded_presets(presets);
+                #[cfg(debug_assertions)]
+                tracing::debug!(
+                    provider = %self.config.model_provider_id,
+                    count = self.presets.len(),
+                    "model_onboarding.load_presets:ready"
+                );
+            }
+            Ok(_) => {
+                self.status = ModelStepStatus::Error(
+                    "LiteLLM returned no models. Use /model later to configure.".to_string(),
+                );
+                #[cfg(debug_assertions)]
+                tracing::debug!("model_onboarding.load_presets:empty");
+            }
+            Err(err) => {
+                #[cfg(debug_assertions)]
+                tracing::debug!(error = %err, "model_onboarding.load_presets:error");
+                self.status = ModelStepStatus::Error(err);
+            }
+        }
+        self.request_frame.schedule_frame();
+    }
+
+    fn apply_loaded_presets(&mut self, mut presets: Vec<ModelPreset>) {
+        if presets.len() > 1 && presets[0].model == self.config.model {
+            let fallback = presets.remove(0);
+            presets.push(fallback);
+        }
+
+        if presets.len() > 1 {
+            presets.retain(|preset| preset.model != FALLBACK_LITELLM_MODEL);
+        }
+
+        for preset in presets.iter_mut() {
+            preset.is_default = false;
+        }
+
+        self.presets = presets;
+        self.selected_index = 0;
+        self.scroll_offset.set(0);
+        self.visible_capacity.set(1);
+        self.stage = SelectionStage::ModelList;
+        self.status = ModelStepStatus::Ready;
+        self.config.litellm_setup_required = false;
+    }
+
+    fn fetch_presets(&self) -> Result<Vec<ModelPreset>, String> {
+        if is_litellm_provider_id(&self.config.model_provider_id) {
+            let state = read_litellm_provider_state(&self.config.codex_home)
+                .map_err(|err| format!("Failed to read LiteLLM credentials: {err}"))?;
+            let base_url = state.base_url.ok_or_else(|| {
+                "LiteLLM endpoint URL is missing. Re-enter it during onboarding.".to_string()
+            })?;
+            return fetch_litellm_model_presets(
+                &base_url,
+                state.api_key.as_deref(),
+                &self.config.model,
+            );
+        }
+
+        let mut presets =
+            presets_for_provider(&self.config.model_provider_id, &self.config.model, None);
+        if presets.is_empty() {
+            presets = builtin_model_presets(None);
+        }
+        if presets.is_empty() {
+            return Err("No models available for the current provider.".to_string());
+        }
+        for preset in presets.iter_mut() {
+            preset.is_default = preset.model == self.config.model;
+        }
+        Ok(presets)
+    }
+
+    fn begin_reasoning_stage(&mut self) {
+        if self.presets.is_empty() {
+            return;
+        }
+        let preset_index = self.selected_index.min(self.presets.len() - 1);
+        let preset = self.presets[preset_index].clone();
+        let options = self.build_reasoning_options(&preset);
+
+        let target_effort = if preset.model == self.config.model {
+            self.config.model_reasoning_effort
+        } else {
+            Some(preset.default_reasoning_effort)
+        };
+        let default_effort = Some(preset.default_reasoning_effort);
+        let selected_index = options
+            .iter()
+            .position(|opt| opt.effort == target_effort)
+            .or_else(|| options.iter().position(|opt| opt.effort == default_effort))
+            .unwrap_or(0);
+
+        self.stage = SelectionStage::Reasoning(ReasoningStageState::new(
+            preset_index,
+            options,
+            selected_index,
+        ));
+        self.request_frame.schedule_frame();
+    }
+
+    fn build_reasoning_options(&self, preset: &ModelPreset) -> Vec<ReasoningOption> {
+        let supported = preset.supported_reasoning_efforts;
+        let default_effort = preset.default_reasoning_effort;
+
+        let mut options: Vec<ReasoningOption> = supported
+            .iter()
+            .map(|entry| {
+                let mut label = entry.effort.to_string();
+                if let Some(first) = label.get_mut(0..1) {
+                    first.make_ascii_uppercase();
+                }
+                if entry.effort == default_effort {
+                    label.push_str(" (default)");
+                }
+
+                let description =
+                    (!entry.description.is_empty()).then(|| entry.description.to_string());
+                let warning =
+                    if preset.model == "gpt-5-codex" && entry.effort == ReasoningEffort::High {
+                        Some(
+                            " High reasoning effort can quickly consume Plus plan rate limits."
+                                .to_string(),
+                        )
+                    } else {
+                        None
+                    };
+
+                ReasoningOption {
+                    effort: Some(entry.effort),
+                    label,
+                    description,
+                    selected_description: warning,
+                }
+            })
+            .collect();
+
+        if options.is_empty() {
+            let mut label = default_effort.to_string();
+            if let Some(first) = label.get_mut(0..1) {
+                first.make_ascii_uppercase();
+            }
+
+            options.push(ReasoningOption {
+                effort: Some(default_effort),
+                label,
+                description: None,
+                selected_description: None,
+            });
+        }
+
+        options
+    }
+
+    fn confirm_reasoning_selection(&mut self) {
+        let selection = match &self.stage {
+            SelectionStage::Reasoning(state) => state
+                .options
+                .get(state.selected_index)
+                .cloned()
+                .map(|option| (state.preset_index, option)),
+            _ => None,
+        };
+
+        if let Some((preset_index, option)) = selection {
+            self.save_selection(preset_index, option.effort);
+        }
+    }
+
+    fn save_selection(&mut self, preset_index: usize, selected_effort: Option<ReasoningEffort>) {
+        let Some(selected_preset) = self.presets.get(preset_index).cloned() else {
+            return;
+        };
+
+        self.status = ModelStepStatus::Saving;
+        self.request_frame.schedule_frame();
+
+        let codex_home = self.config.codex_home.clone();
+        let active_profile = self.config.active_profile.clone();
+        let model = selected_preset.model.clone();
+
+        let apply_result = apply_blocking(
+            &codex_home,
+            active_profile.as_deref(),
+            &[ConfigEdit::SetModel {
+                model: Some(model.clone()),
+                effort: selected_effort,
+            }],
+        );
+
+        match apply_result {
+            Ok(()) => {
+                for (idx, preset) in self.presets.iter_mut().enumerate() {
+                    preset.is_default = idx == preset_index;
+                }
+                self.selected_index = preset_index;
+                self.scroll_offset.set(0);
+                self.visible_capacity.set(1);
+                self.selection_result = Some(ModelSelectionResult {
+                    model: model.clone(),
+                    reasoning_effort: selected_effort,
+                });
+                self.config.model = model;
+                self.config.model_reasoning_effort = selected_effort;
+                self.stage = SelectionStage::ModelList;
+                self.status = ModelStepStatus::Complete;
+            }
+            Err(err) => {
+                self.status =
+                    ModelStepStatus::Error(format!("Failed to write model selection: {err}"));
+            }
+        }
+
+        self.request_frame.schedule_frame();
+    }
+
+    fn move_model_selection(&mut self, delta: isize) {
+        if !matches!(
+            self.status,
+            ModelStepStatus::Ready | ModelStepStatus::Error(_)
+        ) {
+            return;
+        }
+        if !matches!(self.stage, SelectionStage::ModelList) {
+            return;
+        }
+        let len = self.presets.len();
+        if len == 0 {
+            return;
+        }
+        let next = (self.selected_index as isize + delta).rem_euclid(len as isize);
+        self.selected_index = next as usize;
+        self.adjust_model_scroll();
+        self.request_frame.schedule_frame();
+    }
+
+    fn move_model_selection_page(&mut self, delta_pages: isize) {
+        if !matches!(self.stage, SelectionStage::ModelList) {
+            return;
+        }
+        let len = self.presets.len();
+        if len == 0 {
+            return;
+        }
+        let page = self.visible_capacity.get().max(1) as isize;
+        let mut target = self.selected_index as isize + delta_pages * page;
+        target = target.clamp(0, len as isize - 1);
+        self.selected_index = target as usize;
+        self.adjust_model_scroll();
+        self.request_frame.schedule_frame();
+    }
+
+    fn move_reasoning_selection(&mut self, delta: isize) {
+        let SelectionStage::Reasoning(state) = &mut self.stage else {
+            return;
+        };
+        let len = state.options.len();
+        if len == 0 {
+            return;
+        }
+        let next = (state.selected_index as isize + delta).rem_euclid(len as isize);
+        state.selected_index = next as usize;
+        Self::adjust_reasoning_scroll(state);
+        self.request_frame.schedule_frame();
+    }
+
+    fn move_reasoning_selection_page(&mut self, delta_pages: isize) {
+        let SelectionStage::Reasoning(state) = &mut self.stage else {
+            return;
+        };
+        let len = state.options.len();
+        if len == 0 {
+            return;
+        }
+        let page = state.visible_capacity.get().max(1) as isize;
+        let mut target = state.selected_index as isize + delta_pages * page;
+        target = target.clamp(0, len as isize - 1);
+        state.selected_index = target as usize;
+        Self::adjust_reasoning_scroll(state);
+        self.request_frame.schedule_frame();
+    }
+
+    fn adjust_model_scroll(&self) {
+        let visible = self.visible_capacity.get().max(1);
+        let len = self.presets.len();
+        if len <= visible {
+            self.scroll_offset.set(0);
+            return;
+        }
+        let max_offset = len - visible;
+        let mut offset = self.scroll_offset.get().min(max_offset);
+        if self.selected_index < offset {
+            offset = self.selected_index;
+        } else if self.selected_index >= offset + visible {
+            offset = self.selected_index + 1 - visible;
+        }
+        self.scroll_offset.set(offset);
+    }
+
+    fn adjust_reasoning_scroll(state: &ReasoningStageState) {
+        let visible = state.visible_capacity.get().max(1);
+        let len = state.options.len();
+        if len <= visible {
+            state.scroll_offset.set(0);
+            return;
+        }
+        let max_offset = len - visible;
+        let mut offset = state.scroll_offset.get().min(max_offset);
+        if state.selected_index < offset {
+            offset = state.selected_index;
+        } else if state.selected_index >= offset + visible {
+            offset = state.selected_index + 1 - visible;
+        }
+        state.scroll_offset.set(offset);
+    }
+
+    fn render_loading(&self, area: Rect, buf: &mut Buffer) {
+        let lines = vec![
+            Line::from("Connecting to LiteLLM".bold()),
+            Line::from("Fetching available models. This may take a moment.".dim()),
+        ];
+        Paragraph::new(lines)
+            .block(Block::default().borders(Borders::ALL))
+            .wrap(Wrap { trim: false })
+            .render(area, buf);
+    }
+
+    fn render_error(&self, area: Rect, buf: &mut Buffer, message: &str) {
+        let lines = vec![
+            Line::from("Model selection unavailable".bold().fg(Color::Red)),
+            Line::from(message),
+            Line::from(""),
+            Line::from("Press Enter or Esc to continue without selecting a model."),
+        ];
+        Paragraph::new(lines)
+            .block(Block::default().borders(Borders::ALL))
+            .wrap(Wrap { trim: false })
+            .render(area, buf);
+    }
+
+    fn render_ready(&self, area: Rect, buf: &mut Buffer) {
+        match &self.stage {
+            SelectionStage::ModelList => self.render_model_stage(area, buf),
+            SelectionStage::Reasoning(state) => self.render_reasoning_stage(area, buf, state),
+        }
+    }
+
+    fn render_model_stage(&self, area: Rect, buf: &mut Buffer) {
+        let instructions = vec![
+            Line::from(vec![Span::styled(
+                "Select a LiteLLM model for Codex CLI.",
+                Style::default().add_modifier(Modifier::BOLD),
+            )]),
+            Line::from(vec![Span::styled(
+                "Use / (PageUp/PageDown) to browse models.",
+                Style::default().fg(Color::Gray),
+            )]),
+            Line::from(vec![Span::styled(
+                "Press Enter to choose reasoning level. Esc skips selection.",
+                Style::default().fg(Color::Gray),
+            )]),
+        ];
+        let instruction_height = instructions.len() as u16 + 1;
+        let instruction_height = instruction_height.min(area.height);
+
+        let [instructions_area, list_area] =
+            Layout::vertical([Constraint::Length(instruction_height), Constraint::Min(3)])
+                .areas(area);
+
+        Paragraph::new(instructions)
+            .wrap(Wrap { trim: false })
+            .render(instructions_area, buf);
+
+        if list_area.height == 0 {
+            return;
+        }
+
+        let max_visible = list_area.height.saturating_sub(2).max(1) as usize;
+        self.visible_capacity.set(max_visible.max(1));
+        self.adjust_model_scroll();
+
+        let len = self.presets.len();
+        if len == 0 {
+            Paragraph::new(vec![Line::from(
+                "No models available. Configure LiteLLM credentials first.".dim(),
+            )])
+            .block(Block::default().borders(Borders::ALL))
+            .wrap(Wrap { trim: false })
+            .render(list_area, buf);
+            return;
+        }
+
+        let start = self
+            .scroll_offset
+            .get()
+            .min(len.saturating_sub(max_visible));
+        let end = (start + max_visible).min(len);
+
+        let mut lines: Vec<Line> = Vec::new();
+        for (visible_idx, preset_idx) in (start..end).enumerate() {
+            let preset = &self.presets[preset_idx];
+            let global_index = preset_idx + 1;
+            let is_selected = preset_idx == self.selected_index;
+            let is_current = preset.model == self.config.model;
+
+            let marker = if is_selected { '' } else { ' ' };
+            let mut name = preset.display_name.clone();
+            if is_current {
+                name.push_str(" (current)");
+            }
+            let entry = format!("{marker} {global_index}. {name}");
+
+            let mut style = Style::default();
+            if is_selected {
+                style = style
+                    .fg(Color::Cyan)
+                    .add_modifier(Modifier::BOLD | Modifier::REVERSED);
+            }
+
+            lines.push(Line::from(vec![Span::styled(entry, style)]));
+
+            if let Some(desc) = preset.description.as_ref() {
+                if is_selected || visible_idx < 3 {
+                    lines.push(Line::from(vec![Span::styled(
+                        format!("    {desc}"),
+                        if is_selected {
+                            Style::default()
+                                .fg(Color::Cyan)
+                                .add_modifier(Modifier::REVERSED)
+                        } else {
+                            Style::default().add_modifier(Modifier::DIM)
+                        },
+                    )]));
+                }
+            }
+        }
+
+        Paragraph::new(lines)
+            .block(Block::default().borders(Borders::ALL))
+            .wrap(Wrap { trim: false })
+            .render(list_area, buf);
+    }
+
+    fn render_reasoning_stage(&self, area: Rect, buf: &mut Buffer, state: &ReasoningStageState) {
+        let preset = &self.presets[state.preset_index];
+        let header = vec![
+            Line::from(vec![Span::styled(
+                format!("Select reasoning level for {}", preset.display_name),
+                Style::default().add_modifier(Modifier::BOLD),
+            )]),
+            Line::from(vec![Span::styled(
+                "Use / to highlight, Enter to confirm, Esc to pick a different model.",
+                Style::default().fg(Color::Gray),
+            )]),
+        ];
+        let header_height = header.len() as u16 + 1;
+
+        let [header_area, list_area] = Layout::vertical([
+            Constraint::Length(header_height.min(area.height)),
+            Constraint::Min(3),
+        ])
+        .areas(area);
+
+        Paragraph::new(header)
+            .wrap(Wrap { trim: false })
+            .render(header_area, buf);
+
+        if list_area.height == 0 {
+            return;
+        }
+
+        let max_visible = list_area.height.saturating_sub(2).max(1) as usize;
+        state.visible_capacity.set(max_visible.max(1));
+        Self::adjust_reasoning_scroll(state);
+
+        let len = state.options.len();
+        if len == 0 {
+            Paragraph::new(vec![Line::from(
+                "No reasoning levels available; using provider defaults.".dim(),
+            )])
+            .block(Block::default().borders(Borders::ALL))
+            .wrap(Wrap { trim: false })
+            .render(list_area, buf);
+            return;
+        }
+
+        let start = state
+            .scroll_offset
+            .get()
+            .min(len.saturating_sub(max_visible));
+        let end = (start + max_visible).min(len);
+
+        let mut lines: Vec<Line> = Vec::new();
+        for option_idx in start..end {
+            let option = &state.options[option_idx];
+            let is_selected = option_idx == state.selected_index;
+            let marker = if is_selected { '' } else { ' ' };
+            let entry = format!("{marker} {}", option.label);
+            let mut style = Style::default();
+            if is_selected {
+                style = style
+                    .fg(Color::Cyan)
+                    .add_modifier(Modifier::BOLD | Modifier::REVERSED);
+            }
+
+            lines.push(Line::from(vec![Span::styled(entry, style)]));
+
+            if let Some(desc) = option.description.as_ref() {
+                lines.push(Line::from(vec![Span::styled(
+                    format!("    {desc}"),
+                    if is_selected {
+                        Style::default()
+                            .fg(Color::Cyan)
+                            .add_modifier(Modifier::REVERSED)
+                    } else {
+                        Style::default().add_modifier(Modifier::DIM)
+                    },
+                )]));
+            }
+
+            if is_selected {
+                if let Some(extra) = option.selected_description.as_ref() {
+                    lines.push(Line::from(vec![Span::styled(
+                        format!("    {extra}"),
+                        Style::default()
+                            .fg(Color::Yellow)
+                            .add_modifier(Modifier::BOLD),
+                    )]));
+                }
+            }
+        }
+
+        Paragraph::new(lines)
+            .block(Block::default().borders(Borders::ALL))
+            .wrap(Wrap { trim: false })
+            .render(list_area, buf);
+    }
+
+    fn render_completion(&self, area: Rect, buf: &mut Buffer) {
+        let result = self.selection_result.as_ref();
+        let effort_label = result
+            .and_then(|r| r.reasoning_effort)
+            .map(|eff| {
+                let mut label = eff.to_string();
+                if let Some(first) = label.get_mut(0..1) {
+                    first.make_ascii_uppercase();
+                }
+                label
+            })
+            .unwrap_or_else(|| "default effort".to_string());
+
+        let lines = vec![
+            Line::from(
+                format!(
+                    "Model set to {} ({effort_label}).",
+                    result
+                        .as_ref()
+                        .map(|r| r.model.as_str())
+                        .unwrap_or("unchanged")
+                )
+                .bold(),
+            ),
+            Line::from("Press Enter to continue."),
+        ];
+        Paragraph::new(lines)
+            .wrap(Wrap { trim: false })
+            .render(area, buf);
+    }
+}
+
+impl StepStateProvider for ModelSelectionWidget {
+    fn get_step_state(&self) -> StepState {
+        match self.status {
+            ModelStepStatus::Hidden | ModelStepStatus::AwaitingCredentials => StepState::Hidden,
+            ModelStepStatus::Loading
+            | ModelStepStatus::Ready
+            | ModelStepStatus::Saving
+            | ModelStepStatus::Error(_) => StepState::InProgress,
+            ModelStepStatus::Complete => StepState::Complete,
+        }
+    }
+}
+
+impl KeyboardHandler for ModelSelectionWidget {
+    fn handle_key_event(&mut self, key_event: KeyEvent) {
+        if key_event.kind != KeyEventKind::Press {
+            return;
+        }
+
+        match key_event.code {
+            KeyCode::Up if key_event.modifiers.is_empty() => match self.stage {
+                SelectionStage::ModelList => self.move_model_selection(-1),
+                SelectionStage::Reasoning(_) => self.move_reasoning_selection(-1),
+            },
+            KeyCode::Down if key_event.modifiers.is_empty() => match self.stage {
+                SelectionStage::ModelList => self.move_model_selection(1),
+                SelectionStage::Reasoning(_) => self.move_reasoning_selection(1),
+            },
+            KeyCode::PageUp if key_event.modifiers.is_empty() => match self.stage {
+                SelectionStage::ModelList => self.move_model_selection_page(-1),
+                SelectionStage::Reasoning(_) => self.move_reasoning_selection_page(-1),
+            },
+            KeyCode::PageDown if key_event.modifiers.is_empty() => match self.stage {
+                SelectionStage::ModelList => self.move_model_selection_page(1),
+                SelectionStage::Reasoning(_) => self.move_reasoning_selection_page(1),
+            },
+            KeyCode::Home if key_event.modifiers.is_empty() => {
+                if matches!(self.stage, SelectionStage::ModelList) && !self.presets.is_empty() {
+                    self.selected_index = 0;
+                    self.adjust_model_scroll();
+                    self.request_frame.schedule_frame();
+                }
+            }
+            KeyCode::End if key_event.modifiers.is_empty() => {
+                if matches!(self.stage, SelectionStage::ModelList) && !self.presets.is_empty() {
+                    self.selected_index = self.presets.len() - 1;
+                    self.adjust_model_scroll();
+                    self.request_frame.schedule_frame();
+                }
+            }
+            KeyCode::Esc if key_event.modifiers.is_empty() => {
+                if matches!(self.stage, SelectionStage::Reasoning(_)) {
+                    self.stage = SelectionStage::ModelList;
+                    self.request_frame.schedule_frame();
+                } else if matches!(self.status, ModelStepStatus::Error(_)) {
+                    self.status = ModelStepStatus::Complete;
+                    self.request_frame.schedule_frame();
+                }
+            }
+            KeyCode::Enter if key_event.modifiers.is_empty() => match &self.status {
+                ModelStepStatus::Ready => match self.stage {
+                    SelectionStage::ModelList => self.begin_reasoning_stage(),
+                    SelectionStage::Reasoning(_) => self.confirm_reasoning_selection(),
+                },
+                ModelStepStatus::Error(_) | ModelStepStatus::Complete => {
+                    self.status = ModelStepStatus::Complete;
+                    self.request_frame.schedule_frame();
+                }
+                _ => {}
+            },
+            _ => {}
+        }
+    }
+}
+
+impl WidgetRef for ModelSelectionWidget {
+    fn render_ref(&self, area: Rect, buf: &mut Buffer) {
+        Clear.render(area, buf);
+
+        match &self.status {
+            ModelStepStatus::Hidden | ModelStepStatus::AwaitingCredentials => {
+                let lines = vec![
+                    Line::from("Configure LiteLLM credentials first.").dim(),
+                    Line::from("The model selector will appear afterwards."),
+                ];
+                Paragraph::new(lines)
+                    .wrap(Wrap { trim: false })
+                    .render(area, buf);
+            }
+            ModelStepStatus::Loading => self.render_loading(area, buf),
+            ModelStepStatus::Ready => self.render_ready(area, buf),
+            ModelStepStatus::Saving => {
+                let lines = vec![
+                    Line::from("Saving model selection".bold()),
+                    Line::from("Please wait."),
+                ];
+                Paragraph::new(lines)
+                    .block(Block::default().borders(Borders::ALL))
+                    .wrap(Wrap { trim: false })
+                    .render(area, buf);
+            }
+            ModelStepStatus::Complete => self.render_completion(area, buf),
+            ModelStepStatus::Error(message) => self.render_error(area, buf, message),
+        }
+    }
+}
diff --git a/codex-rs/tui/src/onboarding/onboarding_screen.rs b/codex-rs/tui/src/onboarding/onboarding_screen.rs
index 709f158b..6eb408b7 100644
--- a/codex-rs/tui/src/onboarding/onboarding_screen.rs
+++ b/codex-rs/tui/src/onboarding/onboarding_screen.rs
@@ -12,11 +12,13 @@ use ratatui::widgets::Clear;
 use ratatui::widgets::WidgetRef;
 
 use codex_app_server_protocol::AuthMode;
+use codex_common::model_presets::is_litellm_provider_id;
 use codex_protocol::config_types::ForcedLoginMethod;
 
 use crate::LoginStatus;
 use crate::onboarding::auth::AuthModeWidget;
 use crate::onboarding::auth::SignInState;
+use crate::onboarding::model::ModelSelectionWidget;
 use crate::onboarding::trust_directory::TrustDirectorySelection;
 use crate::onboarding::trust_directory::TrustDirectoryWidget;
 use crate::onboarding::welcome::WelcomeWidget;
@@ -33,6 +35,7 @@ enum Step {
     Windows(WindowsSetupWidget),
     Welcome(WelcomeWidget),
     Auth(AuthModeWidget),
+    Model(ModelSelectionWidget),
     TrustDirectory(TrustDirectoryWidget),
 }
 
@@ -57,6 +60,8 @@ pub(crate) struct OnboardingScreen {
     steps: Vec<Step>,
     is_done: bool,
     windows_install_selected: bool,
+    litellm_credentials_updated: bool,
+    litellm_model_selected: bool,
 }
 
 pub(crate) struct OnboardingScreenArgs {
@@ -71,6 +76,8 @@ pub(crate) struct OnboardingScreenArgs {
 pub(crate) struct OnboardingResult {
     pub directory_trust_decision: Option<TrustDirectorySelection>,
     pub windows_install_selected: bool,
+    pub litellm_credentials_updated: bool,
+    pub litellm_model_selected: bool,
 }
 
 impl OnboardingScreen {
@@ -86,8 +93,7 @@ impl OnboardingScreen {
         let cwd = config.cwd.clone();
         let forced_chatgpt_workspace_id = config.forced_chatgpt_workspace_id.clone();
         let forced_login_method = config.forced_login_method;
-        let codex_home = config.codex_home;
-        let cli_auth_credentials_store_mode = config.cli_auth_credentials_store_mode;
+        let codex_home = config.codex_home.clone();
         let mut steps: Vec<Step> = Vec::new();
         if show_windows_wsl_screen {
             steps.push(Step::Windows(WindowsSetupWidget::new(codex_home.clone())));
@@ -98,21 +104,29 @@ impl OnboardingScreen {
         )));
         if show_login_screen {
             let highlighted_mode = match forced_login_method {
-                Some(ForcedLoginMethod::Api) => AuthMode::ApiKey,
-                _ => AuthMode::ChatGPT,
+                Some(ForcedLoginMethod::Api) | None => AuthMode::ApiKey,
+                Some(ForcedLoginMethod::Chatgpt) => AuthMode::ChatGPT,
             };
-            steps.push(Step::Auth(AuthModeWidget {
+            let mut auth_widget = AuthModeWidget {
                 request_frame: tui.frame_requester(),
                 highlighted_mode,
                 error: None,
                 sign_in_state: Arc::new(RwLock::new(SignInState::PickMode)),
                 codex_home: codex_home.clone(),
-                cli_auth_credentials_store_mode,
                 login_status,
+                cli_auth_credentials_store_mode: config.cli_auth_credentials_store_mode,
                 auth_manager,
                 forced_chatgpt_workspace_id,
                 forced_login_method,
-            }))
+            };
+            if config.litellm_setup_required {
+                auth_widget.start_api_key_entry();
+            }
+            steps.push(Step::Auth(auth_widget))
+        }
+        if is_litellm_provider_id(&config.model_provider_id) {
+            let model_widget = ModelSelectionWidget::new(tui.frame_requester(), config.clone());
+            steps.push(Step::Model(model_widget));
         }
         let is_git_repo = get_git_repo_root(&cwd).is_some();
         let highlighted = if is_git_repo {
@@ -132,11 +146,15 @@ impl OnboardingScreen {
             }))
         }
         // TODO: add git warning.
+        let is_litellm = is_litellm_provider_id(&config.model_provider_id);
+
         Self {
             request_frame: tui.frame_requester(),
             steps,
             is_done: false,
             windows_install_selected: false,
+            litellm_credentials_updated: false,
+            litellm_model_selected: !is_litellm,
         }
     }
 
@@ -194,6 +212,60 @@ impl OnboardingScreen {
     pub fn windows_install_selected(&self) -> bool {
         self.windows_install_selected
     }
+
+    pub fn litellm_credentials_updated(&self) -> bool {
+        self.litellm_credentials_updated
+    }
+
+    pub fn litellm_model_selected(&self) -> bool {
+        self.litellm_model_selected
+    }
+
+    fn refresh_litellm_state(&mut self) {
+        #[cfg(debug_assertions)]
+        tracing::debug!(
+            credentials_updated = self.litellm_credentials_updated,
+            model_selected = self.litellm_model_selected,
+            "onboarding.refresh_litellm_state:start"
+        );
+
+        let credentials_ready = {
+            if let Some(Step::Auth(widget)) =
+                self.steps.iter().find(|step| matches!(step, Step::Auth(_)))
+            {
+                widget
+                    .sign_in_state
+                    .read()
+                    .map(|guard| matches!(&*guard, super::auth::SignInState::ApiKeyConfigured))
+                    .unwrap_or(false)
+            } else {
+                false
+            }
+        };
+
+        if credentials_ready && !self.litellm_credentials_updated {
+            self.litellm_credentials_updated = true;
+            #[cfg(debug_assertions)]
+            tracing::debug!("onboarding: LiteLLM credentials captured");
+        }
+
+        if let Some(Step::Model(widget)) = self
+            .steps
+            .iter_mut()
+            .find(|step| matches!(step, Step::Model(_)))
+        {
+            if self.litellm_credentials_updated {
+                #[cfg(debug_assertions)]
+                tracing::debug!("onboarding: triggering model preset load");
+                widget.credentials_ready();
+            }
+            if widget.selection().is_some() {
+                self.litellm_model_selected = true;
+                #[cfg(debug_assertions)]
+                tracing::debug!("onboarding: LiteLLM model selected");
+            }
+        }
+    }
 }
 
 impl KeyboardHandler for OnboardingScreen {
@@ -240,6 +312,7 @@ impl KeyboardHandler for OnboardingScreen {
             self.is_done = true;
         }
         self.request_frame.schedule_frame();
+        self.refresh_litellm_state();
     }
 
     fn handle_paste(&mut self, pasted: String) {
@@ -251,6 +324,7 @@ impl KeyboardHandler for OnboardingScreen {
             active_step.handle_paste(pasted);
         }
         self.request_frame.schedule_frame();
+        self.refresh_litellm_state();
     }
 }
 
@@ -323,6 +397,7 @@ impl KeyboardHandler for Step {
             Step::Windows(widget) => widget.handle_key_event(key_event),
             Step::Welcome(widget) => widget.handle_key_event(key_event),
             Step::Auth(widget) => widget.handle_key_event(key_event),
+            Step::Model(widget) => widget.handle_key_event(key_event),
             Step::TrustDirectory(widget) => widget.handle_key_event(key_event),
         }
     }
@@ -332,6 +407,7 @@ impl KeyboardHandler for Step {
             Step::Windows(_) => {}
             Step::Welcome(_) => {}
             Step::Auth(widget) => widget.handle_paste(pasted),
+            Step::Model(_) => {}
             Step::TrustDirectory(widget) => widget.handle_paste(pasted),
         }
     }
@@ -343,6 +419,7 @@ impl StepStateProvider for Step {
             Step::Windows(w) => w.get_step_state(),
             Step::Welcome(w) => w.get_step_state(),
             Step::Auth(w) => w.get_step_state(),
+            Step::Model(w) => w.get_step_state(),
             Step::TrustDirectory(w) => w.get_step_state(),
         }
     }
@@ -360,6 +437,9 @@ impl WidgetRef for Step {
             Step::Auth(widget) => {
                 widget.render_ref(area, buf);
             }
+            Step::Model(widget) => {
+                widget.render_ref(area, buf);
+            }
             Step::TrustDirectory(widget) => {
                 widget.render_ref(area, buf);
             }
@@ -434,5 +514,7 @@ pub(crate) async fn run_onboarding_app(
     Ok(OnboardingResult {
         directory_trust_decision: onboarding_screen.directory_trust_decision(),
         windows_install_selected: onboarding_screen.windows_install_selected(),
+        litellm_credentials_updated: onboarding_screen.litellm_credentials_updated(),
+        litellm_model_selected: onboarding_screen.litellm_model_selected(),
     })
 }
diff --git a/codex-rs/tui/src/onboarding/welcome.rs b/codex-rs/tui/src/onboarding/welcome.rs
index 645c86ba..cd9b13e0 100644
--- a/codex-rs/tui/src/onboarding/welcome.rs
+++ b/codex-rs/tui/src/onboarding/welcome.rs
@@ -68,8 +68,18 @@ impl WidgetRef for &WelcomeWidget {
         lines.push(Line::from(vec![
             "  ".into(),
             "Welcome to ".into(),
-            "Codex".bold(),
-            ", OpenAI's command-line coding agent".into(),
+            "codex-litellm".bold(),
+            ", the LiteLLM-native build of Codex".into(),
+        ]));
+        lines.push(Line::from(vec![
+            "  We'll walk through connecting to your LiteLLM endpoint on the next screen.".into(),
+        ]));
+        lines.push(Line::from(vec![
+            "  Tip: set ".into(),
+            "LITELLM_BASE_URL".bold(),
+            " and ".into(),
+            "LITELLM_API_KEY".bold(),
+            " before launching to skip the credential step entirely.".into(),
         ]));
 
         Paragraph::new(lines)
diff --git a/codex-rs/tui/src/slash_command.rs b/codex-rs/tui/src/slash_command.rs
index 969d279b..6f7f87ab 100644
--- a/codex-rs/tui/src/slash_command.rs
+++ b/codex-rs/tui/src/slash_command.rs
@@ -12,6 +12,7 @@ use strum_macros::IntoStaticStr;
 pub enum SlashCommand {
     // DO NOT ALPHA-SORT! Enum order is presentation order in the popup, so
     // more frequently used commands should be listed first.
+    #[strum(serialize = "model")]
     Model,
     Approvals,
     Review,
diff --git a/codex-rs/tui/src/status/card.rs b/codex-rs/tui/src/status/card.rs
index fdf6629a..612f15e4 100644
--- a/codex-rs/tui/src/status/card.rs
+++ b/codex-rs/tui/src/status/card.rs
@@ -33,6 +33,8 @@ use super::rate_limits::format_status_limit_summary;
 use super::rate_limits::render_status_limit_progress_bar;
 use crate::wrapping::RtOptions;
 use crate::wrapping::word_wrap_lines;
+use codex_litellm_model_session_telemetry as session_telemetry;
+use codex_litellm_model_session_telemetry::SessionTelemetrySnapshot;
 
 #[derive(Debug, Clone)]
 struct StatusContextWindowData {
@@ -61,6 +63,7 @@ struct StatusHistoryCell {
     session_id: Option<String>,
     token_usage: StatusTokenUsageData,
     rate_limits: StatusRateLimitData,
+    model_session_summary: Option<SessionTelemetrySnapshot>,
 }
 
 pub(crate) fn new_status_output(
@@ -108,6 +111,7 @@ impl StatusHistoryCell {
         let agents_summary = compose_agents_summary(config);
         let account = compose_account_display(config);
         let session_id = session_id.as_ref().map(std::string::ToString::to_string);
+        let model_session_summary = session_telemetry::snapshot(session_id.as_deref());
         let context_window = config.model_context_window.and_then(|window| {
             context_usage.map(|usage| StatusContextWindowData {
                 percent_remaining: usage.percent_of_context_window_remaining(window),
@@ -135,6 +139,7 @@ impl StatusHistoryCell {
             session_id,
             token_usage,
             rate_limits,
+            model_session_summary,
         }
     }
 
@@ -239,6 +244,40 @@ impl StatusHistoryCell {
         lines
     }
 
+    fn litellm_usage_lines(
+        &self,
+        summary: &SessionTelemetrySnapshot,
+        formatter: &FieldFormatter,
+    ) -> Vec<Line<'static>> {
+        let total_fmt = format_tokens_compact(summary.total_tokens);
+        let turn_label = match summary.total_turns {
+            1 => "1 turn".to_string(),
+            turns => format!("{turns} turns"),
+        };
+
+        let mut lines = Vec::new();
+        lines.push(formatter.line(
+            "LiteLLM usage",
+            vec![Span::from(format!("{total_fmt} across {turn_label}"))],
+        ));
+
+        for model in summary.models.iter() {
+            let model_tokens = format_tokens_compact(model.total_tokens);
+            let model_turns = match model.turns {
+                1 => "1 turn".to_string(),
+                turns => format!("{turns} turns"),
+            };
+            let mut detail = format!("{model_tokens}  {model_turns}");
+            if let Some(effort) = model.last_reasoning_effort.as_ref() {
+                detail.push_str(&format!("  last effort {effort}"));
+            }
+            let value_spans = vec![Span::from(format!("{}  {}", model.model, detail)).dim()];
+            lines.push(formatter.continuation(value_spans));
+        }
+
+        lines
+    }
+
     fn collect_rate_limit_labels(&self, seen: &mut BTreeSet<String>, labels: &mut Vec<String>) {
         match &self.rate_limits {
             StatusRateLimitData::Available(rows) => {
@@ -268,7 +307,11 @@ impl HistoryCell for StatusHistoryCell {
             Span::from(format!("{}>_ ", FieldFormatter::INDENT)).dim(),
             Span::from("OpenAI Codex").bold(),
             Span::from(" ").dim(),
-            Span::from(format!("(v{CODEX_CLI_VERSION})")).dim(),
+            Span::from(format!(
+                "(v{})",
+                codex_common::litellm::decorate_version(CODEX_CLI_VERSION)
+            ))
+            .dim(),
         ]));
         lines.push(Line::from(Vec::<Span<'static>>::new()));
 
@@ -306,6 +349,9 @@ impl HistoryCell for StatusHistoryCell {
         if self.token_usage.context_window.is_some() {
             push_label(&mut labels, &mut seen, "Context window");
         }
+        if self.model_session_summary.is_some() {
+            push_label(&mut labels, &mut seen, "LiteLLM usage");
+        }
         self.collect_rate_limit_labels(&mut seen, &mut labels);
 
         let formatter = FieldFormatter::from_labels(labels.iter().map(String::as_str));
@@ -359,6 +405,10 @@ impl HistoryCell for StatusHistoryCell {
             lines.push(formatter.line("Context window", spans));
         }
 
+        if let Some(summary) = self.model_session_summary.as_ref() {
+            lines.extend(self.litellm_usage_lines(summary, &formatter));
+        }
+
         lines.extend(self.rate_limit_lines(available_inner_width, &formatter));
 
         let content_width = lines.iter().map(line_display_width).max().unwrap_or(0);

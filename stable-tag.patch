diff --git a/codex-rs/Cargo.lock b/codex-rs/Cargo.lock
index 4961486e6..be31e327a 100644
--- a/codex-rs/Cargo.lock
+++ b/codex-rs/Cargo.lock
@@ -27,17 +27,6 @@ version = "2.0.1"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "320119579fcad9c21884f5c4861d16174d0e06250625266f50fe6898340abefa"
 
-[[package]]
-name = "aes"
-version = "0.8.4"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "b169f7a6d4742236a0a00c541b845991d0ac43e546831af1249753ab4c3aa3a0"
-dependencies = [
- "cfg-if",
- "cipher",
- "cpufeatures",
-]
-
 [[package]]
 name = "ahash"
 version = "0.8.12"
@@ -178,7 +167,7 @@ checksum = "a23eb6b1614318a8071c9b2521f36b424b2c83db5eb3a0fead4a6c0809af6e61"
 
 [[package]]
 name = "app_test_support"
-version = "0.0.0"
+version = "0.55.0"
 dependencies = [
  "anyhow",
  "assert_cmd",
@@ -309,18 +298,6 @@ version = "1.5.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "9b34d609dfbaf33d6889b2b7106d3ca345eacad44200913df5ba02bfd31d2ba9"
 
-[[package]]
-name = "async-broadcast"
-version = "0.7.2"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "435a87a52755b8f27fcf321ac4f04b2802e337c8c4872923137471ec39c37532"
-dependencies = [
- "event-listener",
- "event-listener-strategy",
- "futures-core",
- "pin-project-lite",
-]
-
 [[package]]
 name = "async-channel"
 version = "2.5.0"
@@ -333,107 +310,6 @@ dependencies = [
  "pin-project-lite",
 ]
 
-[[package]]
-name = "async-executor"
-version = "1.13.3"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "497c00e0fd83a72a79a39fcbd8e3e2f055d6f6c7e025f3b3d91f4f8e76527fb8"
-dependencies = [
- "async-task",
- "concurrent-queue",
- "fastrand",
- "futures-lite",
- "pin-project-lite",
- "slab",
-]
-
-[[package]]
-name = "async-fs"
-version = "2.2.0"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "8034a681df4aed8b8edbd7fbe472401ecf009251c8b40556b304567052e294c5"
-dependencies = [
- "async-lock",
- "blocking",
- "futures-lite",
-]
-
-[[package]]
-name = "async-io"
-version = "2.6.0"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "456b8a8feb6f42d237746d4b3e9a178494627745c3c56c6ea55d92ba50d026fc"
-dependencies = [
- "autocfg",
- "cfg-if",
- "concurrent-queue",
- "futures-io",
- "futures-lite",
- "parking",
- "polling",
- "rustix 1.0.8",
- "slab",
- "windows-sys 0.61.1",
-]
-
-[[package]]
-name = "async-lock"
-version = "3.4.1"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "5fd03604047cee9b6ce9de9f70c6cd540a0520c813cbd49bae61f33ab80ed1dc"
-dependencies = [
- "event-listener",
- "event-listener-strategy",
- "pin-project-lite",
-]
-
-[[package]]
-name = "async-process"
-version = "2.5.0"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "fc50921ec0055cdd8a16de48773bfeec5c972598674347252c0399676be7da75"
-dependencies = [
- "async-channel",
- "async-io",
- "async-lock",
- "async-signal",
- "async-task",
- "blocking",
- "cfg-if",
- "event-listener",
- "futures-lite",
- "rustix 1.0.8",
-]
-
-[[package]]
-name = "async-recursion"
-version = "1.1.1"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "3b43422f69d8ff38f95f1b2bb76517c91589a924d1559a0e935d7c8ce0274c11"
-dependencies = [
- "proc-macro2",
- "quote",
- "syn 2.0.104",
-]
-
-[[package]]
-name = "async-signal"
-version = "0.2.13"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "43c070bbf59cd3570b6b2dd54cd772527c7c3620fce8be898406dd3ed6adc64c"
-dependencies = [
- "async-io",
- "async-lock",
- "atomic-waker",
- "cfg-if",
- "futures-core",
- "futures-io",
- "rustix 1.0.8",
- "signal-hook-registry",
- "slab",
- "windows-sys 0.61.1",
-]
-
 [[package]]
 name = "async-stream"
 version = "0.3.6"
@@ -456,12 +332,6 @@ dependencies = [
  "syn 2.0.104",
 ]
 
-[[package]]
-name = "async-task"
-version = "4.7.1"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "8b75356056920673b02621b35afd0f7dda9306d03c79a30f5c56c44cf256e3de"
-
 [[package]]
 name = "async-trait"
 version = "0.1.89"
@@ -605,28 +475,6 @@ dependencies = [
  "generic-array",
 ]
 
-[[package]]
-name = "block-padding"
-version = "0.3.3"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "a8894febbff9f758034a5b8e12d87918f56dfc64a8e1fe757d65e29041538d93"
-dependencies = [
- "generic-array",
-]
-
-[[package]]
-name = "blocking"
-version = "1.6.2"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "e83f8d02be6967315521be875afa792a316e28d57b5a2d401897e2a7921b7f21"
-dependencies = [
- "async-channel",
- "async-task",
- "futures-io",
- "futures-lite",
- "piper",
-]
-
 [[package]]
 name = "bstr"
 version = "1.12.0"
@@ -683,15 +531,6 @@ dependencies = [
  "rustversion",
 ]
 
-[[package]]
-name = "cbc"
-version = "0.1.2"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "26b52a9543ae338f279b96b0b9fed9c8093744685043739079ce85cd58f289a6"
-dependencies = [
- "cipher",
-]
-
 [[package]]
 name = "cc"
 version = "1.2.30"
@@ -745,16 +584,6 @@ version = "1.5.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "6e4de3bc4ea267985becf712dc6d9eed8b04c953b3fcfb339ebc87acd9804901"
 
-[[package]]
-name = "cipher"
-version = "0.4.4"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "773f3b9af64447d2ce9850330c473515014aa235e6a783b02db81ff39e4a3dad"
-dependencies = [
- "crypto-common",
- "inout",
-]
-
 [[package]]
 name = "clap"
 version = "4.5.47"
@@ -822,7 +651,7 @@ checksum = "e9b18233253483ce2f65329a24072ec414db782531bdbb7d0bbc4bd2ce6b7e21"
 
 [[package]]
 name = "codex-ansi-escape"
-version = "0.0.0"
+version = "0.55.0"
 dependencies = [
  "ansi-to-tui",
  "ratatui",
@@ -831,7 +660,7 @@ dependencies = [
 
 [[package]]
 name = "codex-app-server"
-version = "0.0.0"
+version = "0.55.0"
 dependencies = [
  "anyhow",
  "app_test_support",
@@ -866,7 +695,7 @@ dependencies = [
 
 [[package]]
 name = "codex-app-server-protocol"
-version = "0.0.0"
+version = "0.55.0"
 dependencies = [
  "anyhow",
  "clap",
@@ -883,7 +712,7 @@ dependencies = [
 
 [[package]]
 name = "codex-apply-patch"
-version = "0.0.0"
+version = "0.55.0"
 dependencies = [
  "anyhow",
  "assert_cmd",
@@ -898,7 +727,7 @@ dependencies = [
 
 [[package]]
 name = "codex-arg0"
-version = "0.0.0"
+version = "0.55.0"
 dependencies = [
  "anyhow",
  "codex-apply-patch",
@@ -911,7 +740,7 @@ dependencies = [
 
 [[package]]
 name = "codex-async-utils"
-version = "0.0.0"
+version = "0.55.0"
 dependencies = [
  "async-trait",
  "pretty_assertions",
@@ -935,16 +764,20 @@ dependencies = [
 
 [[package]]
 name = "codex-backend-openapi-models"
-version = "0.0.0"
+version = "0.55.0"
 dependencies = [
  "serde",
  "serde_json",
  "serde_with",
 ]
 
+[[package]]
+name = "codex-build-info"
+version = "0.55.0"
+
 [[package]]
 name = "codex-chatgpt"
-version = "0.0.0"
+version = "0.55.0"
 dependencies = [
  "anyhow",
  "clap",
@@ -959,7 +792,7 @@ dependencies = [
 
 [[package]]
 name = "codex-cli"
-version = "0.0.0"
+version = "0.55.0"
 dependencies = [
  "anyhow",
  "assert_cmd",
@@ -969,6 +802,7 @@ dependencies = [
  "codex-app-server",
  "codex-app-server-protocol",
  "codex-arg0",
+ "codex-build-info",
  "codex-chatgpt",
  "codex-cloud-tasks",
  "codex-common",
@@ -997,7 +831,7 @@ dependencies = [
 
 [[package]]
 name = "codex-cloud-tasks"
-version = "0.0.0"
+version = "0.55.0"
 dependencies = [
  "anyhow",
  "async-trait",
@@ -1023,7 +857,7 @@ dependencies = [
 
 [[package]]
 name = "codex-cloud-tasks-client"
-version = "0.0.0"
+version = "0.55.0"
 dependencies = [
  "anyhow",
  "async-trait",
@@ -1038,19 +872,22 @@ dependencies = [
 
 [[package]]
 name = "codex-common"
-version = "0.0.0"
+version = "0.55.0"
 dependencies = [
  "clap",
  "codex-app-server-protocol",
+ "codex-build-info",
  "codex-core",
  "codex-protocol",
+ "reqwest",
  "serde",
+ "serde_json",
  "toml",
 ]
 
 [[package]]
 name = "codex-core"
-version = "0.0.0"
+version = "0.55.0"
 dependencies = [
  "anyhow",
  "askama",
@@ -1129,12 +966,13 @@ dependencies = [
 
 [[package]]
 name = "codex-exec"
-version = "0.0.0"
+version = "0.55.0"
 dependencies = [
  "anyhow",
  "assert_cmd",
  "clap",
  "codex-arg0",
+ "codex-build-info",
  "codex-common",
  "codex-core",
  "codex-ollama",
@@ -1162,7 +1000,7 @@ dependencies = [
 
 [[package]]
 name = "codex-execpolicy"
-version = "0.0.0"
+version = "0.55.0"
 dependencies = [
  "allocative",
  "anyhow",
@@ -1182,7 +1020,7 @@ dependencies = [
 
 [[package]]
 name = "codex-feedback"
-version = "0.0.0"
+version = "0.55.0"
 dependencies = [
  "anyhow",
  "codex-protocol",
@@ -1193,7 +1031,7 @@ dependencies = [
 
 [[package]]
 name = "codex-file-search"
-version = "0.0.0"
+version = "0.55.0"
 dependencies = [
  "anyhow",
  "clap",
@@ -1206,7 +1044,7 @@ dependencies = [
 
 [[package]]
 name = "codex-git"
-version = "0.0.0"
+version = "0.55.0"
 dependencies = [
  "assert_matches",
  "once_cell",
@@ -1222,7 +1060,7 @@ dependencies = [
 
 [[package]]
 name = "codex-keyring-store"
-version = "0.0.0"
+version = "0.55.0"
 dependencies = [
  "keyring",
  "tracing",
@@ -1230,7 +1068,7 @@ dependencies = [
 
 [[package]]
 name = "codex-linux-sandbox"
-version = "0.0.0"
+version = "0.55.0"
 dependencies = [
  "clap",
  "codex-core",
@@ -1241,9 +1079,20 @@ dependencies = [
  "tokio",
 ]
 
+[[package]]
+name = "codex-litellm-model-session-telemetry"
+version = "0.1.0"
+dependencies = [
+ "chrono",
+ "once_cell",
+ "parking_lot",
+ "serde",
+ "serde_json",
+]
+
 [[package]]
 name = "codex-login"
-version = "0.0.0"
+version = "0.55.0"
 dependencies = [
  "anyhow",
  "base64",
@@ -1267,7 +1116,7 @@ dependencies = [
 
 [[package]]
 name = "codex-mcp-server"
-version = "0.0.0"
+version = "0.55.0"
 dependencies = [
  "anyhow",
  "assert_cmd",
@@ -1294,7 +1143,7 @@ dependencies = [
 
 [[package]]
 name = "codex-ollama"
-version = "0.0.0"
+version = "0.55.0"
 dependencies = [
  "assert_matches",
  "async-stream",
@@ -1310,7 +1159,7 @@ dependencies = [
 
 [[package]]
 name = "codex-otel"
-version = "0.0.0"
+version = "0.55.0"
 dependencies = [
  "chrono",
  "codex-app-server-protocol",
@@ -1331,14 +1180,14 @@ dependencies = [
 
 [[package]]
 name = "codex-process-hardening"
-version = "0.0.0"
+version = "0.55.0"
 dependencies = [
  "libc",
 ]
 
 [[package]]
 name = "codex-protocol"
-version = "0.0.0"
+version = "0.55.0"
 dependencies = [
  "anyhow",
  "base64",
@@ -1364,7 +1213,7 @@ dependencies = [
 
 [[package]]
 name = "codex-protocol-ts"
-version = "0.0.0"
+version = "0.55.0"
 dependencies = [
  "anyhow",
  "clap",
@@ -1374,7 +1223,7 @@ dependencies = [
 
 [[package]]
 name = "codex-responses-api-proxy"
-version = "0.0.0"
+version = "0.55.0"
 dependencies = [
  "anyhow",
  "clap",
@@ -1390,7 +1239,7 @@ dependencies = [
 
 [[package]]
 name = "codex-rmcp-client"
-version = "0.0.0"
+version = "0.55.0"
 dependencies = [
  "anyhow",
  "axum",
@@ -1419,7 +1268,7 @@ dependencies = [
 
 [[package]]
 name = "codex-stdio-to-uds"
-version = "0.0.0"
+version = "0.55.0"
 dependencies = [
  "anyhow",
  "assert_cmd",
@@ -1430,7 +1279,7 @@ dependencies = [
 
 [[package]]
 name = "codex-tui"
-version = "0.0.0"
+version = "0.55.0"
 dependencies = [
  "anyhow",
  "arboard",
@@ -1446,6 +1295,7 @@ dependencies = [
  "codex-core",
  "codex-feedback",
  "codex-file-search",
+ "codex-litellm-model-session-telemetry",
  "codex-login",
  "codex-ollama",
  "codex-protocol",
@@ -1493,7 +1343,7 @@ dependencies = [
 
 [[package]]
 name = "codex-utils-cache"
-version = "0.0.0"
+version = "0.55.0"
 dependencies = [
  "lru",
  "sha1",
@@ -1502,7 +1352,7 @@ dependencies = [
 
 [[package]]
 name = "codex-utils-image"
-version = "0.0.0"
+version = "0.55.0"
 dependencies = [
  "base64",
  "codex-utils-cache",
@@ -1514,7 +1364,7 @@ dependencies = [
 
 [[package]]
 name = "codex-utils-json-to-toml"
-version = "0.0.0"
+version = "0.55.0"
 dependencies = [
  "pretty_assertions",
  "serde_json",
@@ -1523,7 +1373,7 @@ dependencies = [
 
 [[package]]
 name = "codex-utils-pty"
-version = "0.0.0"
+version = "0.55.0"
 dependencies = [
  "anyhow",
  "portable-pty",
@@ -1532,7 +1382,7 @@ dependencies = [
 
 [[package]]
 name = "codex-utils-readiness"
-version = "0.0.0"
+version = "0.55.0"
 dependencies = [
  "assert_matches",
  "async-trait",
@@ -1543,11 +1393,11 @@ dependencies = [
 
 [[package]]
 name = "codex-utils-string"
-version = "0.0.0"
+version = "0.55.0"
 
 [[package]]
 name = "codex-utils-tokenizer"
-version = "0.0.0"
+version = "0.55.0"
 dependencies = [
  "anyhow",
  "pretty_assertions",
@@ -1682,7 +1532,7 @@ checksum = "773648b94d0e5d620f64f280777445740e61fe701025087ec8b57f45c791888b"
 
 [[package]]
 name = "core_test_support"
-version = "0.0.0"
+version = "0.55.0"
 dependencies = [
  "anyhow",
  "assert_cmd",
@@ -1886,35 +1736,6 @@ dependencies = [
  "syn 2.0.104",
 ]
 
-[[package]]
-name = "dbus"
-version = "0.9.9"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "190b6255e8ab55a7b568df5a883e9497edc3e4821c06396612048b430e5ad1e9"
-dependencies = [
- "libc",
- "libdbus-sys",
- "windows-sys 0.59.0",
-]
-
-[[package]]
-name = "dbus-secret-service"
-version = "4.1.0"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "708b509edf7889e53d7efb0ffadd994cc6c2345ccb62f55cfd6b0682165e4fa6"
-dependencies = [
- "aes",
- "block-padding",
- "cbc",
- "dbus",
- "fastrand",
- "hkdf",
- "num",
- "once_cell",
- "sha2",
- "zeroize",
-]
-
 [[package]]
 name = "deadpool"
 version = "0.12.3"
@@ -2047,7 +1868,6 @@ checksum = "9ed9a281f7bc9b7576e61468ba615a66a5c8cfdff42420a70aa82701a3b1e292"
 dependencies = [
  "block-buffer",
  "crypto-common",
- "subtle",
 ]
 
 [[package]]
@@ -2209,21 +2029,6 @@ version = "1.0.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "34aa73646ffb006b8f5147f3dc182bd4bcb190227ce861fc4a4844bf8e3cb2c0"
 
-[[package]]
-name = "encoding_rs"
-version = "0.8.35"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "75030f3c4f45dafd7586dd6780965a8c7e8e285a5ecb86713e63a79c5b2766f3"
-dependencies = [
- "cfg-if",
-]
-
-[[package]]
-name = "endi"
-version = "1.1.0"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "a3d8a32ae18130a3c84dd492d4215c3d913c3b07c6b63c2eb3eb7ff1101ab7bf"
-
 [[package]]
 name = "endian-type"
 version = "0.1.2"
@@ -2237,7 +2042,6 @@ source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "1027f7680c853e056ebcec683615fb6fbbc07dbaa13b4d5d9442b146ded4ecef"
 dependencies = [
  "enumflags2_derive",
- "serde",
 ]
 
 [[package]]
@@ -2492,21 +2296,6 @@ version = "0.1.5"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "d9c4f5dac5e15c24eb999c26181a6ca40b39fe946cbe4c263c7209467bc83af2"
 
-[[package]]
-name = "foreign-types"
-version = "0.3.2"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "f6f339eb8adc052cd2ca78910fda869aefa38d22d5cb648e6485e4d3fc06f3b1"
-dependencies = [
- "foreign-types-shared",
-]
-
-[[package]]
-name = "foreign-types-shared"
-version = "0.1.1"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "00b0228411908ca8685dba7fc2cdd70ec9990a6e753e89b6ac91a84c40fbaf4b"
-
 [[package]]
 name = "form_urlencoded"
 version = "1.2.1"
@@ -2573,19 +2362,6 @@ version = "0.3.31"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "9e5c1b78ca4aae1ac06c48a526a655760685149f0d465d21f37abfe57ce075c6"
 
-[[package]]
-name = "futures-lite"
-version = "2.6.1"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "f78e10609fe0e0b3f4157ffab1876319b5b0db102a2c60dc4626306dc46b44ad"
-dependencies = [
- "fastrand",
- "futures-core",
- "futures-io",
- "parking",
- "pin-project-lite",
-]
-
 [[package]]
 name = "futures-macro"
 version = "0.3.31"
@@ -2791,24 +2567,6 @@ version = "0.4.3"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "7f24254aa9a54b5c858eaee2f5bccdb46aaf0e486a595ed5fd8f86ba55232a70"
 
-[[package]]
-name = "hkdf"
-version = "0.12.4"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "7b5f8eb2ad728638ea2c7d47a21db23b7b58a72ed6a38256b8a1849f15fbbdf7"
-dependencies = [
- "hmac",
-]
-
-[[package]]
-name = "hmac"
-version = "0.12.1"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "6c49c37c09c17a53d937dfbb742eb3a961d65a994e6bcdcf37e7399d0cc8ab5e"
-dependencies = [
- "digest",
-]
-
 [[package]]
 name = "home"
 version = "0.5.11"
@@ -2916,13 +2674,13 @@ dependencies = [
  "http",
  "hyper",
  "hyper-util",
- "rustls",
+ "rustls 0.23.29",
  "rustls-native-certs",
  "rustls-pki-types",
  "tokio",
  "tokio-rustls",
  "tower-service",
- "webpki-roots",
+ "webpki-roots 1.0.2",
 ]
 
 [[package]]
@@ -2938,22 +2696,6 @@ dependencies = [
  "tower-service",
 ]
 
-[[package]]
-name = "hyper-tls"
-version = "0.6.0"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "70206fc6890eaca9fde8a0bf71caa2ddfc9fe045ac9e5c70df101a7dbde866e0"
-dependencies = [
- "bytes",
- "http-body-util",
- "hyper",
- "hyper-util",
- "native-tls",
- "tokio",
- "tokio-native-tls",
- "tower-service",
-]
-
 [[package]]
 name = "hyper-util"
 version = "0.1.16"
@@ -2973,11 +2715,9 @@ dependencies = [
  "percent-encoding",
  "pin-project-lite",
  "socket2 0.6.0",
- "system-configuration",
  "tokio",
  "tower-service",
  "tracing",
- "windows-registry",
 ]
 
 [[package]]
@@ -3244,16 +2984,6 @@ dependencies = [
  "libc",
 ]
 
-[[package]]
-name = "inout"
-version = "0.1.4"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "879f10e63c20629ecabbb64a8010319738c66a5cd0c29b02d63d272b03751d01"
-dependencies = [
- "block-padding",
- "generic-array",
-]
-
 [[package]]
 name = "insta"
 version = "1.43.2"
@@ -3432,15 +3162,7 @@ version = "3.6.3"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "eebcc3aff044e5944a8fbaf69eb277d11986064cba30c468730e8b9909fb551c"
 dependencies = [
- "byteorder",
- "dbus-secret-service",
- "linux-keyutils",
  "log",
- "secret-service",
- "security-framework 2.11.1",
- "security-framework 3.5.1",
- "windows-sys 0.60.2",
- "zbus",
  "zeroize",
 ]
 
@@ -3518,15 +3240,6 @@ version = "0.2.175"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "6a82ae493e598baaea5209805c49bbf2ea7de956d50d7da0da1164f9c6d28543"
 
-[[package]]
-name = "libdbus-sys"
-version = "0.2.6"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "5cbe856efeb50e4681f010e9aaa2bf0a644e10139e54cde10fc83a307c23bd9f"
-dependencies = [
- "pkg-config",
-]
-
 [[package]]
 name = "libm"
 version = "0.2.15"
@@ -3543,16 +3256,6 @@ dependencies = [
  "libc",
 ]
 
-[[package]]
-name = "linux-keyutils"
-version = "0.2.4"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "761e49ec5fd8a5a463f9b84e877c373d888935b71c6be78f3767fe2ae6bed18e"
-dependencies = [
- "bitflags 2.10.0",
- "libc",
-]
-
 [[package]]
 name = "linux-raw-sys"
 version = "0.4.15"
@@ -3661,7 +3364,7 @@ checksum = "47e1ffaa40ddd1f3ed91f717a33c8c0ee23fff369e3aa8772b9605cc1d22f4c3"
 
 [[package]]
 name = "mcp-types"
-version = "0.0.0"
+version = "0.55.0"
 dependencies = [
  "schemars 0.8.22",
  "serde",
@@ -3671,7 +3374,7 @@ dependencies = [
 
 [[package]]
 name = "mcp_test_support"
-version = "0.0.0"
+version = "0.55.0"
 dependencies = [
  "anyhow",
  "assert_cmd",
@@ -3773,23 +3476,6 @@ dependencies = [
  "serde",
 ]
 
-[[package]]
-name = "native-tls"
-version = "0.2.14"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "87de3442987e9dbec73158d5c715e7ad9072fda936bb03d19d7fa10e00520f0e"
-dependencies = [
- "libc",
- "log",
- "openssl",
- "openssl-probe",
- "openssl-sys",
- "schannel",
- "security-framework 2.11.1",
- "security-framework-sys",
- "tempfile",
-]
-
 [[package]]
 name = "ndk-context"
 version = "0.1.1"
@@ -3806,34 +3492,21 @@ checksum = "650eef8c711430f1a879fdd01d4745a7deea475becfb90269c06775983bbf086"
 name = "nibble_vec"
 version = "0.1.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "77a5d83df9f36fe23f0c3648c6bbb8b0298bb5f1939c8f2704431371f4b84d43"
-dependencies = [
- "smallvec",
-]
-
-[[package]]
-name = "nix"
-version = "0.28.0"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "ab2156c4fce2f8df6c499cc1c763e4394b7482525bf2a9701c9d79d215f519e4"
-dependencies = [
- "bitflags 2.10.0",
- "cfg-if",
- "cfg_aliases 0.1.1",
- "libc",
+checksum = "77a5d83df9f36fe23f0c3648c6bbb8b0298bb5f1939c8f2704431371f4b84d43"
+dependencies = [
+ "smallvec",
 ]
 
 [[package]]
 name = "nix"
-version = "0.29.0"
+version = "0.28.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "71e2746dc3a24dd78b3cfcb7be93368c6de9963d30f43a6a73998a9cf4b17b46"
+checksum = "ab2156c4fce2f8df6c499cc1c763e4394b7482525bf2a9701c9d79d215f519e4"
 dependencies = [
  "bitflags 2.10.0",
  "cfg-if",
- "cfg_aliases 0.2.1",
+ "cfg_aliases 0.1.1",
  "libc",
- "memoffset 0.9.1",
 ]
 
 [[package]]
@@ -3907,20 +3580,6 @@ dependencies = [
  "unicode-segmentation",
 ]
 
-[[package]]
-name = "num"
-version = "0.4.3"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "35bd024e8b2ff75562e5f34e7f4905839deb4b22955ef5e73d2fea1b9813cb23"
-dependencies = [
- "num-bigint",
- "num-complex",
- "num-integer",
- "num-iter",
- "num-rational",
- "num-traits",
-]
-
 [[package]]
 name = "num-bigint"
 version = "0.4.6"
@@ -3931,15 +3590,6 @@ dependencies = [
  "num-traits",
 ]
 
-[[package]]
-name = "num-complex"
-version = "0.4.6"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "73f88a1307638156682bada9d7604135552957b7818057dcef22705b4d509495"
-dependencies = [
- "num-traits",
-]
-
 [[package]]
 name = "num-conv"
 version = "0.1.0"
@@ -3955,28 +3605,6 @@ dependencies = [
  "num-traits",
 ]
 
-[[package]]
-name = "num-iter"
-version = "0.1.45"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "1429034a0490724d0075ebb2bc9e875d6503c3cf69e235a8941aa757d83ef5bf"
-dependencies = [
- "autocfg",
- "num-integer",
- "num-traits",
-]
-
-[[package]]
-name = "num-rational"
-version = "0.4.2"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "f83d14da390562dca69fc84082e73e548e1ad308d24accdedd2720017cb37824"
-dependencies = [
- "num-bigint",
- "num-integer",
- "num-traits",
-]
-
 [[package]]
 name = "num-traits"
 version = "0.2.19"
@@ -4119,32 +3747,6 @@ version = "1.70.1"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "a4895175b425cb1f87721b59f0f286c2092bd4af812243672510e1ac53e2e0ad"
 
-[[package]]
-name = "openssl"
-version = "0.10.73"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "8505734d46c8ab1e19a1dce3aef597ad87dcb4c37e7188231769bd6bd51cebf8"
-dependencies = [
- "bitflags 2.10.0",
- "cfg-if",
- "foreign-types",
- "libc",
- "once_cell",
- "openssl-macros",
- "openssl-sys",
-]
-
-[[package]]
-name = "openssl-macros"
-version = "0.1.1"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "a948666b637a0f465e8564c73e89d4dde00d72d4d473cc972f390fc3dcee7d9c"
-dependencies = [
- "proc-macro2",
- "quote",
- "syn 2.0.104",
-]
-
 [[package]]
 name = "openssl-probe"
 version = "0.1.6"
@@ -4277,16 +3879,6 @@ version = "0.2.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "04744f49eae99ab78e0d5c0b603ab218f515ea8cfe5a456d7629ad883a3b6e7d"
 
-[[package]]
-name = "ordered-stream"
-version = "0.2.0"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "9aa2b01e1d916879f73a53d01d1d6cee68adbb31d6d9177a8cfce093cced1d50"
-dependencies = [
- "futures-core",
- "pin-project-lite",
-]
-
 [[package]]
 name = "os_info"
 version = "3.12.0"
@@ -4421,17 +4013,6 @@ version = "0.1.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "8b870d8c151b6f2fb93e84a13146138f05d02ed11c7e7c54f8826aaaf7c9f184"
 
-[[package]]
-name = "piper"
-version = "0.2.4"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "96c8c490f422ef9a4efd2cb5b42b76c8613d7e7dfc1caf667b8a3350a5acc066"
-dependencies = [
- "atomic-waker",
- "fastrand",
- "futures-io",
-]
-
 [[package]]
 name = "pkg-config"
 version = "0.3.32"
@@ -4464,20 +4045,6 @@ dependencies = [
  "miniz_oxide",
 ]
 
-[[package]]
-name = "polling"
-version = "3.11.0"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "5d0e4f59085d47d8241c88ead0f274e8a0cb551f3625263c05eb8dd897c34218"
-dependencies = [
- "cfg-if",
- "concurrent-queue",
- "hermit-abi",
- "pin-project-lite",
- "rustix 1.0.8",
- "windows-sys 0.61.1",
-]
-
 [[package]]
 name = "portable-atomic"
 version = "1.11.1"
@@ -4586,15 +4153,6 @@ dependencies = [
  "yansi",
 ]
 
-[[package]]
-name = "proc-macro-crate"
-version = "3.4.0"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "219cb19e96be00ab2e37d6e299658a0cfa83e52429179969b0f0121b4ac46983"
-dependencies = [
- "toml_edit",
-]
-
 [[package]]
 name = "proc-macro2"
 version = "1.0.95"
@@ -4696,7 +4254,7 @@ dependencies = [
  "quinn-proto",
  "quinn-udp",
  "rustc-hash 2.1.1",
- "rustls",
+ "rustls 0.23.29",
  "socket2 0.6.0",
  "thiserror 2.0.17",
  "tokio",
@@ -4716,7 +4274,7 @@ dependencies = [
  "rand 0.9.2",
  "ring",
  "rustc-hash 2.1.1",
- "rustls",
+ "rustls 0.23.29",
  "rustls-pki-types",
  "slab",
  "thiserror 2.0.17",
@@ -4952,26 +4510,21 @@ checksum = "d429f34c8092b2d42c7c93cec323bb4adeb7c67698f70839adec842ec10c7ceb"
 dependencies = [
  "base64",
  "bytes",
- "encoding_rs",
  "futures-channel",
  "futures-core",
  "futures-util",
- "h2",
  "http",
  "http-body",
  "http-body-util",
  "hyper",
  "hyper-rustls",
- "hyper-tls",
  "hyper-util",
  "js-sys",
  "log",
- "mime",
- "native-tls",
  "percent-encoding",
  "pin-project-lite",
  "quinn",
- "rustls",
+ "rustls 0.23.29",
  "rustls-native-certs",
  "rustls-pki-types",
  "serde",
@@ -4979,7 +4532,6 @@ dependencies = [
  "serde_urlencoded",
  "sync_wrapper",
  "tokio",
- "tokio-native-tls",
  "tokio-rustls",
  "tokio-util",
  "tower",
@@ -4990,7 +4542,7 @@ dependencies = [
  "wasm-bindgen-futures",
  "wasm-streams",
  "web-sys",
- "webpki-roots",
+ "webpki-roots 1.0.2",
 ]
 
 [[package]]
@@ -5107,16 +4659,31 @@ dependencies = [
  "windows-sys 0.60.2",
 ]
 
+[[package]]
+name = "rustls"
+version = "0.22.4"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "bf4ef73721ac7bcd79b2b315da7779d8fc09718c6b3d2d1b2d94850eb8c18432"
+dependencies = [
+ "log",
+ "ring",
+ "rustls-pki-types",
+ "rustls-webpki 0.102.8",
+ "subtle",
+ "zeroize",
+]
+
 [[package]]
 name = "rustls"
 version = "0.23.29"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "2491382039b29b9b11ff08b76ff6c97cf287671dbb74f0be44bda389fffe9bd1"
 dependencies = [
+ "log",
  "once_cell",
  "ring",
  "rustls-pki-types",
- "rustls-webpki",
+ "rustls-webpki 0.103.4",
  "subtle",
  "zeroize",
 ]
@@ -5130,7 +4697,7 @@ dependencies = [
  "openssl-probe",
  "rustls-pki-types",
  "schannel",
- "security-framework 3.5.1",
+ "security-framework",
 ]
 
 [[package]]
@@ -5143,6 +4710,17 @@ dependencies = [
  "zeroize",
 ]
 
+[[package]]
+name = "rustls-webpki"
+version = "0.102.8"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "64ca1bc8749bd4cf37b5ce386cc146580777b4e8572c7b97baf22c83f444bee9"
+dependencies = [
+ "ring",
+ "rustls-pki-types",
+ "untrusted",
+]
+
 [[package]]
 name = "rustls-webpki"
 version = "0.103.4"
@@ -5340,38 +4918,6 @@ dependencies = [
  "libc",
 ]
 
-[[package]]
-name = "secret-service"
-version = "4.0.0"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "e4d35ad99a181be0a60ffcbe85d680d98f87bdc4d7644ade319b87076b9dbfd4"
-dependencies = [
- "aes",
- "cbc",
- "futures-util",
- "generic-array",
- "hkdf",
- "num",
- "once_cell",
- "rand 0.8.5",
- "serde",
- "sha2",
- "zbus",
-]
-
-[[package]]
-name = "security-framework"
-version = "2.11.1"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "897b2245f0b511c87893af39b033e5ca9cce68824c4d7e7630b5a1d339658d02"
-dependencies = [
- "bitflags 2.10.0",
- "core-foundation 0.9.4",
- "core-foundation-sys",
- "libc",
- "security-framework-sys",
-]
-
 [[package]]
 name = "security-framework"
 version = "3.5.1"
@@ -5408,8 +4954,8 @@ source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "5484316556650182f03b43d4c746ce0e3e48074a21e2f51244b648b6542e1066"
 dependencies = [
  "httpdate",
- "native-tls",
  "reqwest",
+ "rustls 0.22.4",
  "sentry-backtrace",
  "sentry-contexts",
  "sentry-core",
@@ -5418,6 +4964,7 @@ dependencies = [
  "sentry-tracing",
  "tokio",
  "ureq",
+ "webpki-roots 0.26.11",
 ]
 
 [[package]]
@@ -6077,27 +5624,6 @@ dependencies = [
  "libc",
 ]
 
-[[package]]
-name = "system-configuration"
-version = "0.6.1"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "3c879d448e9d986b661742763247d3693ed13609438cf3d006f51f5368a5ba6b"
-dependencies = [
- "bitflags 2.10.0",
- "core-foundation 0.9.4",
- "system-configuration-sys",
-]
-
-[[package]]
-name = "system-configuration-sys"
-version = "0.6.0"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "8e1d1b10ced5ca923a1fcb8d03e96b8d3268065d724548c0211415ff6ac6bac4"
-dependencies = [
- "core-foundation-sys",
- "libc",
-]
-
 [[package]]
 name = "tempfile"
 version = "3.23.0"
@@ -6377,23 +5903,13 @@ dependencies = [
  "syn 2.0.104",
 ]
 
-[[package]]
-name = "tokio-native-tls"
-version = "0.3.1"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "bbae76ab933c85776efabc971569dd6119c580d8f5d448769dec1764bf796ef2"
-dependencies = [
- "native-tls",
- "tokio",
-]
-
 [[package]]
 name = "tokio-rustls"
 version = "0.26.2"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "8e727b36a1a0e8b74c376ac2211e40c2c8af09fb4013c60d910495810f008e9b"
 dependencies = [
- "rustls",
+ "rustls 0.23.29",
  "tokio",
 ]
 
@@ -6835,9 +6351,11 @@ checksum = "02d1a66277ed75f640d608235660df48c8e3c19f3b4edb6a263315626cc3c01d"
 dependencies = [
  "base64",
  "log",
- "native-tls",
  "once_cell",
+ "rustls 0.23.29",
+ "rustls-pki-types",
  "url",
+ "webpki-roots 0.26.11",
 ]
 
 [[package]]
@@ -7085,6 +6603,15 @@ dependencies = [
  "web-sys",
 ]
 
+[[package]]
+name = "webpki-roots"
+version = "0.26.11"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "521bc38abb08001b01866da9f51eb7c5d647a19260e00054a8c7fd5f9e57f7a9"
+dependencies = [
+ "webpki-roots 1.0.2",
+]
+
 [[package]]
 name = "webpki-roots"
 version = "1.0.2"
@@ -7239,17 +6766,6 @@ dependencies = [
  "windows-link 0.1.3",
 ]
 
-[[package]]
-name = "windows-registry"
-version = "0.5.3"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "5b8a9ed28765efc97bbc954883f4e6796c33a06546ebafacbabee9696967499e"
-dependencies = [
- "windows-link 0.1.3",
- "windows-result",
- "windows-strings",
-]
-
 [[package]]
 name = "windows-result"
 version = "0.3.4"
@@ -7643,16 +7159,6 @@ version = "0.13.1"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "ec107c4503ea0b4a98ef47356329af139c0a4f7750e621cf2973cd3385ebcb3d"
 
-[[package]]
-name = "xdg-home"
-version = "1.3.0"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "ec1cdab258fb55c0da61328dc52c8764709b249011b2cad0454c72f0bf10a1f6"
-dependencies = [
- "libc",
- "windows-sys 0.59.0",
-]
-
 [[package]]
 name = "yansi"
 version = "1.0.1"
@@ -7683,68 +7189,6 @@ dependencies = [
  "synstructure",
 ]
 
-[[package]]
-name = "zbus"
-version = "4.4.0"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "bb97012beadd29e654708a0fdb4c84bc046f537aecfde2c3ee0a9e4b4d48c725"
-dependencies = [
- "async-broadcast",
- "async-executor",
- "async-fs",
- "async-io",
- "async-lock",
- "async-process",
- "async-recursion",
- "async-task",
- "async-trait",
- "blocking",
- "enumflags2",
- "event-listener",
- "futures-core",
- "futures-sink",
- "futures-util",
- "hex",
- "nix 0.29.0",
- "ordered-stream",
- "rand 0.8.5",
- "serde",
- "serde_repr",
- "sha1",
- "static_assertions",
- "tracing",
- "uds_windows",
- "windows-sys 0.52.0",
- "xdg-home",
- "zbus_macros",
- "zbus_names",
- "zvariant",
-]
-
-[[package]]
-name = "zbus_macros"
-version = "4.4.0"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "267db9407081e90bbfa46d841d3cbc60f59c0351838c4bc65199ecd79ab1983e"
-dependencies = [
- "proc-macro-crate",
- "proc-macro2",
- "quote",
- "syn 2.0.104",
- "zvariant_utils",
-]
-
-[[package]]
-name = "zbus_names"
-version = "3.0.0"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "4b9b1fef7d021261cc16cba64c351d291b715febe0fa10dc3a443ac5a5022e6c"
-dependencies = [
- "serde",
- "static_assertions",
- "zvariant",
-]
-
 [[package]]
 name = "zerocopy"
 version = "0.8.26"
@@ -7791,20 +7235,6 @@ name = "zeroize"
 version = "1.8.1"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "ced3678a2879b30306d323f4542626697a464a97c0a07c9aebf7ebca65cd4dde"
-dependencies = [
- "zeroize_derive",
-]
-
-[[package]]
-name = "zeroize_derive"
-version = "1.4.2"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "ce36e65b0d2999d2aafac989fb249189a141aee1f53c612c1f37d72631959f69"
-dependencies = [
- "proc-macro2",
- "quote",
- "syn 2.0.104",
-]
 
 [[package]]
 name = "zerotrie"
@@ -7854,40 +7284,3 @@ checksum = "2c9e525af0a6a658e031e95f14b7f889976b74a11ba0eca5a5fc9ac8a1c43a6a"
 dependencies = [
  "zune-core",
 ]
-
-[[package]]
-name = "zvariant"
-version = "4.2.0"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "2084290ab9a1c471c38fc524945837734fbf124487e105daec2bb57fd48c81fe"
-dependencies = [
- "endi",
- "enumflags2",
- "serde",
- "static_assertions",
- "zvariant_derive",
-]
-
-[[package]]
-name = "zvariant_derive"
-version = "4.2.0"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "73e2ba546bda683a90652bac4a279bc146adad1386f25379cf73200d2002c449"
-dependencies = [
- "proc-macro-crate",
- "proc-macro2",
- "quote",
- "syn 2.0.104",
- "zvariant_utils",
-]
-
-[[package]]
-name = "zvariant_utils"
-version = "2.1.0"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "c51bcff7cc3dbb5055396bcf774748c3dab426b4b8659046963523cee4808340"
-dependencies = [
- "proc-macro2",
- "quote",
- "syn 2.0.104",
-]
diff --git a/codex-rs/Cargo.toml b/codex-rs/Cargo.toml
index a727e1442..5abccd2d1 100644
--- a/codex-rs/Cargo.toml
+++ b/codex-rs/Cargo.toml
@@ -2,6 +2,7 @@
 members = [
     "backend-client",
     "ansi-escape",
+    "build-info",
     "async-utils",
     "app-server",
     "app-server-protocol",
@@ -13,6 +14,7 @@ members = [
     "cloud-tasks-client",
     "cli",
     "common",
+    "codex-litellm-model-session-telemetry",
     "core",
     "exec",
     "execpolicy",
@@ -60,6 +62,7 @@ codex-apply-patch = { path = "apply-patch" }
 codex-arg0 = { path = "arg0" }
 codex-async-utils = { path = "async-utils" }
 codex-backend-client = { path = "backend-client" }
+codex-build-info = { path = "build-info" }
 codex-chatgpt = { path = "chatgpt" }
 codex-common = { path = "common" }
 codex-core = { path = "core" }
@@ -68,6 +71,7 @@ codex-feedback = { path = "feedback" }
 codex-file-search = { path = "file-search" }
 codex-git = { path = "utils/git" }
 codex-keyring-store = { path = "keyring-store" }
+codex-litellm-model-session-telemetry = { path = "codex-litellm-model-session-telemetry" }
 codex-linux-sandbox = { path = "linux-sandbox" }
 codex-login = { path = "login" }
 codex-mcp-server = { path = "mcp-server" }
@@ -142,7 +146,8 @@ mime_guess = "2.0.5"
 multimap = "0.10.0"
 notify = "8.2.0"
 nucleo-matcher = "0.3.1"
-openssl-sys = "*"
+openssl = { version = "0.10", features = ["vendored"] }
+openssl-sys = { version = "0.9", features = ["vendored"] }
 opentelemetry = "0.30.0"
 opentelemetry-appender-tracing = "0.30.0"
 opentelemetry-otlp = "0.30.0"
@@ -161,11 +166,18 @@ rand = "0.9"
 ratatui = "0.29.0"
 ratatui-macros = "0.6.0"
 regex-lite = "0.1.7"
-reqwest = "0.12"
+reqwest = { version = "0.12", default-features = false, features = ["blocking", "json", "rustls-tls", "stream"] }
 rmcp = { version = "0.8.3", default-features = false }
 schemars = "0.8.22"
 seccompiler = "0.5.0"
-sentry = "0.34.0"
+sentry = { version = "0.34.0", default-features = false, features = [
+    "backtrace",
+    "contexts",
+    "debug-images",
+    "panic",
+    "reqwest",
+    "rustls",
+] }
 serde = "1"
 serde_json = "1"
 serde_with = "3.14"
@@ -194,7 +206,7 @@ toml_edit = "0.23.4"
 tonic = "0.13.1"
 tracing = "0.1.41"
 tracing-appender = "0.2.3"
-tracing-subscriber = "0.3.20"
+tracing-subscriber = { version = "0.3.20", features = ["env-filter", "fmt", "registry"] }
 tracing-test = "0.2.5"
 tree-sitter = "0.25.10"
 tree-sitter-bash = "0.25"
diff --git a/codex-rs/app-server/src/codex_message_processor.rs b/codex-rs/app-server/src/codex_message_processor.rs
index 8ea4cef8b..ba38605bb 100644
--- a/codex-rs/app-server/src/codex_message_processor.rs
+++ b/codex-rs/app-server/src/codex_message_processor.rs
@@ -1825,6 +1825,7 @@ async fn derive_config_from_params(
         tools_web_search_request: None,
         experimental_sandbox_command_assessment: None,
         additional_writable_roots: Vec::new(),
+        telemetry_enabled: None,
     };
 
     let cli_overrides = cli_overrides
diff --git a/codex-rs/app-server/src/models.rs b/codex-rs/app-server/src/models.rs
index be47cbc2c..36d3f09f5 100644
--- a/codex-rs/app-server/src/models.rs
+++ b/codex-rs/app-server/src/models.rs
@@ -16,7 +16,7 @@ fn model_from_preset(preset: ModelPreset) -> Model {
         id: preset.id.to_string(),
         model: preset.model.to_string(),
         display_name: preset.display_name.to_string(),
-        description: preset.description.to_string(),
+        description: preset.description.clone().unwrap_or_default(),
         supported_reasoning_efforts: reasoning_efforts_from_preset(
             preset.supported_reasoning_efforts,
         ),
diff --git a/codex-rs/cli/Cargo.toml b/codex-rs/cli/Cargo.toml
index 041644b34..02f889c82 100644
--- a/codex-rs/cli/Cargo.toml
+++ b/codex-rs/cli/Cargo.toml
@@ -23,6 +23,7 @@ codex-app-server-protocol = { workspace = true }
 codex-arg0 = { workspace = true }
 codex-chatgpt = { workspace = true }
 codex-cloud-tasks = { path = "../cloud-tasks" }
+codex-build-info = { workspace = true }
 codex-common = { workspace = true, features = ["cli"] }
 codex-core = { workspace = true }
 codex-exec = { workspace = true }
diff --git a/codex-rs/cli/src/main.rs b/codex-rs/cli/src/main.rs
index 929fd9e78..2c28d09eb 100644
--- a/codex-rs/cli/src/main.rs
+++ b/codex-rs/cli/src/main.rs
@@ -1,4 +1,5 @@
 use clap::CommandFactory;
+use clap::FromArgMatches;
 use clap::Parser;
 use clap_complete::Shell;
 use clap_complete::generate;
@@ -22,6 +23,7 @@ use codex_tui::AppExitInfo;
 use codex_tui::Cli as TuiCli;
 use codex_tui::updates::UpdateAction;
 use owo_colors::OwoColorize;
+use std::ffi::OsString;
 use std::path::PathBuf;
 use supports_color::Stream;
 
@@ -32,13 +34,15 @@ use codex_core::config::Config;
 use codex_core::config::ConfigOverrides;
 use codex_core::features::is_known_feature_key;
 
+const CODEX_LITELLM_VERSION: &str = codex_build_info::VERSION_WITH_COMMIT;
+
 /// Codex CLI
 ///
 /// If no subcommand is specified, options will be forwarded to the interactive CLI.
 #[derive(Debug, Parser)]
 #[clap(
     author,
-    version,
+    version = CODEX_LITELLM_VERSION,
     // If a sub‑command is given, ignore requirements of the default args.
     subcommand_negates_reqs = true,
     // The executable is sometimes invoked via a platform‑specific name like
@@ -344,7 +348,32 @@ fn pre_main_hardening() {
     codex_process_hardening::pre_main_hardening();
 }
 
+fn parse_multitool_cli() -> MultitoolCli {
+    let mut command = MultitoolCli::command();
+    command = command
+        .version(CODEX_LITELLM_VERSION)
+        .long_version(CODEX_LITELLM_VERSION);
+    MultitoolCli::from_arg_matches(&command.get_matches()).unwrap_or_else(|err| err.exit())
+}
+
+fn maybe_print_version_and_exit() -> bool {
+    let mut args = std::env::args_os();
+    let _ = args.next();
+    match (args.next(), args.next()) {
+        (Some(flag), None)
+            if flag == OsString::from("--version") || flag == OsString::from("-V") =>
+        {
+            println!("codex-litellm {}", CODEX_LITELLM_VERSION);
+            true
+        }
+        _ => false,
+    }
+}
+
 fn main() -> anyhow::Result<()> {
+    if maybe_print_version_and_exit() {
+        return Ok(());
+    }
     arg0_dispatch_or_else(|codex_linux_sandbox_exe| async move {
         cli_main(codex_linux_sandbox_exe).await?;
         Ok(())
@@ -357,7 +386,7 @@ async fn cli_main(codex_linux_sandbox_exe: Option<PathBuf>) -> anyhow::Result<()
         feature_toggles,
         mut interactive,
         subcommand,
-    } = MultitoolCli::parse();
+    } = parse_multitool_cli();
 
     // Fold --enable/--disable into config overrides so they flow to all subcommands.
     let toggle_overrides = feature_toggles.to_overrides()?;
@@ -525,6 +554,8 @@ async fn cli_main(codex_linux_sandbox_exe: Option<PathBuf>) -> anyhow::Result<()
                 // Thread through relevant top-level flags (at minimum, `--profile`).
                 let overrides = ConfigOverrides {
                     config_profile: interactive.config_profile.clone(),
+                    tools_web_search_request: interactive.web_search.then_some(true),
+                    telemetry_enabled: None,
                     ..Default::default()
                 };
 
diff --git a/codex-rs/common/Cargo.toml b/codex-rs/common/Cargo.toml
index d8f30cc09..2c667b84f 100644
--- a/codex-rs/common/Cargo.toml
+++ b/codex-rs/common/Cargo.toml
@@ -11,11 +11,14 @@ clap = { workspace = true, features = ["derive", "wrap_help"], optional = true }
 codex-core = { workspace = true }
 codex-protocol = { workspace = true }
 codex-app-server-protocol = { workspace = true }
+codex-build-info = { workspace = true }
 serde = { workspace = true, optional = true }
+serde_json = { workspace = true, optional = true }
+reqwest = { workspace = true, features = ["blocking", "json"], optional = true }
 toml = { workspace = true, optional = true }
 
 [features]
 # Separate feature so that `clap` is not a mandatory dependency.
-cli = ["clap", "serde", "toml"]
+cli = ["clap", "serde", "serde_json", "toml", "reqwest"]
 elapsed = []
 sandbox_summary = []
diff --git a/codex-rs/common/src/lib.rs b/codex-rs/common/src/lib.rs
index 276bfca06..c445cec51 100644
--- a/codex-rs/common/src/lib.rs
+++ b/codex-rs/common/src/lib.rs
@@ -30,6 +30,7 @@ pub use sandbox_summary::summarize_sandbox_policy;
 mod config_summary;
 
 pub use config_summary::create_config_summary_entries;
+pub mod litellm;
 // Shared fuzzy matcher (used by TUI selection popups and other UI filtering)
 pub mod fuzzy_match;
 // Shared model presets used by TUI and MCP server
diff --git a/codex-rs/common/src/model_presets.rs b/codex-rs/common/src/model_presets.rs
index b6d064380..3947ebb46 100644
--- a/codex-rs/common/src/model_presets.rs
+++ b/codex-rs/common/src/model_presets.rs
@@ -1,5 +1,9 @@
 use codex_app_server_protocol::AuthMode;
 use codex_core::protocol_config_types::ReasoningEffort;
+use reqwest::blocking::Client;
+use serde::Deserialize;
+use serde_json::Value;
+use std::collections::HashSet;
 
 /// A reasoning effort option that can be surfaced for a model.
 #[derive(Debug, Clone, Copy)]
@@ -11,16 +15,16 @@ pub struct ReasoningEffortPreset {
 }
 
 /// Metadata describing a Codex-supported model.
-#[derive(Debug, Clone, Copy)]
+#[derive(Debug, Clone)]
 pub struct ModelPreset {
     /// Stable identifier for the preset.
-    pub id: &'static str,
+    pub id: String,
     /// Model slug (e.g., "gpt-5").
-    pub model: &'static str,
+    pub model: String,
     /// Display name shown in UIs.
-    pub display_name: &'static str,
+    pub display_name: String,
     /// Short human description shown in UIs.
-    pub description: &'static str,
+    pub description: Option<String>,
     /// Reasoning effort applied when none is explicitly chosen.
     pub default_reasoning_effort: ReasoningEffort,
     /// Supported reasoning effort options.
@@ -29,68 +33,278 @@ pub struct ModelPreset {
     pub is_default: bool,
 }
 
-const PRESETS: &[ModelPreset] = &[
-    ModelPreset {
-        id: "gpt-5-codex",
-        model: "gpt-5-codex",
-        display_name: "gpt-5-codex",
-        description: "Optimized for coding tasks with many tools.",
-        default_reasoning_effort: ReasoningEffort::Medium,
-        supported_reasoning_efforts: &[
-            ReasoningEffortPreset {
-                effort: ReasoningEffort::Low,
-                description: "Fastest responses with limited reasoning",
-            },
-            ReasoningEffortPreset {
-                effort: ReasoningEffort::Medium,
-                description: "Dynamically adjusts reasoning based on the task",
-            },
-            ReasoningEffortPreset {
-                effort: ReasoningEffort::High,
-                description: "Maximizes reasoning depth for complex or ambiguous problems",
-            },
-        ],
-        is_default: true,
+const STANDARD_REASONING_PRESETS: &[ReasoningEffortPreset] = &[
+    ReasoningEffortPreset {
+        effort: ReasoningEffort::Low,
+        description: "Fastest responses with limited reasoning",
     },
-    ModelPreset {
-        id: "gpt-5",
-        model: "gpt-5",
-        display_name: "gpt-5",
-        description: "Broad world knowledge with strong general reasoning.",
-        default_reasoning_effort: ReasoningEffort::Medium,
-        supported_reasoning_efforts: &[
-            ReasoningEffortPreset {
-                effort: ReasoningEffort::Minimal,
-                description: "Fastest responses with little reasoning",
-            },
-            ReasoningEffortPreset {
-                effort: ReasoningEffort::Low,
-                description: "Balances speed with some reasoning; useful for straightforward queries and short explanations",
-            },
-            ReasoningEffortPreset {
-                effort: ReasoningEffort::Medium,
-                description: "Provides a solid balance of reasoning depth and latency for general-purpose tasks",
-            },
-            ReasoningEffortPreset {
-                effort: ReasoningEffort::High,
-                description: "Maximizes reasoning depth for complex or ambiguous problems",
-            },
-        ],
-        is_default: false,
+    ReasoningEffortPreset {
+        effort: ReasoningEffort::Medium,
+        description: "Dynamically adjusts reasoning based on the task",
+    },
+    ReasoningEffortPreset {
+        effort: ReasoningEffort::High,
+        description: "Maximizes reasoning depth for complex or ambiguous problems",
+    },
+];
+
+const GPT5_REASONING_PRESETS: &[ReasoningEffortPreset] = &[
+    ReasoningEffortPreset {
+        effort: ReasoningEffort::Minimal,
+        description: "Fastest responses with little reasoning",
+    },
+    ReasoningEffortPreset {
+        effort: ReasoningEffort::Low,
+        description: "Balances speed with some reasoning; useful for straightforward queries and short explanations",
+    },
+    ReasoningEffortPreset {
+        effort: ReasoningEffort::Medium,
+        description: "Provides a solid balance of reasoning depth and latency for general-purpose tasks",
+    },
+    ReasoningEffortPreset {
+        effort: ReasoningEffort::High,
+        description: "Maximizes reasoning depth for complex or ambiguous problems",
     },
 ];
 
 pub fn builtin_model_presets(_auth_mode: Option<AuthMode>) -> Vec<ModelPreset> {
-    PRESETS.to_vec()
+    vec![
+        ModelPreset {
+            id: "gpt-5-codex".to_string(),
+            model: "gpt-5-codex".to_string(),
+            display_name: "gpt-5-codex".to_string(),
+            description: Some("Optimized for coding tasks with many tools.".to_string()),
+            default_reasoning_effort: ReasoningEffort::Medium,
+            supported_reasoning_efforts: STANDARD_REASONING_PRESETS,
+            is_default: true,
+        },
+        ModelPreset {
+            id: "gpt-5".to_string(),
+            model: "gpt-5".to_string(),
+            display_name: "gpt-5".to_string(),
+            description: Some("Broad world knowledge with strong general reasoning.".to_string()),
+            default_reasoning_effort: ReasoningEffort::Medium,
+            supported_reasoning_efforts: GPT5_REASONING_PRESETS,
+            is_default: false,
+        },
+    ]
+}
+
+pub fn make_litellm_model_preset(model: &str, is_default: bool) -> ModelPreset {
+    ModelPreset {
+        id: model.to_string(),
+        model: model.to_string(),
+        display_name: model.to_string(),
+        description: None,
+        default_reasoning_effort: ReasoningEffort::Medium,
+        supported_reasoning_efforts: STANDARD_REASONING_PRESETS,
+        is_default,
+    }
+}
+
+pub const LITELLM_PROVIDER_ID: &str = "litellm";
+pub const LEGACY_LITELLM_PROVIDER_ID: &str = "litellm-direct";
+pub const LITELLM_DEFAULT_MODEL: &str = "";
+
+pub fn is_litellm_provider_id(provider_id: &str) -> bool {
+    matches!(
+        provider_id,
+        LITELLM_PROVIDER_ID | LEGACY_LITELLM_PROVIDER_ID
+    )
+}
+
+pub fn presets_for_provider(
+    provider_id: &str,
+    current_model: &str,
+    auth_mode: Option<AuthMode>,
+) -> Vec<ModelPreset> {
+    if matches!(
+        provider_id,
+        LITELLM_PROVIDER_ID | LEGACY_LITELLM_PROVIDER_ID
+    ) {
+        let mut presets = Vec::new();
+        presets.push(make_litellm_model_preset(current_model, true));
+        presets
+    } else {
+        builtin_model_presets(auth_mode)
+    }
+}
+
+#[derive(Debug, Deserialize)]
+struct LiteLlmModelEntry {
+    #[serde(default)]
+    id: Option<String>,
+    #[serde(default)]
+    model: Option<String>,
+    #[serde(default)]
+    description: Option<String>,
+    #[serde(default)]
+    display_name: Option<String>,
+}
+
+pub fn fetch_litellm_model_presets(
+    base_url: &str,
+    api_key: Option<&str>,
+    current_model: &str,
+) -> Result<Vec<ModelPreset>, String> {
+    let base = base_url.to_string();
+    let key = api_key.map(|k| k.to_string());
+    let current = current_model.to_string();
+
+    std::thread::spawn(move || {
+        fetch_litellm_model_presets_blocking(&base, key.as_deref(), &current)
+    })
+    .join()
+    .map_err(|_| "LiteLLM /models worker panicked".to_string())?
+}
+
+fn fetch_litellm_model_presets_blocking(
+    base_url: &str,
+    api_key: Option<&str>,
+    current_model: &str,
+) -> Result<Vec<ModelPreset>, String> {
+    let trimmed = base_url.trim_end_matches('/');
+    let url = format!("{trimmed}/models");
+
+    let client = Client::builder()
+        .user_agent("codex-litellm")
+        .build()
+        .map_err(|err| format!("failed to create HTTP client: {err}"))?;
+
+    let mut request = client.get(&url);
+    if let Some(token) = api_key {
+        request = request.bearer_auth(token);
+    }
+
+    let response = request
+        .send()
+        .map_err(|err| format!("LiteLLM /models request failed: {err}"))?;
+
+    if !response.status().is_success() {
+        return Err(format!(
+            "LiteLLM /models responded with status {}",
+            response.status()
+        ));
+    }
+
+    let payload: Value = response
+        .json()
+        .map_err(|err| format!("failed to decode LiteLLM /models response: {err}"))?;
+
+    let mut presets = extract_litellm_models(payload)?;
+    if presets.is_empty() {
+        return Err("LiteLLM /models returned an empty list".to_string());
+    }
+
+    sanitize_litellm_presets(&mut presets, current_model);
+
+    Ok(presets)
+}
+
+fn sanitize_litellm_presets(presets: &mut Vec<ModelPreset>, current_model: &str) {
+    let mut current_pos = None;
+    for (idx, preset) in presets.iter_mut().enumerate() {
+        if preset.model == current_model {
+            preset.is_default = true;
+            current_pos = Some(idx);
+        } else {
+            preset.is_default = false;
+        }
+    }
+
+    if let Some(pos) = current_pos {
+        if pos != 0 {
+            let preset = presets.remove(pos);
+            presets.insert(0, preset);
+        }
+    } else if !current_model.is_empty() {
+        // Ensure the current model is present as the default choice.
+        presets.insert(0, make_litellm_model_preset(current_model, true));
+    }
+}
+
+fn extract_litellm_models(payload: Value) -> Result<Vec<ModelPreset>, String> {
+    let array_value = payload
+        .get("data")
+        .or_else(|| payload.get("models"))
+        .or_else(|| payload.get("result"))
+        .cloned()
+        .unwrap_or(payload);
+
+    let items = match array_value {
+        Value::Array(values) => values,
+        other => return Err(format!("unexpected LiteLLM /models payload: {other:?}")),
+    };
+
+    let mut presets = Vec::new();
+    let mut seen: HashSet<String> = HashSet::new();
+    for item in items {
+        match item {
+            Value::String(slug) => {
+                if seen.insert(slug.clone()) {
+                    presets.push(make_litellm_model_preset(&slug, false));
+                }
+            }
+            Value::Object(map) => {
+                let entry: LiteLlmModelEntry = serde_json::from_value(Value::Object(map))
+                    .map_err(|err| format!("failed to parse LiteLLM model entry: {err}"))?;
+                if let Some(slug) = entry.id.or(entry.model) {
+                    if seen.insert(slug.clone()) {
+                        let mut preset = make_litellm_model_preset(&slug, false);
+                        if let Some(name) = entry.display_name.clone() {
+                            preset.display_name = name;
+                        }
+                        preset.description = entry.description;
+                        presets.push(preset);
+                    }
+                }
+            }
+            _ => {}
+        }
+    }
+    Ok(presets)
 }
 
 #[cfg(test)]
 mod tests {
     use super::*;
+    use serde_json::json;
 
     #[test]
     fn only_one_default_model_is_configured() {
-        let default_models = PRESETS.iter().filter(|preset| preset.is_default).count();
-        assert!(default_models == 1);
+        let default_models = builtin_model_presets(None)
+            .into_iter()
+            .filter(|preset| preset.is_default)
+            .count();
+        assert_eq!(default_models, 1);
+    }
+
+    #[test]
+    fn litellm_presets_include_current_model_first() {
+        let current = "or/minimax-m2:free";
+        let presets = presets_for_provider(LITELLM_PROVIDER_ID, current, None);
+        assert!(!presets.is_empty());
+        assert_eq!(presets[0].model, current);
+        assert!(presets[0].is_default);
+
+        let legacy = presets_for_provider(LEGACY_LITELLM_PROVIDER_ID, current, None);
+        assert!(!legacy.is_empty());
+        assert_eq!(legacy[0].model, current);
+        assert!(legacy[0].is_default);
+    }
+
+    #[test]
+    fn extract_models_handles_common_shapes() {
+        let payload = json!({
+            "data": [
+                { "id": "or/minimax-m2:free", "description": "MiniMax Lite" },
+                { "model": "vertex/gpt-oss-20b", "display_name": "GPT OSS 20B" },
+                "vertex/gpt-oss-120b"
+            ]
+        });
+        let presets = extract_litellm_models(payload).expect("parse models");
+        assert_eq!(presets.len(), 3);
+        assert_eq!(presets[0].model, "or/minimax-m2:free");
+        assert_eq!(presets[1].display_name, "GPT OSS 20B".to_string());
     }
 }
diff --git a/codex-rs/core/Cargo.toml b/codex-rs/core/Cargo.toml
index 17a7a1660..3a6ce6ead 100644
--- a/codex-rs/core/Cargo.toml
+++ b/codex-rs/core/Cargo.toml
@@ -39,12 +39,7 @@ eventsource-stream = { workspace = true }
 futures = { workspace = true }
 http = { workspace = true }
 indexmap = { workspace = true }
-keyring = { workspace = true, features = [
-    "apple-native",
-    "crypto-rust",
-    "linux-native-async-persistent",
-    "windows-native",
-] }
+keyring = { workspace = true }
 libc = { workspace = true }
 mcp-types = { workspace = true }
 os_info = { workspace = true }
diff --git a/codex-rs/core/src/chat_completions.rs b/codex-rs/core/src/chat_completions.rs
index abb27d9b5..410f099b2 100644
--- a/codex-rs/core/src/chat_completions.rs
+++ b/codex-rs/core/src/chat_completions.rs
@@ -1,6 +1,9 @@
-use std::time::Duration;
+use std::collections::HashMap;
+use std::time::{Duration, Instant};
 
 use crate::ModelProviderInfo;
+use crate::assistant_message::format_noise_reasoning;
+use crate::assistant_message::is_noise_assistant_message;
 use crate::client_common::Prompt;
 use crate::client_common::ResponseEvent;
 use crate::client_common::ResponseStream;
@@ -18,23 +21,33 @@ use bytes::Bytes;
 use codex_otel::otel_event_manager::OtelEventManager;
 use codex_protocol::models::ContentItem;
 use codex_protocol::models::FunctionCallOutputContentItem;
+use codex_protocol::models::FunctionCallOutputPayload;
+use codex_protocol::models::LocalShellAction;
 use codex_protocol::models::ReasoningItemContent;
 use codex_protocol::models::ResponseItem;
 use codex_protocol::protocol::SessionSource;
 use codex_protocol::protocol::SubAgentSource;
+use codex_protocol::protocol::TokenUsage;
 use eventsource_stream::Eventsource;
 use futures::Stream;
 use futures::StreamExt;
 use futures::TryStreamExt;
 use reqwest::StatusCode;
-use serde_json::json;
+use serde::Deserialize;
+use serde_json::{Value, json};
 use std::pin::Pin;
 use std::task::Context;
 use std::task::Poll;
 use tokio::sync::mpsc;
 use tokio::time::timeout;
-use tracing::debug;
-use tracing::trace;
+use tracing::{debug, info, trace, warn};
+use uuid::Uuid;
+
+pub(crate) const SUMMARY_FOLLOWUP_PROMPT: &str = "Pick up from the latest tool output and finish the user’s request. Prefer giving a direct explanation or summary instead of editing files unless the user explicitly asked for changes. If something is still incomplete, describe the remaining steps or run only the minimal tool needed to wrap up, then provide the final answer.";
+
+const MAX_BUFFERED_FOLLOWUP_ATTEMPTS: usize = 2;
+const MAX_AUTOPILOT_ONLY_TURNS: usize = 2;
+const BUFFERED_REQUEST_TIMEOUT_SECS: u64 = 45;
 
 /// Implementation for the classic Chat Completions API.
 pub(crate) async fn stream_chat_completions(
@@ -51,290 +64,24 @@ pub(crate) async fn stream_chat_completions(
         ));
     }
 
-    // Build messages array
-    let mut messages = Vec::<serde_json::Value>::new();
-
-    let full_instructions = prompt.get_full_instructions(model_family);
-    messages.push(json!({"role": "system", "content": full_instructions}));
-
-    let input = prompt.get_formatted_input();
-
-    // Pre-scan: map Reasoning blocks to the adjacent assistant anchor after the last user.
-    // - If the last emitted message is a user message, drop all reasoning.
-    // - Otherwise, for each Reasoning item after the last user message, attach it
-    //   to the immediate previous assistant message (stop turns) or the immediate
-    //   next assistant anchor (tool-call turns: function/local shell call, or assistant message).
-    let mut reasoning_by_anchor_index: std::collections::HashMap<usize, String> =
-        std::collections::HashMap::new();
-
-    // Determine the last role that would be emitted to Chat Completions.
-    let mut last_emitted_role: Option<&str> = None;
-    for item in &input {
-        match item {
-            ResponseItem::Message { role, .. } => last_emitted_role = Some(role.as_str()),
-            ResponseItem::FunctionCall { .. } | ResponseItem::LocalShellCall { .. } => {
-                last_emitted_role = Some("assistant")
-            }
-            ResponseItem::FunctionCallOutput { .. } => last_emitted_role = Some("tool"),
-            ResponseItem::Reasoning { .. } | ResponseItem::Other => {}
-            ResponseItem::CustomToolCall { .. } => {}
-            ResponseItem::CustomToolCallOutput { .. } => {}
-            ResponseItem::WebSearchCall { .. } => {}
-            ResponseItem::GhostSnapshot { .. } => {}
-        }
-    }
-
-    // Find the last user message index in the input.
-    let mut last_user_index: Option<usize> = None;
-    for (idx, item) in input.iter().enumerate() {
-        if let ResponseItem::Message { role, .. } = item
-            && role == "user"
-        {
-            last_user_index = Some(idx);
-        }
-    }
-
-    // Attach reasoning only if the conversation does not end with a user message.
-    if !matches!(last_emitted_role, Some("user")) {
-        for (idx, item) in input.iter().enumerate() {
-            // Only consider reasoning that appears after the last user message.
-            if let Some(u_idx) = last_user_index
-                && idx <= u_idx
-            {
-                continue;
-            }
-
-            if let ResponseItem::Reasoning {
-                content: Some(items),
-                ..
-            } = item
-            {
-                let mut text = String::new();
-                for entry in items {
-                    match entry {
-                        ReasoningItemContent::ReasoningText { text: segment }
-                        | ReasoningItemContent::Text { text: segment } => text.push_str(segment),
-                    }
-                }
-                if text.trim().is_empty() {
-                    continue;
-                }
-
-                // Prefer immediate previous assistant message (stop turns)
-                let mut attached = false;
-                if idx > 0
-                    && let ResponseItem::Message { role, .. } = &input[idx - 1]
-                    && role == "assistant"
-                {
-                    reasoning_by_anchor_index
-                        .entry(idx - 1)
-                        .and_modify(|v| v.push_str(&text))
-                        .or_insert(text.clone());
-                    attached = true;
-                }
-
-                // Otherwise, attach to immediate next assistant anchor (tool-calls or assistant message)
-                if !attached && idx + 1 < input.len() {
-                    match &input[idx + 1] {
-                        ResponseItem::FunctionCall { .. } | ResponseItem::LocalShellCall { .. } => {
-                            reasoning_by_anchor_index
-                                .entry(idx + 1)
-                                .and_modify(|v| v.push_str(&text))
-                                .or_insert(text.clone());
-                        }
-                        ResponseItem::Message { role, .. } if role == "assistant" => {
-                            reasoning_by_anchor_index
-                                .entry(idx + 1)
-                                .and_modify(|v| v.push_str(&text))
-                                .or_insert(text.clone());
-                        }
-                        _ => {}
-                    }
-                }
-            }
-        }
-    }
-
-    // Track last assistant text we emitted to avoid duplicate assistant messages
-    // in the outbound Chat Completions payload (can happen if a final
-    // aggregated assistant message was recorded alongside an earlier partial).
-    let mut last_assistant_text: Option<String> = None;
-
-    for (idx, item) in input.iter().enumerate() {
-        match item {
-            ResponseItem::Message { role, content, .. } => {
-                // Build content either as a plain string (typical for assistant text)
-                // or as an array of content items when images are present (user/tool multimodal).
-                let mut text = String::new();
-                let mut items: Vec<serde_json::Value> = Vec::new();
-                let mut saw_image = false;
-
-                for c in content {
-                    match c {
-                        ContentItem::InputText { text: t }
-                        | ContentItem::OutputText { text: t } => {
-                            text.push_str(t);
-                            items.push(json!({"type":"text","text": t}));
-                        }
-                        ContentItem::InputImage { image_url } => {
-                            saw_image = true;
-                            items.push(json!({"type":"image_url","image_url": {"url": image_url}}));
-                        }
-                    }
-                }
-
-                // Skip exact-duplicate assistant messages.
-                if role == "assistant" {
-                    if let Some(prev) = &last_assistant_text
-                        && prev == &text
-                    {
-                        continue;
-                    }
-                    last_assistant_text = Some(text.clone());
-                }
-
-                // For assistant messages, always send a plain string for compatibility.
-                // For user messages, if an image is present, send an array of content items.
-                let content_value = if role == "assistant" {
-                    json!(text)
-                } else if saw_image {
-                    json!(items)
-                } else {
-                    json!(text)
-                };
-
-                let mut msg = json!({"role": role, "content": content_value});
-                if role == "assistant"
-                    && let Some(reasoning) = reasoning_by_anchor_index.get(&idx)
-                    && let Some(obj) = msg.as_object_mut()
-                {
-                    obj.insert("reasoning".to_string(), json!(reasoning));
-                }
-                messages.push(msg);
-            }
-            ResponseItem::FunctionCall {
-                name,
-                arguments,
-                call_id,
-                ..
-            } => {
-                let mut msg = json!({
-                    "role": "assistant",
-                    "content": null,
-                    "tool_calls": [{
-                        "id": call_id,
-                        "type": "function",
-                        "function": {
-                            "name": name,
-                            "arguments": arguments,
-                        }
-                    }]
-                });
-                if let Some(reasoning) = reasoning_by_anchor_index.get(&idx)
-                    && let Some(obj) = msg.as_object_mut()
-                {
-                    obj.insert("reasoning".to_string(), json!(reasoning));
-                }
-                messages.push(msg);
-            }
-            ResponseItem::LocalShellCall {
-                id,
-                call_id: _,
-                status,
-                action,
-            } => {
-                // Confirm with API team.
-                let mut msg = json!({
-                    "role": "assistant",
-                    "content": null,
-                    "tool_calls": [{
-                        "id": id.clone().unwrap_or_else(|| "".to_string()),
-                        "type": "local_shell_call",
-                        "status": status,
-                        "action": action,
-                    }]
-                });
-                if let Some(reasoning) = reasoning_by_anchor_index.get(&idx)
-                    && let Some(obj) = msg.as_object_mut()
-                {
-                    obj.insert("reasoning".to_string(), json!(reasoning));
-                }
-                messages.push(msg);
-            }
-            ResponseItem::FunctionCallOutput { call_id, output } => {
-                // Prefer structured content items when available (e.g., images)
-                // otherwise fall back to the legacy plain-string content.
-                let content_value = if let Some(items) = &output.content_items {
-                    let mapped: Vec<serde_json::Value> = items
-                        .iter()
-                        .map(|it| match it {
-                            FunctionCallOutputContentItem::InputText { text } => {
-                                json!({"type":"text","text": text})
-                            }
-                            FunctionCallOutputContentItem::InputImage { image_url } => {
-                                json!({"type":"image_url","image_url": {"url": image_url}})
-                            }
-                        })
-                        .collect();
-                    json!(mapped)
-                } else {
-                    json!(output.content)
-                };
-
-                messages.push(json!({
-                    "role": "tool",
-                    "tool_call_id": call_id,
-                    "content": content_value,
-                }));
-            }
-            ResponseItem::CustomToolCall {
-                id,
-                call_id: _,
-                name,
-                input,
-                status: _,
-            } => {
-                messages.push(json!({
-                    "role": "assistant",
-                    "content": null,
-                    "tool_calls": [{
-                        "id": id,
-                        "type": "custom",
-                        "custom": {
-                            "name": name,
-                            "input": input,
-                        }
-                    }]
-                }));
-            }
-            ResponseItem::CustomToolCallOutput { call_id, output } => {
-                messages.push(json!({
-                    "role": "tool",
-                    "tool_call_id": call_id,
-                    "content": output,
-                }));
-            }
-            ResponseItem::GhostSnapshot { .. } => {
-                // Ghost snapshots annotate history but are not sent to the model.
-                continue;
-            }
-            ResponseItem::Reasoning { .. }
-            | ResponseItem::WebSearchCall { .. }
-            | ResponseItem::Other => {
-                // Omit these items from the conversation history.
-                continue;
-            }
-        }
-    }
+    let (messages, _summary_followup_appended) = build_chat_messages(prompt, model_family);
+    let message_count = messages.len();
 
     let tools_json = create_tools_json_for_chat_completions_api(&prompt.tools)?;
-    let payload = json!({
+    let tools_count = tools_json.len();
+    let mut payload = json!({
         "model": model_family.slug,
         "messages": messages,
         "stream": true,
         "tools": tools_json,
     });
 
+    if provider.name.contains("LiteLLM") {
+        if let Some(obj) = payload.as_object_mut() {
+            obj.insert("stream_options".to_string(), json!({"include_usage": true}));
+        }
+    }
+
     debug!(
         "POST to {}: {}",
         provider.get_full_url(&None),
@@ -361,17 +108,60 @@ pub(crate) async fn stream_chat_completions(
             req_builder = req_builder.header("x-openai-subagent", subagent);
         }
 
-        let res = otel_event_manager
-            .log_request(attempt, || {
-                req_builder
-                    .header(reqwest::header::ACCEPT, "text/event-stream")
-                    .json(&payload)
-                    .send()
-            })
-            .await;
+        info!(
+            target: "codex_litellm_debug::model_request",
+            attempt,
+            max_retries,
+            model = %model_family.slug,
+            provider = %provider.name,
+            message_count,
+            tools_count,
+            "chat_completions.request_start"
+        );
+
+        let request_future = otel_event_manager.log_request(attempt, || {
+            req_builder
+                .header(reqwest::header::ACCEPT, "text/event-stream")
+                .json(&payload)
+                .send()
+        });
+
+        let res = match timeout(
+            Duration::from_secs(BUFFERED_REQUEST_TIMEOUT_SECS),
+            request_future,
+        )
+        .await
+        {
+            Ok(res) => res,
+            Err(_) => {
+                warn!(
+                    target: "codex_litellm_debug::model_response",
+                    attempt,
+                    model = %model_family.slug,
+                    provider = %provider.name,
+                    timeout_secs = BUFFERED_REQUEST_TIMEOUT_SECS,
+                    "chat_completions.buffered_request_timeout"
+                );
+                return Err(CodexErr::ConnectionFailed(ConnectionFailedError::new(
+                    format!(
+                        "LiteLLM request timed out after {} seconds",
+                        BUFFERED_REQUEST_TIMEOUT_SECS
+                    ),
+                )));
+            }
+        };
 
         match res {
             Ok(resp) if resp.status().is_success() => {
+                let status_code = resp.status();
+                info!(
+                    target: "codex_litellm_debug::model_response",
+                    attempt,
+                    model = %model_family.slug,
+                    provider = %provider.name,
+                    status = status_code.as_u16(),
+                    "chat_completions.response_stream_open"
+                );
                 let (tx_event, rx_event) = mpsc::channel::<Result<ResponseEvent>>(1600);
                 let stream = resp.bytes_stream().map_err(|e| {
                     CodexErr::ResponseStreamFailed(ResponseStreamFailed {
@@ -391,6 +181,16 @@ pub(crate) async fn stream_chat_completions(
                 let status = res.status();
                 if !(status == StatusCode::TOO_MANY_REQUESTS || status.is_server_error()) {
                     let body = (res.text().await).unwrap_or_default();
+                    let body_preview: String = body.chars().take(200).collect();
+                    info!(
+                        target: "codex_litellm_debug::model_response",
+                        attempt,
+                        model = %model_family.slug,
+                        provider = %provider.name,
+                        status = status.as_u16(),
+                        body_preview = body_preview,
+                        "chat_completions.response_unexpected_status"
+                    );
                     return Err(CodexErr::UnexpectedStatus(UnexpectedResponseError {
                         status,
                         body,
@@ -399,6 +199,14 @@ pub(crate) async fn stream_chat_completions(
                 }
 
                 if attempt > max_retries {
+                    info!(
+                        target: "codex_litellm_debug::model_response",
+                        attempt,
+                        model = %model_family.slug,
+                        provider = %provider.name,
+                        status = status.as_u16(),
+                        "chat_completions.retry_limit"
+                    );
                     return Err(CodexErr::RetryLimit(RetryLimitReachedError {
                         status,
                         request_id: None,
@@ -414,21 +222,298 @@ pub(crate) async fn stream_chat_completions(
                 let delay = retry_after_secs
                     .map(|s| Duration::from_millis(s * 1_000))
                     .unwrap_or_else(|| backoff(attempt));
+                info!(
+                    target: "codex_litellm_debug::model_response",
+                    attempt,
+                    model = %model_family.slug,
+                    provider = %provider.name,
+                    status = status.as_u16(),
+                    retry_after_secs,
+                    delay_ms = delay.as_millis(),
+                    "chat_completions.retry_scheduled"
+                );
                 tokio::time::sleep(delay).await;
             }
             Err(e) => {
                 if attempt > max_retries {
-                    return Err(CodexErr::ConnectionFailed(ConnectionFailedError {
-                        source: e,
-                    }));
+                    warn!(
+                        target: "codex_litellm_debug::model_response",
+                        attempt,
+                        model = %model_family.slug,
+                        provider = %provider.name,
+                        error = %e,
+                        "chat_completions.request_failed"
+                    );
+                    return Err(CodexErr::ConnectionFailed(ConnectionFailedError::from(e)));
                 }
                 let delay = backoff(attempt);
+                warn!(
+                    target: "codex_litellm_debug::model_response",
+                    attempt,
+                    model = %model_family.slug,
+                    provider = %provider.name,
+                    error = %e,
+                    delay_ms = delay.as_millis(),
+                    "chat_completions.request_retry"
+                );
                 tokio::time::sleep(delay).await;
             }
         }
     }
 }
 
+pub(crate) async fn fetch_chat_completions_buffered(
+    prompt: &Prompt,
+    model_family: &ModelFamily,
+    client: &CodexHttpClient,
+    provider: &ModelProviderInfo,
+    otel_event_manager: &OtelEventManager,
+    session_source: &SessionSource,
+    force_followups_after_tools: bool,
+) -> Result<ResponseStream> {
+    if prompt.output_schema.is_some() {
+        return Err(CodexErr::UnsupportedOperation(
+            "output_schema is not supported for Chat Completions API".to_string(),
+        ));
+    }
+
+    let (mut messages, appended_summary_prompt) = build_chat_messages(prompt, model_family);
+    let fallback_hints = if appended_summary_prompt {
+        collect_recent_tool_hints(&prompt.input)
+    } else {
+        Vec::new()
+    };
+
+    let force_followups_after_tools = match session_source {
+        SessionSource::Exec => false,
+        SessionSource::Cli => force_followups_after_tools,
+        _ => force_followups_after_tools,
+    };
+    let tools_json = create_tools_json_for_chat_completions_api(&prompt.tools)?;
+    // For non-agentic LiteLLM bridges we keep tools enabled even when the summary
+    // follow-up prompt is appended so they can continue editing without user
+    // intervention. Agentic/streaming-compatible models still disable tools to
+    // coax a clean wrap-up.
+    let disable_tools_for_summary_prompt = appended_summary_prompt && !force_followups_after_tools;
+    let effective_tools = if disable_tools_for_summary_prompt {
+        Vec::new()
+    } else {
+        tools_json
+    };
+    let max_followups = if force_followups_after_tools {
+        MAX_BUFFERED_FOLLOWUP_ATTEMPTS
+    } else {
+        0
+    };
+    let mut followups_remaining = max_followups;
+    let mut allow_missing_assistant = appended_summary_prompt;
+    let mut followup_mode = false;
+    let mut summary_prompt_appended = appended_summary_prompt;
+
+    let (tx_event, rx_event) = mpsc::channel::<Result<ResponseEvent>>(16);
+    let mut autopilot_only_turns: usize = 0;
+    loop {
+        let tools = if followup_mode && disable_tools_for_summary_prompt {
+            Vec::new()
+        } else {
+            effective_tools.clone()
+        };
+        let payload = json!({
+            "model": model_family.slug,
+            "messages": messages.clone(),
+            "stream": false,
+            "tools": tools.clone(),
+        });
+        let message_count = messages.len();
+        let tools_count = tools.len();
+        let payload_dump = payload.to_string();
+
+        info!(
+            target: "codex_litellm_debug::oss_buffered",
+            followup_mode,
+            allow_missing_assistant,
+            summary_prompt_appended,
+            followups_remaining,
+            message_count,
+            tools_count,
+            fallback_hint_count = fallback_hints.len(),
+            payload = payload_dump.as_str(),
+            "oss_buffered.request_payload"
+        );
+
+        let request_fut = execute_buffered_request(
+            &payload,
+            message_count,
+            tools_count,
+            client,
+            provider,
+            otel_event_manager,
+            session_source,
+            &model_family.slug,
+        );
+
+        let timeout_secs = Duration::from_secs(BUFFERED_REQUEST_TIMEOUT_SECS);
+        let request_start = Instant::now();
+        let parsed = match tokio::time::timeout(timeout_secs, request_fut).await {
+            Ok(result) => result?,
+            Err(_) => {
+                let elapsed_ms = request_start.elapsed().as_millis() as u64;
+                info!(
+                    target: "codex_litellm_debug::oss_buffered",
+                    timeout_secs = BUFFERED_REQUEST_TIMEOUT_SECS,
+                    elapsed_ms,
+                    "oss_buffered.request_timeout"
+                );
+                let summary = format!(
+                    "The upstream model did not respond within {} seconds. I can retry or take a different approach if you’d like.",
+                    BUFFERED_REQUEST_TIMEOUT_SECS
+                );
+                emit_buffered_reasoning_item(&tx_event, summary.clone()).await;
+                let final_text = format!(
+                    "{summary}\n\nLet me know if you'd like me to keep investigating or try something else."
+                );
+                let _ = tx_event
+                    .send(Ok(ResponseEvent::OutputItemDone(ResponseItem::Message {
+                        id: Some(FALLBACK_ASSISTANT_MESSAGE_ID.to_string()),
+                        role: "assistant".to_string(),
+                        content: vec![ContentItem::OutputText { text: final_text }],
+                    })))
+                    .await;
+                let _ = tx_event
+                    .send(Ok(ResponseEvent::Completed {
+                        response_id: String::new(),
+                        token_usage: None,
+                    }))
+                    .await;
+                drop(tx_event);
+                return Ok(ResponseStream { rx_event });
+            }
+        };
+        let elapsed_ms = request_start.elapsed().as_millis() as u64;
+        info!(
+            target: "codex_litellm_debug::oss_buffered",
+            elapsed_ms,
+            "oss_buffered.request_elapsed_ms"
+        );
+
+        let prompt_tokens = parsed
+            .usage
+            .as_ref()
+            .and_then(|u| u.prompt_tokens)
+            .unwrap_or(0);
+        let completion_tokens = parsed
+            .usage
+            .as_ref()
+            .and_then(|u| u.completion_tokens)
+            .unwrap_or(0);
+        let reasoning_tokens = parsed
+            .usage
+            .as_ref()
+            .and_then(|u| {
+                u.completion_tokens_details
+                    .as_ref()
+                    .and_then(|d| d.reasoning_tokens)
+            })
+            .unwrap_or(0);
+
+        info!(
+            target: "codex_litellm_debug::oss_buffered",
+            response_id = parsed.id.as_deref().unwrap_or(""),
+            choice_count = parsed.choices.len(),
+            prompt_tokens,
+            completion_tokens,
+            reasoning_tokens,
+            "oss_buffered.response_parsed"
+        );
+
+        let suppress_fallback = followups_remaining > 0 && !followup_mode;
+        let disposition = emit_buffered_chat_events(
+            parsed,
+            tx_event.clone(),
+            allow_missing_assistant,
+            fallback_hints.clone(),
+            suppress_fallback,
+        )
+        .await;
+
+        let blended_total = disposition
+            .token_usage
+            .as_ref()
+            .map(|usage| usage.blended_total())
+            .unwrap_or(0);
+        let response_id_str = disposition.response_id.as_str();
+        info!(
+            target: "codex_litellm_debug::oss_buffered",
+            assistant_message_emitted = disposition.assistant_message_emitted,
+            suppress_fallback,
+            followup_mode,
+            followups_remaining,
+            blended_total,
+            response_id = response_id_str,
+            "oss_buffered.disposition"
+        );
+
+        if disposition.fallback_reasoning.is_some() {
+            followups_remaining = 0;
+        }
+
+        if disposition.fallback_reasoning.is_some() {
+            autopilot_only_turns = autopilot_only_turns.saturating_add(1);
+            if autopilot_only_turns >= MAX_AUTOPILOT_ONLY_TURNS {
+                followups_remaining = 0;
+            }
+        } else if disposition.assistant_message_emitted {
+            autopilot_only_turns = 0;
+        }
+
+        if disposition.assistant_message_emitted || followups_remaining == 0 {
+            if !disposition.assistant_message_emitted {
+                let summary = disposition.fallback_reasoning.clone().unwrap_or_else(|| {
+                    "The upstream model paused before returning an answer.".to_string()
+                });
+                let final_text = format!(
+                    "{summary}\n\nLet me know if you'd like me to keep investigating or try a different approach."
+                );
+                info!(
+                    target: "codex_litellm_debug::oss_buffered",
+                    "oss_buffered.synthetic_completion"
+                );
+                let _ = tx_event
+                    .send(Ok(ResponseEvent::OutputItemDone(ResponseItem::Message {
+                        id: Some(FALLBACK_ASSISTANT_MESSAGE_ID.to_string()),
+                        role: "assistant".to_string(),
+                        content: vec![ContentItem::OutputText { text: final_text }],
+                    })))
+                    .await;
+                // downstream consumers should treat this as a real assistant answer.
+                // we don't toggle `assistant_message_emitted` here since we're exiting the loop,
+                // but the TurnComplete logic will now see a final agent message.
+            }
+            let _ = tx_event
+                .send(Ok(ResponseEvent::Completed {
+                    response_id: disposition.response_id,
+                    token_usage: disposition.token_usage,
+                }))
+                .await;
+            break;
+        }
+
+        followups_remaining = followups_remaining.saturating_sub(1);
+        followup_mode = true;
+        allow_missing_assistant = true;
+
+        if !summary_prompt_appended {
+            messages.push(json!({"role": "user", "content": SUMMARY_FOLLOWUP_PROMPT}));
+            summary_prompt_appended = true;
+        } else {
+            messages.push(json!({"role": "user", "content": SUMMARY_FOLLOWUP_PROMPT}));
+        }
+    }
+
+    drop(tx_event);
+    Ok(ResponseStream { rx_event })
+}
+
 async fn append_assistant_text(
     tx_event: &mpsc::Sender<Result<ResponseEvent>>,
     assistant_item: &mut Option<ResponseItem>,
@@ -484,6 +569,17 @@ async fn append_reasoning_text(
             .await;
     }
 }
+
+async fn emit_buffered_reasoning_item(
+    tx_event: &mpsc::Sender<Result<ResponseEvent>>,
+    text: String,
+) {
+    let mut reasoning_item: Option<ResponseItem> = None;
+    append_reasoning_text(tx_event, &mut reasoning_item, text).await;
+    if let Some(item) = reasoning_item {
+        let _ = tx_event.send(Ok(ResponseEvent::OutputItemDone(item))).await;
+    }
+}
 /// Lightweight SSE processor for the Chat Completions streaming format. The
 /// output is mapped onto Codex's internal [`ResponseEvent`] so that the rest
 /// of the pipeline can stay agnostic of the underlying wire format.
@@ -513,6 +609,63 @@ async fn process_chat_sse<S>(
     let mut fn_call_state = FunctionCallState::default();
     let mut assistant_item: Option<ResponseItem> = None;
     let mut reasoning_item: Option<ResponseItem> = None;
+    let mut streaming_tool_call = false;
+    let mut latest_usage: Option<TokenUsage> = None;
+
+    fn assistant_text_from_item(item: &ResponseItem) -> Option<String> {
+        if let ResponseItem::Message { role, content, .. } = item
+            && role == "assistant"
+        {
+            let mut text = String::new();
+            for piece in content {
+                if let ContentItem::OutputText { text: segment } = piece {
+                    text.push_str(segment);
+                }
+            }
+            if text.trim().is_empty() {
+                None
+            } else {
+                Some(text)
+            }
+        } else {
+            None
+        }
+    }
+
+    fn reasoning_text_from_item(item: &ResponseItem) -> Option<String> {
+        if let ResponseItem::Reasoning {
+            content: Some(items),
+            ..
+        } = item
+        {
+            let mut text = String::new();
+            for entry in items {
+                match entry {
+                    ReasoningItemContent::ReasoningText { text: segment }
+                    | ReasoningItemContent::Text { text: segment } => text.push_str(segment),
+                }
+            }
+            if text.trim().is_empty() {
+                None
+            } else {
+                Some(text)
+            }
+        } else {
+            None
+        }
+    }
+
+    fn truncate_preview(text: &str) -> String {
+        const MAX_LEN: usize = 200;
+        let flattened = text.replace('\n', " ");
+        if flattened.len() <= MAX_LEN {
+            flattened
+        } else {
+            let mut truncated = flattened[..MAX_LEN].to_string();
+            truncated.push_str("…");
+            truncated
+        }
+    }
 
     loop {
         let start = std::time::Instant::now();
@@ -529,11 +682,16 @@ async fn process_chat_sse<S>(
                 return;
             }
             Ok(None) => {
-                // Stream closed gracefully – emit Completed with dummy id.
+                if let Some(item) = assistant_item.take() {
+                    let _ = tx_event.send(Ok(ResponseEvent::OutputItemDone(item))).await;
+                }
+                if let Some(item) = reasoning_item.take() {
+                    let _ = tx_event.send(Ok(ResponseEvent::OutputItemDone(item))).await;
+                }
                 let _ = tx_event
                     .send(Ok(ResponseEvent::Completed {
                         response_id: String::new(),
-                        token_usage: None,
+                        token_usage: latest_usage.clone(),
                     }))
                     .await;
                 return;
@@ -551,6 +709,32 @@ async fn process_chat_sse<S>(
 
         // OpenAI Chat streaming sends a literal string "[DONE]" when finished.
         if sse.data.trim() == "[DONE]" {
+            if assistant_item.is_none() {
+                if let Some(reasoning_text) = reasoning_item
+                    .as_ref()
+                    .and_then(reasoning_text_from_item)
+                    .filter(|text| !text.is_empty())
+                {
+                    let preview = truncate_preview(&reasoning_text);
+                    tracing::info!(
+                        target: "codex_litellm_debug::oss_buffered",
+                        preview,
+                        "streaming.reasoning_promoted_to_assistant"
+                    );
+                    let mut item = ResponseItem::Message {
+                        id: None,
+                        role: "assistant".to_string(),
+                        content: vec![],
+                    };
+                    if let ResponseItem::Message { content, .. } = &mut item {
+                        content.push(ContentItem::OutputText {
+                            text: reasoning_text.clone(),
+                        });
+                    }
+                    assistant_item = Some(item);
+                }
+            }
+
             // Emit any finalized items before closing so downstream consumers receive
             // terminal events for both assistant content and raw reasoning.
             if let Some(item) = assistant_item {
@@ -564,7 +748,7 @@ async fn process_chat_sse<S>(
             let _ = tx_event
                 .send(Ok(ResponseEvent::Completed {
                     response_id: String::new(),
-                    token_usage: None,
+                    token_usage: latest_usage.clone(),
                 }))
                 .await;
             return;
@@ -577,18 +761,18 @@ async fn process_chat_sse<S>(
         };
         trace!("chat_completions received SSE chunk: {chunk:?}");
 
+        if let Some(usage_val) = chunk.get("usage") {
+            if let Ok(parsed) =
+                serde_json::from_value::<BufferedChatCompletionUsage>(usage_val.clone())
+            {
+                latest_usage = Some(TokenUsage::from(parsed));
+            }
+        }
+
         let choice_opt = chunk.get("choices").and_then(|c| c.get(0));
 
         if let Some(choice) = choice_opt {
-            // Handle assistant content tokens as streaming deltas.
-            if let Some(content) = choice
-                .get("delta")
-                .and_then(|d| d.get("content"))
-                .and_then(|c| c.as_str())
-                && !content.is_empty()
-            {
-                append_assistant_text(&tx_event, &mut assistant_item, content.to_string()).await;
-            }
+            let finish_reason = choice.get("finish_reason").and_then(|v| v.as_str());
 
             // Forward any reasoning/thinking deltas if present.
             // Some providers stream `reasoning` as a plain string while others
@@ -614,109 +798,1133 @@ async fn process_chat_sse<S>(
                         maybe_text = Some(s.to_string());
                     }
                 }
-
-                if let Some(reasoning) = maybe_text {
-                    // Accumulate so we can emit a terminal Reasoning item at the end.
-                    append_reasoning_text(&tx_event, &mut reasoning_item, reasoning).await;
+
+                if let Some(reasoning) = maybe_text {
+                    // Accumulate so we can emit a terminal Reasoning item at the end.
+                    append_reasoning_text(&tx_event, &mut reasoning_item, reasoning).await;
+                }
+            }
+
+            // Some providers only include reasoning on the final message object.
+            if let Some(message_reasoning) = choice.get("message").and_then(|m| m.get("reasoning"))
+            {
+                // Accept either a plain string or an object with { text | content }
+                if let Some(s) = message_reasoning.as_str() {
+                    if !s.is_empty() {
+                        append_reasoning_text(&tx_event, &mut reasoning_item, s.to_string()).await;
+                    }
+                } else if let Some(obj) = message_reasoning.as_object()
+                    && let Some(s) = obj
+                        .get("text")
+                        .and_then(|v| v.as_str())
+                        .or_else(|| obj.get("content").and_then(|v| v.as_str()))
+                    && !s.is_empty()
+                {
+                    append_reasoning_text(&tx_event, &mut reasoning_item, s.to_string()).await;
+                }
+            }
+
+            let mut pending_content_delta: Option<String> = choice
+                .get("delta")
+                .and_then(|d| d.get("content"))
+                .and_then(|c| c.as_str())
+                .filter(|s| !s.is_empty())
+                .map(str::to_string);
+
+            // Handle streaming function / tool calls.
+            if let Some(tool_calls) = choice
+                .get("delta")
+                .and_then(|d| d.get("tool_calls"))
+                .and_then(|tc| tc.as_array())
+                && let Some(tool_call) = tool_calls.first()
+            {
+                // Mark that we have an active function call in progress.
+                fn_call_state.active = true;
+                streaming_tool_call = true;
+
+                // Extract call_id if present.
+                if let Some(id) = tool_call.get("id").and_then(|v| v.as_str()) {
+                    fn_call_state.call_id.get_or_insert_with(|| id.to_string());
+                }
+
+                // Extract function details if present.
+                if let Some(function) = tool_call.get("function") {
+                    if let Some(name) = function.get("name").and_then(|n| n.as_str()) {
+                        fn_call_state.name.get_or_insert_with(|| name.to_string());
+                    }
+
+                    if let Some(args_fragment) = function.get("arguments").and_then(|a| a.as_str())
+                    {
+                        fn_call_state.arguments.push_str(args_fragment);
+                    }
+                }
+            }
+
+            if let Some(content) = pending_content_delta.take() {
+                if streaming_tool_call && fn_call_state.active {
+                    append_reasoning_text(&tx_event, &mut reasoning_item, content).await;
+                } else {
+                    append_assistant_text(&tx_event, &mut assistant_item, content).await;
+                }
+            }
+
+            // Emit end-of-turn when finish_reason signals completion.
+            if let Some(finish_reason) = finish_reason {
+                match finish_reason {
+                    "tool_calls" if fn_call_state.active => {
+                        let reasoning_text =
+                            reasoning_item.as_ref().and_then(reasoning_text_from_item);
+                        let reasoning_preview =
+                            reasoning_text.as_ref().map(|s| truncate_preview(s));
+                        info!(
+                            target: "codex_litellm_debug::model_response",
+                            finish_reason = finish_reason,
+                            tool_name = fn_call_state.name.as_deref(),
+                            call_id = fn_call_state.call_id.as_deref(),
+                            tool_arguments_len = fn_call_state.arguments.len(),
+                            reasoning_chars = reasoning_text.as_ref().map(|s| s.len()).unwrap_or(0),
+                            reasoning_preview = reasoning_preview.as_deref().unwrap_or(""),
+                            "chat_completions.finish_tool_call"
+                        );
+                        // First, flush the terminal raw reasoning so UIs can finalize
+                        // the reasoning stream before any exec/tool events begin.
+                        if let Some(item) = reasoning_item.take() {
+                            let _ = tx_event.send(Ok(ResponseEvent::OutputItemDone(item))).await;
+                        }
+
+                        // Then emit the FunctionCall response item.
+                        let canonical_name = fn_call_state
+                            .name
+                            .as_deref()
+                            .map(canonicalize_tool_name)
+                            .unwrap_or_else(|| "".to_string());
+                        let item = ResponseItem::FunctionCall {
+                            id: None,
+                            name: canonical_name,
+                            arguments: fn_call_state.arguments.clone(),
+                            call_id: fn_call_state.call_id.clone().unwrap_or_else(String::new),
+                        };
+
+                        let _ = tx_event.send(Ok(ResponseEvent::OutputItemDone(item))).await;
+                    }
+                    "stop" => {
+                        let assistant_text =
+                            assistant_item.as_ref().and_then(assistant_text_from_item);
+                        let assistant_chars = assistant_text.as_ref().map(|s| s.len()).unwrap_or(0);
+                        let assistant_preview =
+                            assistant_text.as_ref().map(|s| truncate_preview(s));
+                        let reasoning_text =
+                            reasoning_item.as_ref().and_then(reasoning_text_from_item);
+                        let reasoning_chars = reasoning_text.as_ref().map(|s| s.len()).unwrap_or(0);
+                        let reasoning_preview =
+                            reasoning_text.as_ref().map(|s| truncate_preview(s));
+                        info!(
+                            target: "codex_litellm_debug::model_response",
+                            finish_reason = finish_reason,
+                            assistant_chars,
+                            assistant_preview = assistant_preview.as_deref().unwrap_or(""),
+                            reasoning_chars,
+                            reasoning_preview = reasoning_preview.as_deref().unwrap_or(""),
+                            "chat_completions.finish_stop"
+                        );
+                        // Regular turn without tool-call. Emit the final assistant message
+                        // as a single OutputItemDone so non-delta consumers see the result.
+                        if let Some(item) = assistant_item.take() {
+                            let _ = tx_event.send(Ok(ResponseEvent::OutputItemDone(item))).await;
+                        }
+                        // Also emit a terminal Reasoning item so UIs can finalize raw reasoning.
+                        if let Some(item) = reasoning_item.take() {
+                            let _ = tx_event.send(Ok(ResponseEvent::OutputItemDone(item))).await;
+                        }
+                    }
+                    _ => {}
+                }
+
+                // Emit Completed regardless of reason so the agent can advance.
+                let _ = tx_event
+                    .send(Ok(ResponseEvent::Completed {
+                        response_id: String::new(),
+                        token_usage: latest_usage.clone(),
+                    }))
+                    .await;
+
+                // Prepare for potential next turn (should not happen in same stream).
+                // fn_call_state = FunctionCallState::default();
+
+                return; // End processing for this SSE stream.
+            }
+        }
+    }
+}
+
+fn build_chat_messages(prompt: &Prompt, model_family: &ModelFamily) -> (Vec<Value>, bool) {
+    // Build messages array
+    let mut messages = Vec::<serde_json::Value>::new();
+    let mut appended_summary_prompt = false;
+
+    let full_instructions = prompt.get_full_instructions(model_family);
+    messages.push(json!({"role": "system", "content": full_instructions}));
+
+    let input = prompt.get_formatted_input();
+
+    // Pre-scan: map Reasoning blocks to the adjacent assistant anchor after the last user.
+    // - If the last emitted message is a user message, drop all reasoning.
+    // - Otherwise, for each Reasoning item after the last user message, attach it
+    //   to the immediate previous assistant message (stop turns) or the immediate
+    //   next assistant anchor (tool-call turns: function/local shell call, or assistant message).
+    let mut reasoning_by_anchor_index: HashMap<usize, String> = HashMap::new();
+
+    // Determine the last role that would be emitted to Chat Completions.
+    let mut last_emitted_role: Option<&str> = None;
+    for item in &input {
+        match item {
+            ResponseItem::Message { role, .. } => last_emitted_role = Some(role.as_str()),
+            ResponseItem::FunctionCall { .. } | ResponseItem::LocalShellCall { .. } => {
+                last_emitted_role = Some("assistant")
+            }
+            ResponseItem::FunctionCallOutput { .. } => last_emitted_role = Some("tool"),
+            ResponseItem::Reasoning { .. } | ResponseItem::Other => {}
+            ResponseItem::CustomToolCall { .. } => {}
+            ResponseItem::CustomToolCallOutput { .. } => {}
+            ResponseItem::WebSearchCall { .. } => {}
+            ResponseItem::GhostSnapshot { .. } => {}
+        }
+    }
+
+    // Find the last user message index in the input.
+    let mut last_user_index: Option<usize> = None;
+    for (idx, item) in input.iter().enumerate() {
+        if let ResponseItem::Message { role, .. } = item
+            && role == "user"
+        {
+            last_user_index = Some(idx);
+        }
+    }
+
+    // Attach reasoning only if the conversation does not end with a user message.
+    if !matches!(last_emitted_role, Some("user")) {
+        for (idx, item) in input.iter().enumerate() {
+            // Only consider reasoning that appears after the last user message.
+            if let Some(u_idx) = last_user_index
+                && idx <= u_idx
+            {
+                continue;
+            }
+
+            if let ResponseItem::Reasoning {
+                content: Some(items),
+                ..
+            } = item
+            {
+                let mut text = String::new();
+                for entry in items {
+                    match entry {
+                        ReasoningItemContent::ReasoningText { text: segment }
+                        | ReasoningItemContent::Text { text: segment } => text.push_str(segment),
+                    }
+                }
+                if text.trim().is_empty() {
+                    continue;
+                }
+
+                // Prefer immediate previous assistant message (stop turns)
+                let mut attached = false;
+                if idx > 0
+                    && let ResponseItem::Message { role, .. } = &input[idx - 1]
+                    && role == "assistant"
+                {
+                    reasoning_by_anchor_index
+                        .entry(idx - 1)
+                        .and_modify(|v| v.push_str(&text))
+                        .or_insert(text.clone());
+                    attached = true;
+                }
+
+                // Otherwise, attach to immediate next assistant anchor (tool-calls or assistant message)
+                if !attached && idx + 1 < input.len() {
+                    match &input[idx + 1] {
+                        ResponseItem::FunctionCall { .. } | ResponseItem::LocalShellCall { .. } => {
+                            reasoning_by_anchor_index
+                                .entry(idx + 1)
+                                .and_modify(|v| v.push_str(&text))
+                                .or_insert(text.clone());
+                        }
+                        ResponseItem::Message { role, .. } if role == "assistant" => {
+                            reasoning_by_anchor_index
+                                .entry(idx + 1)
+                                .and_modify(|v| v.push_str(&text))
+                                .or_insert(text.clone());
+                        }
+                        _ => {}
+                    }
+                }
+            }
+        }
+    }
+
+    // Track last assistant text we emitted to avoid duplicate assistant messages
+    // in the outbound Chat Completions payload (can happen if a final
+    // aggregated assistant message was recorded alongside an earlier partial).
+    let mut last_assistant_text: Option<String> = None;
+
+    for (idx, item) in input.iter().enumerate() {
+        match item {
+            ResponseItem::Message { role, content, .. } => {
+                // Build content either as a plain string (typical for assistant text)
+                // or as an array of content items when images are present (user/tool multimodal).
+                let mut text = String::new();
+                let mut items: Vec<serde_json::Value> = Vec::new();
+                let mut saw_image = false;
+
+                for c in content {
+                    match c {
+                        ContentItem::InputText { text: t }
+                        | ContentItem::OutputText { text: t } => {
+                            text.push_str(t);
+                            items.push(json!({"type":"text","text": t}));
+                        }
+                        ContentItem::InputImage { image_url } => {
+                            saw_image = true;
+                            items.push(json!({"type":"image_url","image_url": {"url": image_url}}));
+                        }
+                    }
+                }
+
+                // Skip exact-duplicate assistant messages.
+                if role == "assistant" {
+                    if let Some(prev) = &last_assistant_text
+                        && prev == &text
+                    {
+                        continue;
+                    }
+                    last_assistant_text = Some(text.clone());
+                }
+
+                // For assistant messages, always send a plain string for compatibility.
+                // For user messages, if an image is present, send an array of content items.
+                let content_value = if role == "assistant" {
+                    json!(text)
+                } else if saw_image {
+                    json!(items)
+                } else {
+                    json!(text)
+                };
+
+                let mut msg = json!({"role": role, "content": content_value});
+                if role == "assistant"
+                    && let Some(reasoning) = reasoning_by_anchor_index.get(&idx)
+                    && let Some(obj) = msg.as_object_mut()
+                {
+                    obj.insert("reasoning".to_string(), json!(reasoning));
+                }
+                messages.push(msg);
+            }
+            ResponseItem::FunctionCall {
+                name,
+                arguments,
+                call_id,
+                ..
+            } => {
+                let mut msg = json!({
+                        "role": "assistant",
+                        "content": null,
+                        "tool_calls": [{
+                                "id": call_id,
+                                "type": "function",
+                                "function": {
+                                        "name": name,
+                                        "arguments": arguments,
+                                }
+                        }]
+                });
+                if let Some(reasoning) = reasoning_by_anchor_index.get(&idx)
+                    && let Some(obj) = msg.as_object_mut()
+                {
+                    obj.insert("reasoning".to_string(), json!(reasoning));
+                }
+                messages.push(msg);
+            }
+            ResponseItem::LocalShellCall {
+                id,
+                call_id: _,
+                status,
+                action,
+            } => {
+                // Confirm with API team.
+                let mut msg = json!({
+                        "role": "assistant",
+                        "content": null,
+                        "tool_calls": [{
+                                "id": id.clone().unwrap_or_else(|| "".to_string()),
+                                "type": "local_shell_call",
+                                "status": status,
+                                "action": action,
+                        }]
+                });
+                if let Some(reasoning) = reasoning_by_anchor_index.get(&idx)
+                    && let Some(obj) = msg.as_object_mut()
+                {
+                    obj.insert("reasoning".to_string(), json!(reasoning));
+                }
+                messages.push(msg);
+            }
+            ResponseItem::FunctionCallOutput { call_id, output } => {
+                // Prefer structured content items when available (e.g., images)
+                // otherwise fall back to the legacy plain-string content.
+                let content_value = if let Some(items) = &output.content_items {
+                    let mapped: Vec<serde_json::Value> = items
+                        .iter()
+                        .map(|it| match it {
+                            FunctionCallOutputContentItem::InputText { text } => {
+                                json!({"type":"text","text": text})
+                            }
+                            FunctionCallOutputContentItem::InputImage { image_url } => {
+                                json!({"type":"image_url","image_url": {"url": image_url}})
+                            }
+                        })
+                        .collect();
+                    json!(mapped)
+                } else {
+                    json!(output.content)
+                };
+
+                messages.push(json!({
+                        "role": "tool",
+                        "tool_call_id": call_id,
+                        "content": content_value,
+                }));
+            }
+            ResponseItem::CustomToolCall {
+                id,
+                call_id: _,
+                name,
+                input,
+                status: _,
+            } => {
+                messages.push(json!({
+                        "role": "assistant",
+                        "content": null,
+                        "tool_calls": [{
+                                "id": id,
+                                "type": "custom",
+                                "custom": {
+                                        "name": name,
+                                        "input": input,
+                                }
+                        }]
+                }));
+            }
+            ResponseItem::CustomToolCallOutput { call_id, output } => {
+                messages.push(json!({
+                        "role": "tool",
+                        "tool_call_id": call_id,
+                        "content": output,
+                }));
+            }
+            ResponseItem::GhostSnapshot { .. } => {
+                // Ghost snapshots annotate history but are not sent to the model.
+                continue;
+            }
+            ResponseItem::Reasoning { .. }
+            | ResponseItem::WebSearchCall { .. }
+            | ResponseItem::Other => {
+                // Omit these items from the conversation history.
+                continue;
+            }
+        }
+    }
+
+    if model_family.prefer_non_stream && needs_summary_followup(&input) {
+        messages.push(json!({"role": "user", "content": SUMMARY_FOLLOWUP_PROMPT}));
+        appended_summary_prompt = true;
+    }
+
+    (messages, appended_summary_prompt)
+}
+
+fn needs_summary_followup(input: &[ResponseItem]) -> bool {
+    let mut saw_tool_artifact = false;
+    for item in input.iter().rev() {
+        match item {
+            ResponseItem::FunctionCall { .. }
+            | ResponseItem::LocalShellCall { .. }
+            | ResponseItem::CustomToolCall { .. }
+            | ResponseItem::WebSearchCall { .. } => {
+                return true;
+            }
+            ResponseItem::FunctionCallOutput { .. } | ResponseItem::CustomToolCallOutput { .. } => {
+                saw_tool_artifact = true;
+            }
+            ResponseItem::Message { role, .. } => {
+                if role == "assistant" {
+                    return false;
+                }
+                if role == "user" && saw_tool_artifact {
+                    return true;
+                }
+            }
+            ResponseItem::Reasoning { .. }
+            | ResponseItem::GhostSnapshot { .. }
+            | ResponseItem::Other => {}
+        }
+    }
+    false
+}
+
+pub(crate) const FALLBACK_ASSISTANT_MESSAGE_ID: &str = "codex_litellm_fallback_assistant";
+
+async fn emit_buffered_chat_events(
+    response: BufferedChatCompletionResponse,
+    tx_event: mpsc::Sender<Result<ResponseEvent>>,
+    allow_missing_assistant: bool,
+    fallback_hints: Vec<ToolHint>,
+    mut suppress_fallback: bool,
+) -> BufferedResponseDisposition {
+    let BufferedChatCompletionResponse { id, choices, usage } = response;
+    let choice_count = choices.len();
+    info!(
+        target: "codex_litellm_debug::oss_buffered",
+        choice_count,
+        allow_missing_assistant,
+        suppress_fallback,
+        fallback_hint_count = fallback_hints.len(),
+        "oss_buffered.emit_start"
+    );
+
+    let mut assistant_message_emitted = false;
+    let mut fallback_reasoning: Option<String> = None;
+    let mut tool_summaries: Vec<String> = Vec::new();
+    let mut saw_tool_call = false;
+
+    if choice_count == 0 {
+        info!(
+            target: "codex_litellm_debug::oss_buffered",
+            "oss_buffered.no_choices_in_response"
+        );
+    }
+
+    let mut choices_iter = choices.into_iter();
+    if let Some(choice) = choices_iter.next() {
+        let mut message = choice.message;
+        let finish_reason = choice
+            .finish_reason
+            .as_deref()
+            .unwrap_or("unknown")
+            .to_string();
+        let role = message
+            .role
+            .clone()
+            .unwrap_or_else(|| "unknown".to_string());
+        let tool_call_count = message.tool_calls.as_ref().map(|v| v.len()).unwrap_or(0);
+        info!(
+            target: "codex_litellm_debug::oss_buffered",
+            finish_reason = finish_reason.as_str(),
+            role = role.as_str(),
+            tool_call_count,
+            "oss_buffered.choice_meta"
+        );
+
+        let mut reasoning_fallback: Option<String> = None;
+
+        let mut tool_calls = message.tool_calls.take().unwrap_or_default();
+        if tool_calls.is_empty() {
+            if let Some(reasoning_raw) = message
+                .reasoning_content
+                .as_ref()
+                .and_then(|s| synthesize_shell_tool_call(s))
+            {
+                info!(
+                    target: "codex_litellm_debug::oss_buffered",
+                    "oss_buffered.synthetic_tool_call_from_reasoning"
+                );
+                tool_calls.push(reasoning_raw);
+            }
+        }
+
+        if let Some(reasoning_raw) = message.reasoning_content.take().and_then(|s| {
+            let trimmed = s.trim();
+            if trimmed.is_empty() {
+                None
+            } else {
+                Some(trimmed.to_string())
+            }
+        }) {
+            emit_buffered_reasoning_item(&tx_event, reasoning_raw.clone()).await;
+            reasoning_fallback = Some(reasoning_raw);
+        }
+
+        for call in tool_calls {
+            saw_tool_call = true;
+            let tool_summary = summarize_tool_call(&call);
+            let call_id = call
+                .id
+                .clone()
+                .unwrap_or_else(|| Uuid::new_v4().to_string());
+            let arguments_preview = truncate_hint(&call.function.arguments).unwrap_or_default();
+            info!(
+                target: "codex_litellm_debug::oss_buffered",
+                call_id = call_id.as_str(),
+                tool_kind = call.kind.as_deref().unwrap_or("function"),
+                tool_name = %call.function.name,
+                arguments_preview = arguments_preview.as_str(),
+                "oss_buffered.tool_call"
+            );
+            let item = ResponseItem::FunctionCall {
+                id: None,
+                name: canonicalize_tool_name(&call.function.name),
+                arguments: call.function.arguments,
+                call_id: call_id.clone(),
+            };
+            let _ = tx_event.send(Ok(ResponseEvent::OutputItemDone(item))).await;
+            if let Some(summary) = tool_summary {
+                tool_summaries.push(summary);
+            }
+        }
+
+        let mut assistant_text = extract_text_from_content(message.content);
+
+        if assistant_text.is_none() {
+            if let Some(reasoning_raw) = reasoning_fallback.clone() {
+                if !is_noise_assistant_message(&reasoning_raw) {
+                    info!(
+                        target: "codex_litellm_debug::oss_buffered",
+                        preview = truncate_hint(&reasoning_raw).unwrap_or_default(),
+                        "oss_buffered.reasoning_promoted_to_assistant"
+                    );
+                    assistant_text = Some(reasoning_raw);
+                } else {
+                    info!(
+                        target: "codex_litellm_debug::oss_buffered",
+                        "oss_buffered.reasoning_kept_as_noise_only"
+                    );
+                }
+            }
+        }
+
+        if let Some(text) = assistant_text {
+            let is_noise = is_noise_assistant_message(&text);
+            let preview = truncate_hint(&text).unwrap_or_else(|| "".to_string());
+            info!(
+                target: "codex_litellm_debug::oss_buffered",
+                text_len = text.len(),
+                preview = preview.as_str(),
+                suppressed = is_noise,
+                "oss_buffered.assistant_text"
+            );
+            if is_noise {
+                if let Some(formatted) = format_noise_reasoning(&text) {
+                    emit_buffered_reasoning_item(&tx_event, formatted).await;
+                }
+            } else {
+                let item = ResponseItem::Message {
+                    id: None,
+                    role: "assistant".to_string(),
+                    content: vec![ContentItem::OutputText { text }],
+                };
+                let _ = tx_event.send(Ok(ResponseEvent::OutputItemDone(item))).await;
+                assistant_message_emitted = true;
+            }
+        } else {
+            info!(
+                target: "codex_litellm_debug::oss_buffered",
+                "oss_buffered.no_assistant_text_in_choice"
+            );
+        }
+    }
+
+    if !assistant_message_emitted && suppress_fallback && saw_tool_call {
+        info!(
+            target: "codex_litellm_debug::oss_buffered",
+            "oss_buffered.unsuppress_fallback_after_tool_only_choice"
+        );
+        suppress_fallback = false;
+    }
+
+    if !assistant_message_emitted && suppress_fallback {
+        info!(
+            target: "codex_litellm_debug::oss_buffered",
+            "oss_buffered.force_fallback_without_assistant"
+        );
+        suppress_fallback = false;
+    }
+
+    if !assistant_message_emitted && !suppress_fallback {
+        let use_history_hints = allow_missing_assistant && !fallback_hints.is_empty();
+        let fallback = if use_history_hints {
+            build_fallback_message(&fallback_hints)
+        } else {
+            let tool_hints: Vec<ToolHint> = tool_summaries
+                .iter()
+                .map(|action| ToolHint {
+                    action: action.clone(),
+                    outcome: None,
+                })
+                .collect();
+            build_fallback_message(&tool_hints)
+        };
+        info!(
+            target: "codex_litellm_debug::oss_buffered",
+            used_history_hints = use_history_hints,
+            fallback_len = fallback.len(),
+            "oss_buffered.emit_fallback"
+        );
+        if let Some(formatted) = format_noise_reasoning(&fallback) {
+            emit_buffered_reasoning_item(&tx_event, formatted).await;
+        }
+        fallback_reasoning = Some(fallback);
+    }
+
+    BufferedResponseDisposition {
+        assistant_message_emitted,
+        token_usage: usage.map(TokenUsage::from),
+        response_id: id.unwrap_or_default(),
+        fallback_reasoning,
+    }
+}
+
+fn extract_text_from_content(content: Option<Value>) -> Option<String> {
+    match content {
+        Some(Value::String(s)) => {
+            if s.trim().is_empty() {
+                None
+            } else {
+                Some(s)
+            }
+        }
+        Some(Value::Array(items)) => {
+            let mut text = String::new();
+            for item in items {
+                if let Some(t) = item.get("text").and_then(Value::as_str) {
+                    text.push_str(t);
+                }
+            }
+            if text.trim().is_empty() {
+                None
+            } else {
+                Some(text)
+            }
+        }
+        _ => None,
+    }
+}
+
+fn summarize_tool_call(call: &BufferedChatCompletionToolCall) -> Option<String> {
+    match call.function.name.as_str() {
+        "shell" => summarize_shell_call(&call.function.arguments),
+        other => Some(format!("invoked {other}")),
+    }
+}
+
+fn canonicalize_tool_name(raw: &str) -> String {
+    let normalized = raw.rsplit_once('.').map(|(_, tail)| tail).unwrap_or(raw);
+    match normalized {
+        "exec" | "bash" => "shell".to_string(),
+        other => other.to_string(),
+    }
+}
+
+fn summarize_shell_call(arguments: &str) -> Option<String> {
+    let value: Value = serde_json::from_str(arguments).ok()?;
+    let command = value.get("command")?.as_array().map(|cmd| {
+        cmd.iter()
+            .filter_map(Value::as_str)
+            .collect::<Vec<_>>()
+            .join(" ")
+    })?;
+    Some(format!("ran shell command `{command}`"))
+}
+
+fn build_fallback_message(hints: &[ToolHint]) -> String {
+    if hints.is_empty() {
+        return "The model stopped before returning an answer. Let me keep investigating or try a different approach if you’d like."
+            .to_string();
+    }
+
+    let mut message = String::from("Here’s what I just ran while checking your request:\n");
+    for hint in hints {
+        message.push_str("- ");
+        message.push_str(&hint.action);
+        message.push('\n');
+        if let Some(outcome) = hint.outcome.as_deref() {
+            message.push_str("    ↳ ");
+            message.push_str(outcome);
+            message.push('\n');
+        }
+    }
+    message.push_str(
+        "\nThe upstream model stopped after those steps, so I can keep digging or apply the findings above if you need more.",
+    );
+    message
+}
+
+const MAX_TOOL_HINTS: usize = 4;
+
+#[derive(Debug, Clone)]
+struct ToolHint {
+    action: String,
+    outcome: Option<String>,
+}
+
+#[derive(Debug, Default)]
+struct BufferedResponseDisposition {
+    assistant_message_emitted: bool,
+    token_usage: Option<TokenUsage>,
+    response_id: String,
+    fallback_reasoning: Option<String>,
+}
+
+fn collect_recent_tool_hints(history: &[ResponseItem]) -> Vec<ToolHint> {
+    let mut hints: Vec<ToolHint> = Vec::new();
+    for item in history.iter().rev() {
+        match item {
+            ResponseItem::Message { role, .. } if role == "assistant" => break,
+            ResponseItem::FunctionCall {
+                name, arguments, ..
+            } => {
+                if let Some(summary) = summarize_function_call_history(name, arguments) {
+                    hints.push(ToolHint {
+                        action: summary,
+                        outcome: None,
+                    });
+                }
+            }
+            ResponseItem::LocalShellCall { action, .. } => {
+                if let Some(summary) = summarize_local_shell_action(action) {
+                    hints.push(ToolHint {
+                        action: summary,
+                        outcome: None,
+                    });
+                }
+            }
+            ResponseItem::FunctionCallOutput { output, .. } => {
+                if let Some(summary) = summarize_tool_output_payload(output) {
+                    if let Some(last) = hints.last_mut() {
+                        if last.outcome.is_none() {
+                            last.outcome = Some(summary);
+                        } else {
+                            hints.push(ToolHint {
+                                action: "tool output".to_string(),
+                                outcome: Some(summary),
+                            });
+                        }
+                    } else {
+                        hints.push(ToolHint {
+                            action: "tool output".to_string(),
+                            outcome: Some(summary),
+                        });
+                    }
+                }
+            }
+            ResponseItem::CustomToolCall { name, input, .. } => {
+                if let Some(snippet) = truncate_hint(input) {
+                    hints.push(ToolHint {
+                        action: format!("invoked custom tool `{name}` ({snippet})"),
+                        outcome: None,
+                    });
+                } else {
+                    hints.push(ToolHint {
+                        action: format!("invoked custom tool `{name}`"),
+                        outcome: None,
+                    });
                 }
             }
-
-            // Some providers only include reasoning on the final message object.
-            if let Some(message_reasoning) = choice.get("message").and_then(|m| m.get("reasoning"))
-            {
-                // Accept either a plain string or an object with { text | content }
-                if let Some(s) = message_reasoning.as_str() {
-                    if !s.is_empty() {
-                        append_reasoning_text(&tx_event, &mut reasoning_item, s.to_string()).await;
+            ResponseItem::CustomToolCallOutput { output, .. } => {
+                if let Some(summary) = truncate_hint(output) {
+                    if let Some(last) = hints.last_mut() {
+                        if last.outcome.is_none() {
+                            last.outcome = Some(summary);
+                        } else {
+                            hints.push(ToolHint {
+                                action: "custom tool output".to_string(),
+                                outcome: Some(summary),
+                            });
+                        }
+                    } else {
+                        hints.push(ToolHint {
+                            action: "custom tool output".to_string(),
+                            outcome: Some(summary),
+                        });
                     }
-                } else if let Some(obj) = message_reasoning.as_object()
-                    && let Some(s) = obj
-                        .get("text")
-                        .and_then(|v| v.as_str())
-                        .or_else(|| obj.get("content").and_then(|v| v.as_str()))
-                    && !s.is_empty()
-                {
-                    append_reasoning_text(&tx_event, &mut reasoning_item, s.to_string()).await;
                 }
             }
+            _ => {}
+        }
+        if hints.len() >= MAX_TOOL_HINTS {
+            break;
+        }
+    }
+    hints.reverse();
+    hints
+}
 
-            // Handle streaming function / tool calls.
-            if let Some(tool_calls) = choice
-                .get("delta")
-                .and_then(|d| d.get("tool_calls"))
-                .and_then(|tc| tc.as_array())
-                && let Some(tool_call) = tool_calls.first()
-            {
-                // Mark that we have an active function call in progress.
-                fn_call_state.active = true;
-
-                // Extract call_id if present.
-                if let Some(id) = tool_call.get("id").and_then(|v| v.as_str()) {
-                    fn_call_state.call_id.get_or_insert_with(|| id.to_string());
-                }
+fn summarize_function_call_history(name: &str, arguments: &str) -> Option<String> {
+    match name {
+        "shell" | "container.exec" => summarize_shell_call(arguments),
+        other => Some(format!("requested tool `{other}`")),
+    }
+}
 
-                // Extract function details if present.
-                if let Some(function) = tool_call.get("function") {
-                    if let Some(name) = function.get("name").and_then(|n| n.as_str()) {
-                        fn_call_state.name.get_or_insert_with(|| name.to_string());
-                    }
+fn summarize_local_shell_action(action: &LocalShellAction) -> Option<String> {
+    match action {
+        LocalShellAction::Exec(exec) => {
+            if exec.command.is_empty() {
+                None
+            } else {
+                Some(format!("ran local shell `{}`", exec.command.join(" ")))
+            }
+        }
+    }
+}
 
-                    if let Some(args_fragment) = function.get("arguments").and_then(|a| a.as_str())
-                    {
-                        fn_call_state.arguments.push_str(args_fragment);
-                    }
+fn summarize_tool_output_payload(payload: &FunctionCallOutputPayload) -> Option<String> {
+    if let Some(items) = &payload.content_items {
+        for item in items {
+            if let FunctionCallOutputContentItem::InputText { text } = item {
+                if let Some(snippet) = truncate_hint(text) {
+                    return Some(format!("tool output: {snippet}"));
                 }
             }
+        }
+    }
 
-            // Emit end-of-turn when finish_reason signals completion.
-            if let Some(finish_reason) = choice.get("finish_reason").and_then(|v| v.as_str()) {
-                match finish_reason {
-                    "tool_calls" if fn_call_state.active => {
-                        // First, flush the terminal raw reasoning so UIs can finalize
-                        // the reasoning stream before any exec/tool events begin.
-                        if let Some(item) = reasoning_item.take() {
-                            let _ = tx_event.send(Ok(ResponseEvent::OutputItemDone(item))).await;
-                        }
+    truncate_hint(&payload.content).map(|snippet| format!("tool output: {snippet}"))
+}
 
-                        // Then emit the FunctionCall response item.
-                        let item = ResponseItem::FunctionCall {
-                            id: None,
-                            name: fn_call_state.name.clone().unwrap_or_else(|| "".to_string()),
-                            arguments: fn_call_state.arguments.clone(),
-                            call_id: fn_call_state.call_id.clone().unwrap_or_else(String::new),
-                        };
+fn truncate_hint(text: &str) -> Option<String> {
+    let trimmed = text.trim();
+    if trimmed.is_empty() {
+        return None;
+    }
+    const MAX_LEN: usize = 120;
+    let result = if trimmed.len() <= MAX_LEN {
+        trimmed.to_string()
+    } else {
+        let mut slice = trimmed.chars().take(MAX_LEN).collect::<String>();
+        slice.push('…');
+        slice
+    };
+    Some(result)
+}
 
-                        let _ = tx_event.send(Ok(ResponseEvent::OutputItemDone(item))).await;
-                    }
-                    "stop" => {
-                        // Regular turn without tool-call. Emit the final assistant message
-                        // as a single OutputItemDone so non-delta consumers see the result.
-                        if let Some(item) = assistant_item.take() {
-                            let _ = tx_event.send(Ok(ResponseEvent::OutputItemDone(item))).await;
-                        }
-                        // Also emit a terminal Reasoning item so UIs can finalize raw reasoning.
-                        if let Some(item) = reasoning_item.take() {
-                            let _ = tx_event.send(Ok(ResponseEvent::OutputItemDone(item))).await;
+fn synthesize_shell_tool_call(reasoning: &str) -> Option<BufferedChatCompletionToolCall> {
+    let json_fragment = extract_command_payload(reasoning)?;
+    let value: Value = serde_json::from_str(json_fragment).ok()?;
+    let command_value = value.get("command").or_else(|| value.get("cmd"))?;
+    let command = command_value.as_array()?;
+    if command.is_empty() {
+        return None;
+    }
+    let command_vec: Vec<String> = command
+        .iter()
+        .filter_map(Value::as_str)
+        .map(str::to_string)
+        .collect();
+    if command_vec.is_empty() {
+        return None;
+    }
+    let arguments = json!({ "command": command_vec }).to_string();
+    Some(BufferedChatCompletionToolCall {
+        id: Some(format!("synthetic_shell_{}", Uuid::new_v4().simple())),
+        kind: Some("function".to_string()),
+        function: BufferedChatCompletionToolFunction {
+            name: "shell".to_string(),
+            arguments,
+        },
+    })
+}
+
+fn extract_command_payload(reasoning: &str) -> Option<&str> {
+    let bytes = reasoning.as_bytes();
+    let mut idx = 0;
+    while idx < bytes.len() {
+        if bytes[idx] == b'{' {
+            if reasoning[idx..].starts_with("{\"command\"")
+                || reasoning[idx..].starts_with("{\"cmd\"")
+            {
+                let mut depth = 0;
+                for (offset, ch) in reasoning[idx..].char_indices() {
+                    match ch {
+                        '{' => depth += 1,
+                        '}' => {
+                            depth -= 1;
+                            if depth == 0 {
+                                return Some(&reasoning[idx..idx + offset + 1]);
+                            }
                         }
+                        _ => {}
                     }
-                    _ => {}
                 }
+            }
+        }
+        idx += 1;
+    }
+    None
+}
 
-                // Emit Completed regardless of reason so the agent can advance.
-                let _ = tx_event
-                    .send(Ok(ResponseEvent::Completed {
-                        response_id: String::new(),
-                        token_usage: None,
-                    }))
-                    .await;
+#[derive(Debug, Deserialize)]
+struct BufferedChatCompletionResponse {
+    id: Option<String>,
+    choices: Vec<BufferedChatCompletionChoice>,
+    usage: Option<BufferedChatCompletionUsage>,
+}
 
-                // Prepare for potential next turn (should not happen in same stream).
-                // fn_call_state = FunctionCallState::default();
+#[derive(Debug, Deserialize)]
+struct BufferedChatCompletionChoice {
+    message: BufferedChatCompletionMessage,
+    finish_reason: Option<String>,
+}
 
-                return; // End processing for this SSE stream.
-            }
+#[derive(Debug, Deserialize)]
+struct BufferedChatCompletionMessage {
+    role: Option<String>,
+    content: Option<Value>,
+    tool_calls: Option<Vec<BufferedChatCompletionToolCall>>,
+    #[serde(default)]
+    reasoning_content: Option<String>,
+}
+
+#[derive(Debug, Deserialize)]
+struct BufferedChatCompletionToolCall {
+    id: Option<String>,
+    #[serde(rename = "type")]
+    kind: Option<String>,
+    function: BufferedChatCompletionToolFunction,
+}
+
+#[derive(Debug, Deserialize)]
+struct BufferedChatCompletionToolFunction {
+    name: String,
+    arguments: String,
+}
+
+#[derive(Debug, Deserialize)]
+struct BufferedChatCompletionUsage {
+    prompt_tokens: Option<i64>,
+    completion_tokens: Option<i64>,
+    total_tokens: Option<i64>,
+    prompt_tokens_details: Option<BufferedPromptTokensDetails>,
+    completion_tokens_details: Option<BufferedCompletionTokensDetails>,
+}
+
+#[derive(Debug, Deserialize)]
+struct BufferedPromptTokensDetails {
+    cached_tokens: Option<i64>,
+}
+
+#[derive(Debug, Deserialize)]
+struct BufferedCompletionTokensDetails {
+    reasoning_tokens: Option<i64>,
+}
+
+impl From<BufferedChatCompletionUsage> for TokenUsage {
+    fn from(value: BufferedChatCompletionUsage) -> Self {
+        TokenUsage {
+            input_tokens: value.prompt_tokens.unwrap_or(0),
+            cached_input_tokens: value
+                .prompt_tokens_details
+                .and_then(|d| d.cached_tokens)
+                .unwrap_or(0),
+            output_tokens: value.completion_tokens.unwrap_or(0),
+            reasoning_output_tokens: value
+                .completion_tokens_details
+                .and_then(|d| d.reasoning_tokens)
+                .unwrap_or(0),
+            total_tokens: value.total_tokens.unwrap_or(0),
+        }
+    }
+}
+
+#[cfg(test)]
+mod tests {
+    use super::*;
+    use crate::model_family::find_family_for_model;
+
+    fn user_msg(text: &str) -> ResponseItem {
+        ResponseItem::Message {
+            id: None,
+            role: "user".to_string(),
+            content: vec![ContentItem::InputText {
+                text: text.to_string(),
+            }],
+        }
+    }
+
+    fn assistant_msg(text: &str) -> ResponseItem {
+        ResponseItem::Message {
+            id: None,
+            role: "assistant".to_string(),
+            content: vec![ContentItem::OutputText {
+                text: text.to_string(),
+            }],
+        }
+    }
+
+    fn shell_call() -> ResponseItem {
+        ResponseItem::FunctionCall {
+            id: None,
+            name: "shell".to_string(),
+            arguments: r#"{"command":["echo","hello"]}"#.to_string(),
+            call_id: "call-1".to_string(),
+        }
+    }
+
+    fn shell_output(summary: &str) -> ResponseItem {
+        ResponseItem::FunctionCallOutput {
+            call_id: "call-1".to_string(),
+            output: FunctionCallOutputPayload {
+                content: summary.to_string(),
+                ..Default::default()
+            },
         }
     }
+
+    #[test]
+    fn needs_followup_when_tools_have_no_summary() {
+        let history = vec![user_msg("do it"), shell_call(), shell_output("done")];
+        assert!(needs_summary_followup(&history));
+    }
+
+    #[test]
+    fn skips_followup_once_assistant_replies() {
+        let history = vec![
+            user_msg("do it"),
+            shell_call(),
+            shell_output("done"),
+            assistant_msg("all set"),
+        ];
+        assert!(!needs_summary_followup(&history));
+    }
+
+    #[test]
+    fn collects_recent_tool_hints_including_output() {
+        let history = vec![
+            user_msg("do it"),
+            shell_call(),
+            shell_output("Exit code: 0\nOutput: ok"),
+        ];
+        let hints = collect_recent_tool_hints(&history);
+        assert!(hints.iter().any(|h| h.action.contains("shell")));
+        assert!(hints.iter().any(|h| h.outcome.as_deref().is_some()));
+    }
+
+    #[test]
+    fn build_chat_messages_reports_when_summary_prompt_added() {
+        let mut prompt = Prompt::default();
+        prompt.input = vec![user_msg("who?"), shell_call(), shell_output("ok")];
+        let family = find_family_for_model("vercel/gpt-oss-120b").expect("family exists");
+        let (_messages, appended) = build_chat_messages(&prompt, &family);
+        assert!(appended);
+
+        let mut prompt = Prompt::default();
+        prompt.input = vec![
+            user_msg("who?"),
+            shell_call(),
+            shell_output("ok"),
+            assistant_msg("done"),
+        ];
+        let (_messages, appended) = build_chat_messages(&prompt, &family);
+        assert!(!appended);
+    }
 }
 
 /// Optional client-side aggregation helper
@@ -776,6 +1984,10 @@ where
                         &item,
                         codex_protocol::models::ResponseItem::Message { role, .. } if role == "assistant"
                     );
+                    let is_reasoning = matches!(
+                        &item,
+                        codex_protocol::models::ResponseItem::Reasoning { .. }
+                    );
 
                     if is_assistant_message {
                         match this.mode {
@@ -814,6 +2026,9 @@ where
                             }
                         }
                     }
+                    if is_reasoning && matches!(this.mode, AggregateMode::AggregatedOnly) {
+                        this.cumulative_reasoning.clear();
+                    }
 
                     // Not an assistant message – forward immediately.
                     return Poll::Ready(Some(Ok(ResponseEvent::OutputItemDone(item))));
@@ -965,3 +2180,175 @@ impl<S> AggregatedChatStream<S> {
         Self::new(inner, AggregateMode::Streaming)
     }
 }
+async fn execute_buffered_request(
+    payload: &Value,
+    message_count: usize,
+    tools_count: usize,
+    client: &CodexHttpClient,
+    provider: &ModelProviderInfo,
+    otel_event_manager: &OtelEventManager,
+    session_source: &SessionSource,
+    model_label: &str,
+) -> Result<BufferedChatCompletionResponse> {
+    let mut attempt = 0;
+    let max_retries = provider.request_max_retries();
+    loop {
+        attempt += 1;
+
+        let mut req_builder = provider.create_request_builder(client, &None).await?;
+
+        if let SessionSource::SubAgent(sub) = session_source.clone() {
+            let subagent = if let SubAgentSource::Other(label) = sub {
+                label
+            } else {
+                serde_json::to_value(&sub)
+                    .ok()
+                    .and_then(|v| v.as_str().map(std::string::ToString::to_string))
+                    .unwrap_or_else(|| "other".to_string())
+            };
+            req_builder = req_builder.header("x-openai-subagent", subagent);
+        }
+
+        info!(
+            target: "codex_litellm_debug::model_request",
+            attempt,
+            max_retries,
+            model = model_label,
+            provider = %provider.name,
+            message_count,
+            tools_count,
+            "chat_completions.buffered_request_start"
+        );
+
+        let res = otel_event_manager
+            .log_request(attempt, || {
+                req_builder
+                    .header(reqwest::header::ACCEPT, "application/json")
+                    .json(payload)
+                    .send()
+            })
+            .await;
+
+        match res {
+            Ok(resp) if resp.status().is_success() => {
+                let status_code = resp.status();
+                info!(
+                    target: "codex_litellm_debug::model_response",
+                    attempt,
+                    model = model_label,
+                    provider = %provider.name,
+                    status = status_code.as_u16(),
+                    "chat_completions.buffered_response_open"
+                );
+                let body = resp.text().await.map_err(|e| {
+                    CodexErr::ResponseStreamFailed(ResponseStreamFailed {
+                        source: e,
+                        request_id: None,
+                    })
+                })?;
+
+                if provider.name.contains("LiteLLM") {
+                    info!(
+                        target: "codex_litellm_debug::oss_buffered",
+                        attempt,
+                        model = model_label,
+                        provider = %provider.name,
+                        body = body.as_str(),
+                        "chat_completions.buffered_response_body"
+                    );
+                }
+
+                let parsed: BufferedChatCompletionResponse =
+                    serde_json::from_str(&body).map_err(|_err| {
+                        CodexErr::UnexpectedStatus(UnexpectedResponseError {
+                            status: status_code,
+                            body,
+                            request_id: None,
+                        })
+                    })?;
+                return Ok(parsed);
+            }
+            Ok(res) => {
+                let status = res.status();
+                if !(status == StatusCode::TOO_MANY_REQUESTS || status.is_server_error()) {
+                    let body = (res.text().await).unwrap_or_default();
+                    let body_preview: String = body.chars().take(200).collect();
+                    info!(
+                        target: "codex_litellm_debug::model_response",
+                        attempt,
+                        model = model_label,
+                        provider = %provider.name,
+                        status = status.as_u16(),
+                        body_preview = body_preview,
+                        "chat_completions.buffered_response_unexpected_status"
+                    );
+                    return Err(CodexErr::UnexpectedStatus(UnexpectedResponseError {
+                        status,
+                        body,
+                        request_id: None,
+                    }));
+                }
+
+                if attempt > max_retries {
+                    info!(
+                        target: "codex_litellm_debug::model_response",
+                        attempt,
+                        model = model_label,
+                        provider = %provider.name,
+                        status = status.as_u16(),
+                        "chat_completions.buffered_retry_limit"
+                    );
+                    return Err(CodexErr::RetryLimit(RetryLimitReachedError {
+                        status,
+                        request_id: None,
+                    }));
+                }
+
+                let retry_after_secs = res
+                    .headers()
+                    .get(reqwest::header::RETRY_AFTER)
+                    .and_then(|v| v.to_str().ok())
+                    .and_then(|s| s.parse::<u64>().ok());
+
+                let delay = retry_after_secs
+                    .map(|s| Duration::from_millis(s * 1_000))
+                    .unwrap_or_else(|| backoff(attempt));
+                info!(
+                    target: "codex_litellm_debug::model_response",
+                    attempt,
+                    model = model_label,
+                    provider = %provider.name,
+                    status = status.as_u16(),
+                    retry_after_secs,
+                    delay_ms = delay.as_millis(),
+                    "chat_completions.buffered_retry_scheduled"
+                );
+                tokio::time::sleep(delay).await;
+            }
+            Err(e) => {
+                if attempt > max_retries {
+                    warn!(
+                        target: "codex_litellm_debug::model_response",
+                        attempt,
+                        model = model_label,
+                        provider = %provider.name,
+                        error = %e,
+                        "chat_completions.buffered_request_failed"
+                    );
+                    return Err(CodexErr::ConnectionFailed(ConnectionFailedError::from(e)));
+                }
+                let delay = backoff(attempt);
+                warn!(
+                    target: "codex_litellm_debug::model_response",
+                    attempt,
+                    model = model_label,
+                    provider = %provider.name,
+                    error = %e,
+                    delay_ms = delay.as_millis(),
+                    "chat_completions.buffered_request_retry"
+                );
+                tokio::time::sleep(delay).await;
+            }
+        }
+    }
+}
diff --git a/codex-rs/core/src/client.rs b/codex-rs/core/src/client.rs
index 9dfa3a131..4aa0a4f98 100644
--- a/codex-rs/core/src/client.rs
+++ b/codex-rs/core/src/client.rs
@@ -32,6 +32,7 @@ use tracing::warn;
 use crate::AuthManager;
 use crate::auth::CodexAuth;
 use crate::chat_completions::AggregateStreamExt;
+use crate::chat_completions::fetch_chat_completions_buffered;
 use crate::chat_completions::stream_chat_completions;
 use crate::client_common::Prompt;
 use crate::client_common::ResponseEvent;
@@ -144,15 +145,36 @@ impl ModelClient {
             WireApi::Responses => self.stream_responses(prompt).await,
             WireApi::Chat => {
                 // Create the raw streaming connection first.
-                let response_stream = stream_chat_completions(
-                    prompt,
-                    &self.config.model_family,
-                    &self.client,
-                    &self.provider,
-                    &self.otel_event_manager,
-                    &self.session_source,
-                )
-                .await?;
+                let response_stream = if self.config.model_family.prefer_non_stream {
+                    debug!(
+                        model = %self.config.model,
+                        "chat_completions: using buffered LiteLLM request"
+                    );
+                    fetch_chat_completions_buffered(
+                        prompt,
+                        &self.config.model_family,
+                        &self.client,
+                        &self.provider,
+                        &self.otel_event_manager,
+                        &self.session_source,
+                        self.config.model_family.force_followups_after_tools,
+                    )
+                    .await
+                } else {
+                    stream_chat_completions(
+                        prompt,
+                        &self.config.model_family,
+                        &self.client,
+                        &self.provider,
+                        &self.otel_event_manager,
+                        &self.session_source,
+                    )
+                    .await
+                }?;
+
+                if self.config.model_family.prefer_non_stream {
+                    return Ok(response_stream);
+                }
 
                 // Wrap it with the aggregation adapter so callers see *only*
                 // the final assistant message per turn (matching the
@@ -452,7 +474,7 @@ impl ModelClient {
                 })
             }
             Err(e) => Err(StreamAttemptError::RetryableTransportError(
-                CodexErr::ConnectionFailed(ConnectionFailedError { source: e }),
+                CodexErr::ConnectionFailed(ConnectionFailedError::from(e)),
             )),
         }
     }
diff --git a/codex-rs/core/src/codex.rs b/codex-rs/core/src/codex.rs
index a2756a7ad..c2e9bd29a 100644
--- a/codex-rs/core/src/codex.rs
+++ b/codex-rs/core/src/codex.rs
@@ -3,9 +3,11 @@ use std::fmt::Debug;
 use std::path::PathBuf;
 use std::sync::Arc;
 use std::sync::atomic::AtomicU64;
+use std::time::Instant;
 
 use crate::AuthManager;
 use crate::client_common::REVIEW_PROMPT;
+use crate::client_common::tools::ToolSpec;
 use crate::features::Feature;
 use crate::function_tool::FunctionCallError;
 use crate::mcp::auth::McpAuthStatusEntry;
@@ -66,6 +68,8 @@ use crate::error::Result as CodexResult;
 use crate::exec::StreamOutput;
 // Removed: legacy executor wiring replaced by ToolOrchestrator flows.
 // legacy normalize_exec_result no longer used after orchestrator migration
+use crate::chat_completions::FALLBACK_ASSISTANT_MESSAGE_ID;
+use crate::chat_completions::SUMMARY_FOLLOWUP_PROMPT;
 use crate::mcp::auth::compute_auth_statuses;
 use crate::mcp_connection_manager::McpConnectionManager;
 use crate::model_family::find_family_for_model;
@@ -1173,6 +1177,20 @@ impl Session {
         }
     }
 
+    pub async fn inject_response_input(&self, input: Vec<ResponseInputItem>) -> bool {
+        let mut active = self.active_turn.lock().await;
+        match active.as_mut() {
+            Some(at) => {
+                let mut ts = at.turn_state.lock().await;
+                for item in input {
+                    ts.push_pending_input(item);
+                }
+                true
+            }
+            None => false,
+        }
+    }
+
     pub async fn get_pending_input(&self) -> Vec<ResponseInputItem> {
         let mut active = self.active_turn.lock().await;
         match active.as_mut() {
@@ -1754,6 +1772,18 @@ pub(crate) async fn run_task(
     // many turns, from the perspective of the user, it is a single turn.
     let turn_diff_tracker = Arc::new(tokio::sync::Mutex::new(TurnDiffTracker::new()));
     let mut auto_compact_recently_attempted = false;
+    let model_family = turn_context.client.get_model_family();
+    let prefer_non_stream = model_family.prefer_non_stream;
+    let force_followups_after_tools = model_family.force_followups_after_tools;
+    debug!(
+        prefer_non_stream,
+        force_followups_after_tools,
+        model_slug = %model_family.slug,
+        "run_task follow-up flags"
+    );
+    let mut total_forced_summary_followups_sent: u32 = 0;
+    let turn_start = Instant::now();
+    let mut buffered_iteration: u32 = 0;
 
     loop {
         // Note that pending_input would be something like a message the user
@@ -1772,6 +1802,16 @@ pub(crate) async fn run_task(
                 .await;
             sess.clone_history().await.get_history_for_prompt()
         };
+        let pending_count = pending_input.len();
+        let history_len = turn_input.len();
+        buffered_iteration = buffered_iteration.saturating_add(1);
+        let elapsed = turn_start.elapsed().as_secs_f32();
+        let prep_event = EventMsg::BackgroundEvent(BackgroundEventEvent {
+            message: format!(
+                "Preparing buffered turn #{buffered_iteration}: pending_input={pending_count} history_items={history_len} elapsed={elapsed:.1}s"
+            ),
+        });
+        sess.send_event(&turn_context, prep_event).await;
 
         let turn_input_messages = turn_input
             .iter()
@@ -1829,7 +1869,48 @@ pub(crate) async fn run_task(
 
                 auto_compact_recently_attempted = false;
 
+                let force_followups = force_followups_after_tools
+                    && !matches!(
+                        turn_context.client.get_session_source(),
+                        SessionSource::Exec
+                    );
+                let followup_reason = determine_followup_reason(
+                    &items_to_record_in_conversation_history,
+                    force_followups,
+                );
+                let should_force_followup = followup_reason != FollowupReason::None
+                    && prefer_non_stream
+                    && total_forced_summary_followups_sent
+                        < MAX_FORCED_NON_STREAM_SUMMARY_FOLLOWUPS;
+                debug!(
+                    ?followup_reason,
+                    total_forced_summary_followups_sent, "non-stream follow-up evaluation"
+                );
+
+                if should_force_followup {
+                    let original_request = turn_input_messages.first().cloned();
+                    let prompt =
+                        build_forced_followup_prompt(original_request.as_deref(), followup_reason);
+                    if try_enqueue_summary_followup(&sess, prompt).await {
+                        let elapsed = turn_start.elapsed().as_secs_f32();
+                        let msg = EventMsg::BackgroundEvent(BackgroundEventEvent {
+                            message: format!(
+                                "Auto-followup queued ({:?}) after {:.1}s",
+                                followup_reason, elapsed
+                            ),
+                        });
+                        sess.send_event(&turn_context, msg).await;
+                        total_forced_summary_followups_sent += 1;
+                        continue;
+                    }
+                }
+
                 if responses.is_empty() {
+                    let elapsed = turn_start.elapsed().as_secs_f32();
+                    let done_msg = EventMsg::BackgroundEvent(BackgroundEventEvent {
+                        message: format!("Buffered turn complete after {:.1}s", elapsed),
+                    });
+                    sess.send_event(&turn_context, done_msg).await;
                     last_agent_message = get_last_assistant_message_from_turn(
                         &items_to_record_in_conversation_history,
                     );
@@ -1843,11 +1924,40 @@ pub(crate) async fn run_task(
                         });
                     break;
                 }
+
+                let msg = EventMsg::BackgroundEvent(BackgroundEventEvent {
+                    message: format!(
+                        "Queued {} tool outputs for buffered follow-up",
+                        responses.len()
+                    ),
+                });
+                sess.send_event(&turn_context, msg).await;
+                let elapsed = turn_start.elapsed().as_secs_f32();
+                let msg = EventMsg::BackgroundEvent(BackgroundEventEvent {
+                    message: format!(
+                        "Queued {} tool outputs for buffered follow-up (elapsed {:.1}s)",
+                        responses.len(),
+                        elapsed
+                    ),
+                });
+                sess.send_event(&turn_context, msg).await;
+                if !sess.inject_response_input(responses).await {
+                    warn!("failed to enqueue response input for follow-up turn");
+                    break;
+                }
                 continue;
             }
             Err(CodexErr::TurnAborted {
                 dangling_artifacts: processed_items,
             }) => {
+                let elapsed = turn_start.elapsed().as_secs_f32();
+                let abort_event = EventMsg::BackgroundEvent(BackgroundEventEvent {
+                    message: format!(
+                        "Turn aborted while buffered follow-up in flight (elapsed {:.1}s)",
+                        elapsed
+                    ),
+                });
+                sess.send_event(&turn_context, abort_event).await;
                 let _ = process_items(processed_items, &sess, &turn_context).await;
                 // Aborted turn is reported via a different event.
                 break;
@@ -1867,6 +1977,168 @@ pub(crate) async fn run_task(
     last_agent_message
 }
 
+const MAX_FORCED_NON_STREAM_SUMMARY_FOLLOWUPS: u32 = 3;
+
+async fn try_enqueue_summary_followup(sess: &Arc<Session>, prompt: String) -> bool {
+    match sess
+        .inject_input(vec![UserInput::Text { text: prompt }])
+        .await
+    {
+        Ok(_) => true,
+        Err(returned_input) => {
+            warn!(
+                "failed to enqueue summary follow-up input: {:?}",
+                returned_input
+            );
+            false
+        }
+    }
+}
+
+#[derive(Copy, Clone, Eq, PartialEq, Debug)]
+enum FollowupReason {
+    None,
+    MissingAssistantAfterTool,
+    NoToolWork,
+    ReadOnlyToolLoop,
+}
+
+fn determine_followup_reason(items: &[ResponseItem], force_after_tools: bool) -> FollowupReason {
+    if items.is_empty() {
+        return FollowupReason::None;
+    }
+
+    let mut saw_tool = false;
+    let mut saw_assistant = false;
+    let mut read_only_tool_calls = true;
+    let mut total_tool_calls = 0;
+
+    for item in items {
+        match item {
+            ResponseItem::FunctionCall {
+                name, arguments, ..
+            } => {
+                saw_tool = true;
+                total_tool_calls += 1;
+                if !is_read_only_tool_call(name, arguments) {
+                    read_only_tool_calls = false;
+                }
+            }
+            ResponseItem::LocalShellCall { .. }
+            | ResponseItem::CustomToolCall { .. }
+            | ResponseItem::WebSearchCall { .. } => {
+                saw_tool = true;
+                total_tool_calls += 1;
+                read_only_tool_calls = false;
+            }
+            ResponseItem::Message { role, id, .. } if role == "assistant" => {
+                if id.as_deref() != Some(FALLBACK_ASSISTANT_MESSAGE_ID) {
+                    saw_assistant = true;
+                }
+            }
+            _ => {}
+        }
+    }
+
+    if saw_tool && read_only_tool_calls && total_tool_calls >= 2 {
+        return FollowupReason::ReadOnlyToolLoop;
+    }
+
+    if saw_tool && !saw_assistant {
+        return FollowupReason::MissingAssistantAfterTool;
+    }
+
+    if !saw_tool {
+        if saw_assistant {
+            return FollowupReason::None;
+        }
+        return if force_after_tools {
+            FollowupReason::NoToolWork
+        } else {
+            FollowupReason::None
+        };
+    }
+
+    FollowupReason::None
+}
+
+fn build_forced_followup_prompt(original_request: Option<&str>, reason: FollowupReason) -> String {
+    let original = original_request.unwrap_or_default().trim();
+    let original_suffix = if original.is_empty() {
+        String::new()
+    } else {
+        format!("\n\nOriginal request: {original}")
+    };
+
+    match reason {
+        FollowupReason::MissingAssistantAfterTool => format!(
+            "{SUMMARY_FOLLOWUP_PROMPT}\n\nThe previous turn ended after tool output but no assistant answer was given. If the evidence you gathered already resolves the request, stop calling tools and provide the final answer immediately; otherwise run only the next necessary command before summarizing.{original_suffix}"
+        ),
+        FollowupReason::NoToolWork => format!(
+            "{SUMMARY_FOLLOWUP_PROMPT}\n\nYou have not executed any tools yet. Start modifying the repository immediately, then summarize your changes.{original_suffix}"
+        ),
+        FollowupReason::ReadOnlyToolLoop => format!(
+            "{SUMMARY_FOLLOWUP_PROMPT}\n\nYou have only listed files or searched text so far. Stop gathering reconnaissance and start applying concrete edits (create files, run apply_patch, update code) before summarizing.{original_suffix}"
+        ),
+        FollowupReason::None => SUMMARY_FOLLOWUP_PROMPT.to_string(),
+    }
+}
+
+fn is_read_only_tool_call(name: &str, arguments: &str) -> bool {
+    let canonical = name.rsplit('.').next().unwrap_or(name);
+    match canonical {
+        "shell" => is_read_only_shell_call(arguments),
+        // MCP discovery-style tools are inherently read-only.
+        "list_dir" | "grep_files" | "read_file" | "plan" | "view_image" => true,
+        _ => false,
+    }
+}
+
+fn is_read_only_shell_call(arguments: &str) -> bool {
+    if let Ok(value) = serde_json::from_str::<Value>(arguments) {
+        if let Some(command_value) = value.get("command").or_else(|| value.get("cmd")) {
+            if let Some(parts) = command_value.as_array() {
+                let tokens = parts
+                    .iter()
+                    .filter_map(Value::as_str)
+                    .map(str::to_string)
+                    .collect::<Vec<_>>();
+                if tokens.is_empty() {
+                    return false;
+                }
+                let script = if tokens.len() >= 3 && tokens[0] == "bash" && tokens[1] == "-lc" {
+                    tokens[2].clone()
+                } else {
+                    tokens.join(" ")
+                };
+                return is_read_only_shell_script(&script);
+            }
+        }
+    }
+    false
+}
+
+fn is_read_only_shell_script(script: &str) -> bool {
+    let trimmed = script.trim();
+    if trimmed.is_empty() {
+        return false;
+    }
+    if trimmed.contains(">>")
+        || trimmed.contains("<<")
+        || trimmed.contains(" >")
+        || trimmed.contains("apply_patch")
+        || trimmed.contains("cat <<")
+    {
+        return false;
+    }
+    const READ_ONLY_PREFIXES: &[&str] = &[
+        "ls", "rg", "grep", "find", "cat", "sed", "head", "tail", "wc", "stat", "tree", "pwd",
+    ];
+    READ_ONLY_PREFIXES
+        .iter()
+        .any(|prefix| trimmed.starts_with(prefix))
+}
+
 async fn run_turn(
     sess: Arc<Session>,
     turn_context: Arc<TurnContext>,
@@ -1880,14 +2152,22 @@ async fn run_turn(
         Some(mcp_tools),
     ));
 
-    let model_supports_parallel = turn_context
-        .client
-        .get_model_family()
-        .supports_parallel_tool_calls;
+    let model_family = turn_context.client.get_model_family();
+    let model_supports_parallel = model_family.supports_parallel_tool_calls;
     let parallel_tool_calls = model_supports_parallel;
+    let mut tools = router.specs();
+    if !model_family.force_followups_after_tools {
+        tools.retain(|tool| {
+            !matches!(
+                tool,
+                ToolSpec::Function(f) if f.name == "update_plan"
+                    || matches!(tool, ToolSpec::Freeform(ff) if ff.name == "update_plan")
+            )
+        });
+    }
     let prompt = Prompt {
         input,
-        tools: router.specs(),
+        tools,
         parallel_tool_calls,
         base_instructions_override: turn_context.base_instructions.clone(),
         output_schema: turn_context.final_output_json_schema.clone(),
diff --git a/codex-rs/core/src/config/mod.rs b/codex-rs/core/src/config/mod.rs
index 1ee48d30b..a3999e34c 100644
--- a/codex-rs/core/src/config/mod.rs
+++ b/codex-rs/core/src/config/mod.rs
@@ -11,6 +11,8 @@ use crate::config::types::ReasoningSummaryFormat;
 use crate::config::types::SandboxWorkspaceWrite;
 use crate::config::types::ShellEnvironmentPolicy;
 use crate::config::types::ShellEnvironmentPolicyToml;
+use crate::config::types::TelemetryLogToml;
+use crate::config::types::TelemetryToml;
 use crate::config::types::Tui;
 use crate::config::types::UriBasedFileOpener;
 use crate::config_loader::LoadedConfigLayers;
@@ -32,6 +34,7 @@ use crate::project_doc::DEFAULT_PROJECT_DOC_FILENAME;
 use crate::project_doc::LOCAL_PROJECT_DOC_FILENAME;
 use crate::protocol::AskForApproval;
 use crate::protocol::SandboxPolicy;
+use chrono::Utc;
 use codex_app_server_protocol::Tools;
 use codex_app_server_protocol::UserSavedConfig;
 use codex_protocol::config_types::ForcedLoginMethod;
@@ -53,6 +56,9 @@ use std::path::PathBuf;
 use crate::config::profile::ConfigProfile;
 use toml::Value as TomlValue;
 use toml_edit::DocumentMut;
+use toml_edit::Item as TomlItem;
+use toml_edit::Table as TomlTable;
+use toml_edit::{Array as TomlArray, value};
 
 pub mod edit;
 pub mod profile;
@@ -71,6 +77,473 @@ pub const GPT_5_CODEX_MEDIUM_MODEL: &str = "gpt-5-codex";
 pub(crate) const PROJECT_DOC_MAX_BYTES: usize = 32 * 1024; // 32 KiB
 
 pub(crate) const CONFIG_TOML_FILE: &str = "config.toml";
+pub(crate) const LITELLM_PROVIDER_ID: &str = "litellm";
+const LEGACY_LITELLM_PROVIDER_ID: &str = "litellm-direct";
+pub(crate) const LITELLM_DEFAULT_MODEL: &str = "";
+const LITELLM_BASE_URL_ENV: &str = "LITELLM_BASE_URL";
+const LITELLM_API_KEY_ENV: &str = "LITELLM_API_KEY";
+const LITELLM_PROVIDER_NAME: &str = "LiteLLM Direct";
+const LITELLM_DEFAULT_MAX_TOKENS: i64 = 128_000;
+const LITELLM_DEFAULT_SANDBOX_MODE: &str = "workspace-write";
+const LITELLM_DEFAULT_APPROVAL_POLICY: &str = "never";
+const LITELLM_DEFAULT_REQUEST_MAX_RETRIES: i64 = 4;
+const LITELLM_DEFAULT_STREAM_MAX_RETRIES: i64 = 5;
+const LITELLM_DEFAULT_STREAM_IDLE_TIMEOUT_MS: i64 = 300_000;
+const LITELLM_DEFAULT_CONTEXT_WINDOW: i64 = 130_000;
+const LITELLM_DEFAULT_AUTO_COMPACT_TOKEN_LIMIT: i64 = (LITELLM_DEFAULT_CONTEXT_WINDOW * 9) / 10;
+const DEFAULT_TELEMETRY_LOG_DIR: &str = "logs";
+const DEFAULT_TUI_LOG: &str = "codex-tui.log";
+const DEFAULT_SESSION_LOG: &str = "codex-litellm-session.jsonl";
+const DEFAULT_TELEMETRY_MAX_TOTAL_BYTES: u64 = 100 * 1024 * 1024;
+
+#[derive(Debug, Clone, Default)]
+pub struct LiteLlmProviderUpdate {
+    pub base_url: Option<String>,
+    pub api_key: Option<String>,
+}
+
+impl LiteLlmProviderUpdate {
+    fn is_empty(&self) -> bool {
+        self.base_url.is_none() && self.api_key.is_none()
+    }
+}
+
+#[derive(Debug, Clone)]
+pub struct LiteLlmProviderState {
+    pub base_url: Option<String>,
+    pub api_key: Option<String>,
+}
+
+#[derive(Debug, Clone, PartialEq)]
+pub struct TelemetryLogConfig {
+    pub enabled: bool,
+    pub path: PathBuf,
+}
+
+#[derive(Debug, Clone, PartialEq)]
+pub struct TelemetryConfig {
+    pub enabled: bool,
+    pub dir: PathBuf,
+    pub logs: HashMap<String, TelemetryLogConfig>,
+    pub max_total_bytes: Option<u64>,
+    pub session_usage_enabled: bool,
+    pub tui_log_enabled: bool,
+    pub debug_log_enabled: bool,
+}
+
+impl TelemetryConfig {
+    pub fn log_config(&self, name: &str) -> Option<&TelemetryLogConfig> {
+        self.logs.get(name)
+    }
+}
+
+impl Default for TelemetryConfig {
+    fn default() -> Self {
+        Self {
+            enabled: true,
+            dir: PathBuf::new(),
+            logs: HashMap::new(),
+            max_total_bytes: None,
+            session_usage_enabled: true,
+            tui_log_enabled: false,
+            debug_log_enabled: cfg!(debug_assertions),
+        }
+    }
+}
+
+pub fn ensure_litellm_baseline(codex_home: &Path) -> std::io::Result<()> {
+    std::fs::create_dir_all(codex_home)?;
+    let config_path = codex_home.join(CONFIG_TOML_FILE);
+    let mut doc = load_config_document(&config_path)?;
+    let mut changed = migrate_legacy_litellm_provider(&mut doc);
+
+    if !doc.as_table().contains_key("model_provider") {
+        doc["model_provider"] = value(LITELLM_PROVIDER_ID);
+        changed = true;
+    }
+
+    if !doc.as_table().contains_key("model") {
+        doc["model"] = value(LITELLM_DEFAULT_MODEL);
+        changed = true;
+    }
+
+    if !doc.as_table().contains_key("max_tokens") {
+        doc["max_tokens"] = value(LITELLM_DEFAULT_MAX_TOKENS);
+        changed = true;
+    }
+
+    if !doc.as_table().contains_key("model_context_window") {
+        doc["model_context_window"] = value(LITELLM_DEFAULT_CONTEXT_WINDOW);
+        changed = true;
+    }
+
+    if !doc.as_table().contains_key("context_length") {
+        doc["context_length"] = value(LITELLM_DEFAULT_CONTEXT_WINDOW);
+        changed = true;
+    }
+
+    if !doc
+        .as_table()
+        .contains_key("model_auto_compact_token_limit")
+    {
+        doc["model_auto_compact_token_limit"] = value(LITELLM_DEFAULT_AUTO_COMPACT_TOKEN_LIMIT);
+        changed = true;
+    }
+
+    if !doc.as_table().contains_key("sandbox_mode") {
+        doc["sandbox_mode"] = value(LITELLM_DEFAULT_SANDBOX_MODE);
+        changed = true;
+    }
+
+    if !doc.as_table().contains_key("approval_policy") {
+        doc["approval_policy"] = value(LITELLM_DEFAULT_APPROVAL_POLICY);
+        changed = true;
+    }
+
+    let provider_table = ensure_litellm_provider_table(&mut doc);
+    if !provider_table.contains_key("name") {
+        provider_table["name"] = value(LITELLM_PROVIDER_NAME);
+        changed = true;
+    }
+    if !provider_table.contains_key("wire_api") {
+        provider_table["wire_api"] = value("chat");
+        changed = true;
+    }
+    if !provider_table.contains_key("requires_openai_auth") {
+        provider_table["requires_openai_auth"] = value(false);
+        changed = true;
+    }
+    if !provider_table.contains_key("request_max_retries") {
+        provider_table["request_max_retries"] = value(LITELLM_DEFAULT_REQUEST_MAX_RETRIES);
+        changed = true;
+    }
+    if !provider_table.contains_key("stream_max_retries") {
+        provider_table["stream_max_retries"] = value(LITELLM_DEFAULT_STREAM_MAX_RETRIES);
+        changed = true;
+    }
+    if !provider_table.contains_key("stream_idle_timeout_ms") {
+        provider_table["stream_idle_timeout_ms"] = value(LITELLM_DEFAULT_STREAM_IDLE_TIMEOUT_MS);
+        changed = true;
+    }
+
+    if let Some(env_base_url) = litellm_env(LITELLM_BASE_URL_ENV) {
+        let current = provider_table
+            .get("base_url")
+            .and_then(TomlItem::as_str)
+            .map(str::trim)
+            .filter(|value| !value.is_empty());
+        if current != Some(env_base_url.as_str()) {
+            provider_table["base_url"] = value(env_base_url);
+            changed = true;
+        }
+    }
+
+    if let Some(env_api_key) = litellm_env(LITELLM_API_KEY_ENV) {
+        let formatted = format_litellm_bearer(&env_api_key);
+        let current = provider_table
+            .get("experimental_bearer_token")
+            .and_then(TomlItem::as_str)
+            .map(str::trim)
+            .filter(|value| !value.is_empty());
+        if current != Some(formatted.as_str()) {
+            provider_table["experimental_bearer_token"] = value(formatted);
+            changed = true;
+        }
+    }
+
+    let chat_params = provider_table
+        .entry("chat_completion_parameters")
+        .or_insert(TomlItem::Table(TomlTable::new()));
+    if let Some(params_table) = chat_params.as_table_mut() {
+        params_table.set_implicit(false);
+        if !params_table.contains_key("modalities") {
+            let mut arr = TomlArray::new();
+            arr.push("text");
+            params_table["modalities"] = TomlItem::Value(arr.into());
+            changed = true;
+        }
+        let response_format = params_table
+            .entry("response_format")
+            .or_insert(TomlItem::Table(TomlTable::new()));
+        if let Some(response_table) = response_format.as_table_mut() {
+            response_table.set_implicit(false);
+            if !response_table.contains_key("type") {
+                response_table["type"] = value("text");
+                changed = true;
+            }
+        }
+    }
+
+    let telemetry_entry = doc
+        .as_table_mut()
+        .entry("telemetry")
+        .or_insert(TomlItem::Table(TomlTable::new()));
+    if let Some(telemetry_table) = telemetry_entry.as_table_mut() {
+        telemetry_table.set_implicit(false);
+        if !telemetry_table.contains_key("dir") {
+            telemetry_table["dir"] = value(DEFAULT_TELEMETRY_LOG_DIR);
+            changed = true;
+        }
+        if !telemetry_table.contains_key("session_usage") {
+            telemetry_table["session_usage"] = value(true);
+            changed = true;
+        }
+        if !telemetry_table.contains_key("tui") {
+            telemetry_table["tui"] = value(false);
+            changed = true;
+        }
+        if !telemetry_table.contains_key("debug") {
+            telemetry_table["debug"] = value(cfg!(debug_assertions));
+            changed = true;
+        }
+        let logs_item = telemetry_table
+            .entry("logs")
+            .or_insert(TomlItem::Table(TomlTable::new()));
+        if let Some(logs_table) = logs_item.as_table_mut() {
+            logs_table.set_implicit(false);
+            let mut ensure_entry = |name: &str, default_file: Option<&str>| -> bool {
+                let mut modified = false;
+                if !logs_table.contains_key(name) {
+                    let mut entry_table = TomlTable::new();
+                    entry_table.set_implicit(false);
+                    if let Some(file) = default_file {
+                        entry_table["file"] = value(file);
+                    }
+                    logs_table[name] = TomlItem::Table(entry_table);
+                    return true;
+                }
+                if let Some(entry_table) = logs_table.get_mut(name).and_then(TomlItem::as_table_mut)
+                {
+                    entry_table.set_implicit(false);
+                    if let Some(file) = default_file {
+                        if !entry_table.contains_key("file") {
+                            entry_table["file"] = value(file);
+                            modified = true;
+                        }
+                    }
+                }
+                modified
+            };
+            if ensure_entry("tui", Some(DEFAULT_TUI_LOG)) {
+                changed = true;
+            }
+            if ensure_entry("session", Some(DEFAULT_SESSION_LOG)) {
+                changed = true;
+            }
+            if ensure_entry("debug", None) {
+                changed = true;
+            }
+        }
+    }
+
+    if changed {
+        save_config_document(&config_path, &doc)?;
+    }
+
+    Ok(())
+}
+
+fn litellm_env(name: &str) -> Option<String> {
+    std::env::var(name)
+        .ok()
+        .map(|value| value.trim().to_string())
+        .filter(|value| !value.is_empty())
+}
+
+fn format_litellm_bearer(token: &str) -> String {
+    let trimmed = token.trim();
+    if let Some(rest) = trimmed.strip_prefix("Bearer") {
+        let rest = rest.trim_start();
+        if rest.is_empty() {
+            "Bearer".to_string()
+        } else {
+            format!("Bearer {rest}")
+        }
+    } else if let Some(rest) = trimmed.strip_prefix("bearer") {
+        let rest = rest.trim_start();
+        if rest.is_empty() {
+            "Bearer".to_string()
+        } else {
+            format!("Bearer {rest}")
+        }
+    } else {
+        format!("Bearer {trimmed}")
+    }
+}
+
+pub fn read_litellm_provider_state(codex_home: &Path) -> std::io::Result<LiteLlmProviderState> {
+    ensure_litellm_baseline(codex_home)?;
+    let config_path = codex_home.join(CONFIG_TOML_FILE);
+    let doc = load_config_document(&config_path)?;
+    let provider_table = ensure_litellm_provider_table_ref(&doc);
+    let base_url = provider_table
+        .and_then(|table| table.get("base_url"))
+        .and_then(TomlItem::as_str)
+        .map(|value| value.trim().to_string())
+        .filter(|value| !value.is_empty());
+    let api_key = provider_table
+        .and_then(|table| table.get("experimental_bearer_token"))
+        .and_then(TomlItem::as_str)
+        .map(|value| value.trim().to_string())
+        .filter(|value| !value.is_empty());
+
+    Ok(LiteLlmProviderState { base_url, api_key })
+}
+
+pub fn write_litellm_provider_state(
+    codex_home: &Path,
+    update: LiteLlmProviderUpdate,
+) -> std::io::Result<()> {
+    if update.is_empty() {
+        return Ok(());
+    }
+
+    ensure_litellm_baseline(codex_home)?;
+    let config_path = codex_home.join(CONFIG_TOML_FILE);
+    let mut doc = load_config_document(&config_path)?;
+    let provider_table = ensure_litellm_provider_table(&mut doc);
+    let mut changed = false;
+
+    if let Some(base_url) = update.base_url {
+        if base_url.is_empty() {
+            provider_table.remove("base_url");
+        } else {
+            provider_table["base_url"] = value(base_url);
+        }
+        changed = true;
+    }
+
+    if let Some(api_key) = update.api_key {
+        if api_key.is_empty() {
+            provider_table.remove("experimental_bearer_token");
+        } else {
+            provider_table["experimental_bearer_token"] = value(api_key);
+        }
+        changed = true;
+    }
+
+    if changed {
+        save_config_document(&config_path, &doc)?;
+    }
+
+    Ok(())
+}
+
+pub fn set_default_model(codex_home: &Path, model: &str) -> std::io::Result<()> {
+    ensure_litellm_baseline(codex_home)?;
+    let config_path = codex_home.join(CONFIG_TOML_FILE);
+    let mut doc = load_config_document(&config_path)?;
+    doc["model"] = value(model);
+    save_config_document(&config_path, &doc)?;
+    Ok(())
+}
+
+pub fn clear_litellm_credentials(codex_home: &Path) -> std::io::Result<bool> {
+    let config_path = codex_home.join(CONFIG_TOML_FILE);
+    let mut doc = load_config_document(&config_path)?;
+    let provider_exists = match ensure_litellm_provider_table_mut(&mut doc) {
+        Some(table) => table,
+        None => return Ok(false),
+    };
+
+    let mut changed = false;
+    if provider_exists.remove("base_url").is_some() {
+        changed = true;
+    }
+    if provider_exists
+        .remove("experimental_bearer_token")
+        .is_some()
+    {
+        changed = true;
+    }
+
+    if changed {
+        save_config_document(&config_path, &doc)?;
+    }
+
+    Ok(changed)
+}
+
+fn migrate_legacy_litellm_provider(doc: &mut DocumentMut) -> bool {
+    let mut changed = false;
+
+    if doc
+        .as_table()
+        .get("model_provider")
+        .and_then(TomlItem::as_str)
+        == Some(LEGACY_LITELLM_PROVIDER_ID)
+    {
+        doc["model_provider"] = value(LITELLM_PROVIDER_ID);
+        changed = true;
+    }
+
+    if let Some(providers) = doc
+        .as_table_mut()
+        .get_mut("model_providers")
+        .and_then(TomlItem::as_table_mut)
+    {
+        if providers.contains_key(LEGACY_LITELLM_PROVIDER_ID) {
+            if !providers.contains_key(LITELLM_PROVIDER_ID) {
+                if let Some(existing) = providers.get(LEGACY_LITELLM_PROVIDER_ID).cloned() {
+                    providers.insert(LITELLM_PROVIDER_ID, existing);
+                }
+            }
+            providers.remove(LEGACY_LITELLM_PROVIDER_ID);
+            changed = true;
+        }
+    }
+
+    changed
+}
+
+fn ensure_litellm_provider_table(doc: &mut DocumentMut) -> &mut TomlTable {
+    let providers_item = doc
+        .as_table_mut()
+        .entry("model_providers")
+        .or_insert(TomlItem::Table(TomlTable::new()));
+    let providers_table = providers_item.as_table_mut().expect("table");
+    providers_table.set_implicit(false);
+    let provider_item = providers_table
+        .entry(LITELLM_PROVIDER_ID)
+        .or_insert(TomlItem::Table(TomlTable::new()));
+    let provider_table = provider_item.as_table_mut().expect("table");
+    provider_table.set_implicit(false);
+    provider_table
+}
+
+fn ensure_litellm_provider_table_ref(doc: &DocumentMut) -> Option<&TomlTable> {
+    doc.as_table()
+        .get("model_providers")
+        .and_then(TomlItem::as_table)
+        .and_then(|providers| providers.get(LITELLM_PROVIDER_ID))
+        .and_then(TomlItem::as_table)
+}
+
+fn ensure_litellm_provider_table_mut(doc: &mut DocumentMut) -> Option<&mut TomlTable> {
+    doc.as_table_mut()
+        .get_mut("model_providers")
+        .and_then(TomlItem::as_table_mut)
+        .and_then(|providers| providers.get_mut(LITELLM_PROVIDER_ID))
+        .and_then(TomlItem::as_table_mut)
+}
+
+fn load_config_document(path: &Path) -> std::io::Result<DocumentMut> {
+    match std::fs::read_to_string(path) {
+        Ok(contents) => match contents.parse::<DocumentMut>() {
+            Ok(doc) => Ok(doc),
+            Err(err) => {
+                tracing::warn!("Failed to parse {}: {err}", path.display());
+                Ok(DocumentMut::new())
+            }
+        },
+        Err(err) if err.kind() == ErrorKind::NotFound => Ok(DocumentMut::new()),
+        Err(err) => Err(err),
+    }
+}
+
+fn save_config_document(path: &Path, doc: &DocumentMut) -> std::io::Result<()> {
+    std::fs::write(path, doc.to_string())
+}
 
 /// Application configuration loaded from disk and merged with overrides.
 #[derive(Debug, Clone, PartialEq)]
@@ -98,6 +571,9 @@ pub struct Config {
     /// Info needed to make an API request to the model.
     pub model_provider: ModelProviderInfo,
 
+    /// Returns true when the bundled LiteLLM provider still needs a base URL.
+    pub litellm_setup_required: bool,
+
     /// Approval policy for executing commands.
     pub approval_policy: AskForApproval,
 
@@ -203,6 +679,9 @@ pub struct Config {
     /// output will be hyperlinked using the specified URI scheme.
     pub file_opener: UriBasedFileOpener,
 
+    /// Telemetry output configuration (log directories and file layout).
+    pub telemetry: TelemetryConfig,
+
     /// Path to the `codex-linux-sandbox` executable. This must be set if
     /// [`crate::exec::SandboxType::LinuxSeccomp`] is used. Note that this
     /// cannot be set in the config file: it must be set in code via
@@ -281,6 +760,7 @@ impl Config {
         overrides: ConfigOverrides,
     ) -> std::io::Result<Self> {
         let codex_home = find_codex_home()?;
+        ensure_litellm_baseline(&codex_home)?;
 
         let root_value = load_resolved_config(
             &codex_home,
@@ -519,6 +999,9 @@ pub struct ConfigToml {
     /// Size of the context window for the model, in tokens.
     pub model_context_window: Option<i64>,
 
+    /// Alias for model_context_window to make LiteLLM configuration explicit.
+    pub context_length: Option<i64>,
+
     /// Maximum number of output tokens.
     pub model_max_output_tokens: Option<i64>,
 
@@ -630,6 +1113,9 @@ pub struct ConfigToml {
 
     pub projects: Option<HashMap<String, ProjectConfig>>,
 
+    #[serde(default)]
+    pub telemetry: TelemetryToml,
+
     /// Nested tools section for feature toggles
     pub tools: Option<ToolsToml>,
 
@@ -845,6 +1331,7 @@ pub struct ConfigOverrides {
     pub experimental_sandbox_command_assessment: Option<bool>,
     /// Additional directories that should be treated as writable roots for this session.
     pub additional_writable_roots: Vec<PathBuf>,
+    pub telemetry_enabled: Option<bool>,
 }
 
 impl Config {
@@ -875,6 +1362,7 @@ impl Config {
             tools_web_search_request: override_tools_web_search_request,
             experimental_sandbox_command_assessment: sandbox_command_assessment_override,
             additional_writable_roots,
+            telemetry_enabled,
         } = overrides;
 
         let active_profile_name = config_profile_key
@@ -976,13 +1464,21 @@ impl Config {
         let mut model_providers = built_in_model_providers();
         // Merge user-defined providers into the built-in list.
         for (key, provider) in cfg.model_providers.into_iter() {
-            model_providers.entry(key).or_insert(provider);
+            model_providers.insert(key, provider);
         }
 
-        let model_provider_id = model_provider
+        // Determine baseline provider
+        let mut resolved_provider_id = model_provider
             .or(config_profile.model_provider)
-            .or(cfg.model_provider)
-            .unwrap_or_else(|| "openai".to_string());
+            .or(cfg.model_provider);
+        if resolved_provider_id.is_none() {
+            if cfg.model.is_some() || config_profile.model.is_some() {
+                resolved_provider_id = Some("openai".to_string());
+            } else {
+                resolved_provider_id = Some(LITELLM_PROVIDER_ID.to_string());
+            }
+        }
+        let model_provider_id = resolved_provider_id.unwrap();
         let model_provider = model_providers
             .get(&model_provider_id)
             .ok_or_else(|| {
@@ -993,6 +1489,20 @@ impl Config {
             })?
             .clone();
 
+        let base_url_missing = model_provider
+            .base_url
+            .as_ref()
+            .map(|value| value.trim().is_empty())
+            .unwrap_or(true);
+        let token_missing = model_provider
+            .experimental_bearer_token
+            .as_ref()
+            .map(|value| value.trim().is_empty())
+            .unwrap_or(true);
+        let litellm_setup_required = (model_provider_id == "litellm"
+            || model_provider_id == LITELLM_PROVIDER_ID)
+            && (base_url_missing || token_missing);
+
         let shell_environment_policy = cfg.shell_environment_policy.into();
 
         let history = cfg.history.unwrap_or_default();
@@ -1020,7 +1530,16 @@ impl Config {
         let model = model
             .or(config_profile.model)
             .or(cfg.model)
-            .unwrap_or_else(default_model);
+            .unwrap_or_else(|| {
+                if matches!(
+                    model_provider_id.as_str(),
+                    LITELLM_PROVIDER_ID | LEGACY_LITELLM_PROVIDER_ID
+                ) {
+                    String::new()
+                } else {
+                    default_model()
+                }
+            });
 
         let mut model_family =
             find_family_for_model(&model).unwrap_or_else(|| derive_default_model_family(&model));
@@ -1033,8 +1552,8 @@ impl Config {
         }
 
         let openai_model_info = get_model_info(&model_family);
-        let model_context_window = cfg
-            .model_context_window
+        let configured_context_window = cfg.context_length.or(cfg.model_context_window);
+        let model_context_window = configured_context_window
             .or_else(|| openai_model_info.as_ref().map(|info| info.context_window));
         let model_max_output_tokens = cfg.model_max_output_tokens.or_else(|| {
             openai_model_info
@@ -1082,6 +1601,78 @@ impl Config {
         )?;
         let compact_prompt = compact_prompt.or(file_compact_prompt);
 
+        let telemetry_cfg = cfg.telemetry.clone();
+        let telemetry_enabled = telemetry_enabled.or(telemetry_cfg.enabled).unwrap_or(true);
+        let session_usage_enabled = telemetry_cfg.session_usage.unwrap_or(true);
+        let tui_log_enabled = telemetry_cfg.tui.unwrap_or(false);
+        let debug_log_enabled = telemetry_cfg.debug.unwrap_or(cfg!(debug_assertions));
+        let telemetry_dir = match telemetry_cfg.dir {
+            Some(path) if path.is_absolute() => path,
+            Some(path) => codex_home.join(path),
+            None => codex_home.join(DEFAULT_TELEMETRY_LOG_DIR),
+        };
+        let telemetry_max_total_bytes = telemetry_cfg
+            .max_total_bytes
+            .or(Some(DEFAULT_TELEMETRY_MAX_TOTAL_BYTES));
+        let mut telemetry_logs: HashMap<String, TelemetryLogConfig> = HashMap::new();
+        for (name, default_file, timestamped) in [
+            ("tui", Some(DEFAULT_TUI_LOG), false),
+            ("session", Some(DEFAULT_SESSION_LOG), false),
+            ("debug", None, true),
+        ] {
+            let entry: Option<&TelemetryLogToml> = telemetry_cfg.logs.get(name);
+            let base_enabled = match name {
+                "tui" => tui_log_enabled,
+                "session" => session_usage_enabled,
+                "debug" => debug_log_enabled,
+                _ => true,
+            };
+            let entry_enabled = entry.and_then(|log| log.enabled).unwrap_or(true);
+            let enabled = telemetry_enabled && base_enabled && entry_enabled;
+            let path = if let Some(file) = entry.and_then(|log| log.file.clone()) {
+                if file.is_absolute() {
+                    file
+                } else {
+                    telemetry_dir.join(file)
+                }
+            } else if timestamped {
+                let filename = format!("{}.log", Utc::now().format("%Y%m%d-%H%M%S"));
+                telemetry_dir.join(filename)
+            } else if let Some(default) = default_file {
+                telemetry_dir.join(default)
+            } else {
+                telemetry_dir.clone()
+            };
+            telemetry_logs.insert(name.to_string(), TelemetryLogConfig { enabled, path });
+        }
+        for (name, details) in telemetry_cfg.logs {
+            if telemetry_logs.contains_key(&name) {
+                continue;
+            }
+            let enabled = telemetry_enabled && details.enabled.unwrap_or(true);
+            if let Some(file) = details.file {
+                let path = if file.is_absolute() {
+                    file
+                } else {
+                    telemetry_dir.join(file)
+                };
+                telemetry_logs.insert(name, TelemetryLogConfig { enabled, path });
+            }
+        }
+        let telemetry = TelemetryConfig {
+            enabled: telemetry_enabled,
+            dir: telemetry_dir.clone(),
+            logs: telemetry_logs,
+            max_total_bytes: telemetry_max_total_bytes,
+            session_usage_enabled,
+            tui_log_enabled,
+            debug_log_enabled,
+        };
+
+        if telemetry.enabled {
+            prune_telemetry_logs(&telemetry);
+        }
+
         // Default review model when not set in config; allow CLI override to take precedence.
         let review_model = override_review_model
             .or(cfg.review_model)
@@ -1096,6 +1687,7 @@ impl Config {
             model_auto_compact_token_limit,
             model_provider_id,
             model_provider,
+            litellm_setup_required,
             cwd: resolved_cwd,
             approval_policy,
             sandbox_policy,
@@ -1132,6 +1724,7 @@ impl Config {
             codex_home,
             history,
             file_opener: cfg.file_opener.unwrap_or(UriBasedFileOpener::VsCode),
+            telemetry,
             codex_linux_sandbox_exe,
 
             hide_agent_reasoning: cfg.hide_agent_reasoning.unwrap_or(false),
@@ -1274,9 +1867,83 @@ pub fn find_codex_home() -> std::io::Result<PathBuf> {
 /// Returns the path to the folder where Codex logs are stored. Does not verify
 /// that the directory exists.
 pub fn log_dir(cfg: &Config) -> std::io::Result<PathBuf> {
-    let mut p = cfg.codex_home.clone();
-    p.push("log");
-    Ok(p)
+    Ok(cfg.telemetry.dir.clone())
+}
+
+pub fn telemetry_log_config<'a>(cfg: &'a Config, name: &str) -> Option<&'a TelemetryLogConfig> {
+    cfg.telemetry.log_config(name)
+}
+
+fn prune_telemetry_logs(telemetry: &TelemetryConfig) {
+    if !telemetry.enabled {
+        return;
+    }
+    let Some(limit) = telemetry.max_total_bytes else {
+        return;
+    };
+    if limit == 0 {
+        return;
+    }
+
+    let dir_entries = match std::fs::read_dir(&telemetry.dir) {
+        Ok(entries) => entries,
+        Err(err) => {
+            tracing::debug!(
+                "telemetry: unable to read log directory {}: {err}",
+                telemetry.dir.display()
+            );
+            return;
+        }
+    };
+
+    let mut files: Vec<(PathBuf, u64, std::time::SystemTime)> = Vec::new();
+    for entry in dir_entries.flatten() {
+        let path = entry.path();
+        if let Ok(meta) = entry.metadata()
+            && meta.is_file()
+        {
+            let modified = meta.modified().unwrap_or(std::time::SystemTime::UNIX_EPOCH);
+            files.push((path, meta.len(), modified));
+        }
+    }
+
+    if files.is_empty() {
+        return;
+    }
+
+    let mut total_size: u64 = files.iter().map(|(_, size, _)| *size).sum();
+    if total_size <= limit {
+        return;
+    }
+
+    files.sort_by_key(|(_, _, modified)| *modified);
+
+    let active_paths: std::collections::HashSet<PathBuf> = telemetry
+        .logs
+        .values()
+        .map(|log| log.path.clone())
+        .collect();
+
+    for (path, size, _) in files {
+        if total_size <= limit {
+            break;
+        }
+        if active_paths.contains(&path) {
+            continue;
+        }
+        match std::fs::remove_file(&path) {
+            Ok(()) => {
+                total_size = total_size.saturating_sub(size);
+                tracing::debug!("telemetry: pruned {}", path.display());
+            }
+            Err(err) => {
+                tracing::warn!(
+                    "telemetry: failed to remove old log {}: {err}",
+                    path.display()
+                );
+            }
+        }
+    }
 }
 
 #[cfg(test)]
@@ -1292,6 +1959,7 @@ mod tests {
     use super::*;
     use pretty_assertions::assert_eq;
 
+    use std::collections::HashMap;
     use std::time::Duration;
     use tempfile::TempDir;
 
@@ -2870,6 +3538,7 @@ model_verbosity = "high"
                 model_auto_compact_token_limit: Some(180_000),
                 model_provider_id: "openai".to_string(),
                 model_provider: fixture.openai_provider.clone(),
+                litellm_setup_required: false,
                 approval_policy: AskForApproval::Never,
                 sandbox_policy: SandboxPolicy::new_read_only_policy(),
                 did_user_set_custom_approval_policy_or_sandbox_mode: true,
@@ -2898,6 +3567,7 @@ model_verbosity = "high"
                 developer_instructions: None,
                 compact_prompt: None,
                 forced_chatgpt_workspace_id: None,
+                telemetry: o3_profile_config.telemetry.clone(),
                 forced_login_method: None,
                 include_apply_patch_tool: false,
                 tools_web_search_request: false,
@@ -2942,6 +3612,7 @@ model_verbosity = "high"
             model_auto_compact_token_limit: Some(14_746),
             model_provider_id: "openai-chat-completions".to_string(),
             model_provider: fixture.openai_chat_completions_provider.clone(),
+            litellm_setup_required: false,
             approval_policy: AskForApproval::UnlessTrusted,
             sandbox_policy: SandboxPolicy::new_read_only_policy(),
             did_user_set_custom_approval_policy_or_sandbox_mode: true,
@@ -2970,6 +3641,7 @@ model_verbosity = "high"
             developer_instructions: None,
             compact_prompt: None,
             forced_chatgpt_workspace_id: None,
+            telemetry: gpt3_profile_config.telemetry.clone(),
             forced_login_method: None,
             include_apply_patch_tool: false,
             tools_web_search_request: false,
@@ -3029,6 +3701,7 @@ model_verbosity = "high"
             model_auto_compact_token_limit: Some(180_000),
             model_provider_id: "openai".to_string(),
             model_provider: fixture.openai_provider.clone(),
+            litellm_setup_required: false,
             approval_policy: AskForApproval::OnFailure,
             sandbox_policy: SandboxPolicy::new_read_only_policy(),
             did_user_set_custom_approval_policy_or_sandbox_mode: true,
@@ -3057,6 +3730,7 @@ model_verbosity = "high"
             developer_instructions: None,
             compact_prompt: None,
             forced_chatgpt_workspace_id: None,
+            telemetry: zdr_profile_config.telemetry.clone(),
             forced_login_method: None,
             include_apply_patch_tool: false,
             tools_web_search_request: false,
@@ -3102,6 +3776,7 @@ model_verbosity = "high"
             model_auto_compact_token_limit: Some(244_800),
             model_provider_id: "openai".to_string(),
             model_provider: fixture.openai_provider.clone(),
+            litellm_setup_required: false,
             approval_policy: AskForApproval::OnFailure,
             sandbox_policy: SandboxPolicy::new_read_only_policy(),
             did_user_set_custom_approval_policy_or_sandbox_mode: true,
@@ -3130,6 +3805,7 @@ model_verbosity = "high"
             developer_instructions: None,
             compact_prompt: None,
             forced_chatgpt_workspace_id: None,
+            telemetry: gpt5_profile_config.telemetry.clone(),
             forced_login_method: None,
             include_apply_patch_tool: false,
             tools_web_search_request: false,
@@ -3272,6 +3948,11 @@ mod notifications_tests {
     use crate::config::types::Notifications;
     use assert_matches::assert_matches;
     use serde::Deserialize;
+    use std::collections::HashMap;
+    use std::time::Duration;
+    use tempfile::TempDir;
+
+    use super::{TelemetryConfig, TelemetryLogConfig, prune_telemetry_logs};
 
     #[derive(Deserialize, Debug, PartialEq)]
     struct TuiTomlTest {
@@ -3306,4 +3987,60 @@ mod notifications_tests {
             Notifications::Custom(ref v) if v == &vec!["foo".to_string()]
         );
     }
+
+    #[test]
+    fn prune_telemetry_respects_size_limit() {
+        let tmp = TempDir::new().expect("tempdir");
+        let dir = tmp.path();
+
+        let old_a = dir.join("20240101.log");
+        let old_b = dir.join("20240201.log");
+        let active = dir.join("codex-tui.log");
+
+        std::fs::write(&old_a, vec![0u8; 30]).expect("write old_a");
+        std::thread::sleep(Duration::from_millis(10));
+        std::fs::write(&old_b, vec![0u8; 25]).expect("write old_b");
+        std::thread::sleep(Duration::from_millis(10));
+        std::fs::write(&active, vec![0u8; 40]).expect("write active");
+
+        let telemetry = TelemetryConfig {
+            enabled: true,
+            dir: dir.to_path_buf(),
+            logs: HashMap::from([(
+                "tui".to_string(),
+                TelemetryLogConfig {
+                    enabled: true,
+                    path: active.clone(),
+                },
+            )]),
+            max_total_bytes: Some(70),
+            session_usage_enabled: true,
+            tui_log_enabled: true,
+            debug_log_enabled: cfg!(debug_assertions),
+        };
+
+        prune_telemetry_logs(&telemetry);
+
+        assert!(
+            active.exists(),
+            "active log should never be removed by pruning"
+        );
+        let remaining_old = [old_a.clone(), old_b.clone()]
+            .into_iter()
+            .filter(|p| p.exists())
+            .count();
+        assert_eq!(
+            remaining_old, 1,
+            "only one of the old logs should remain after pruning"
+        );
+        let total: u64 = [old_a, old_b, active]
+            .iter()
+            .filter_map(|p| std::fs::metadata(p).ok())
+            .map(|m| m.len())
+            .sum();
+        assert!(
+            total <= 70,
+            "total telemetry size should respect the configured limit"
+        );
+    }
 }
diff --git a/codex-rs/core/src/config/types.rs b/codex-rs/core/src/config/types.rs
index fd93df8f9..d38bf5bc5 100644
--- a/codex-rs/core/src/config/types.rs
+++ b/codex-rs/core/src/config/types.rs
@@ -161,6 +161,25 @@ const fn default_enabled() -> bool {
     true
 }
 
+#[derive(Serialize, Deserialize, Debug, Clone, PartialEq, Default)]
+#[serde(default, deny_unknown_fields)]
+pub struct TelemetryLogToml {
+    pub enabled: Option<bool>,
+    pub file: Option<PathBuf>,
+}
+
+#[derive(Serialize, Deserialize, Debug, Clone, PartialEq, Default)]
+#[serde(default, deny_unknown_fields)]
+pub struct TelemetryToml {
+    pub dir: Option<PathBuf>,
+    pub logs: HashMap<String, TelemetryLogToml>,
+    pub enabled: Option<bool>,
+    pub max_total_bytes: Option<u64>,
+    pub session_usage: Option<bool>,
+    pub tui: Option<bool>,
+    pub debug: Option<bool>,
+}
+
 #[derive(Serialize, Deserialize, Debug, Clone, PartialEq)]
 #[serde(untagged, deny_unknown_fields, rename_all = "snake_case")]
 pub enum McpServerTransportConfig {
diff --git a/codex-rs/core/src/error.rs b/codex-rs/core/src/error.rs
index 646832754..cbc26c5dc 100644
--- a/codex-rs/core/src/error.rs
+++ b/codex-rs/core/src/error.rs
@@ -172,12 +172,35 @@ impl From<CancelErr> for CodexErr {
 
 #[derive(Debug)]
 pub struct ConnectionFailedError {
-    pub source: reqwest::Error,
+    pub message: String,
+    pub source: Option<reqwest::Error>,
+}
+
+impl ConnectionFailedError {
+    pub fn new(message: impl Into<String>) -> Self {
+        Self {
+            message: message.into(),
+            source: None,
+        }
+    }
+}
+
+impl From<reqwest::Error> for ConnectionFailedError {
+    fn from(err: reqwest::Error) -> Self {
+        let message = err.to_string();
+        Self {
+            message,
+            source: Some(err),
+        }
+    }
 }
 
 impl std::fmt::Display for ConnectionFailedError {
     fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
-        write!(f, "Connection failed: {}", self.source)
+        match &self.source {
+            Some(source) => write!(f, "Connection failed: {}", source),
+            None => write!(f, "Connection failed: {}", self.message),
+        }
     }
 }
 
diff --git a/codex-rs/core/src/lib.rs b/codex-rs/core/src/lib.rs
index 413cc76bf..4c5f2bf54 100644
--- a/codex-rs/core/src/lib.rs
+++ b/codex-rs/core/src/lib.rs
@@ -6,7 +6,9 @@
 #![deny(clippy::print_stdout, clippy::print_stderr)]
 
 mod apply_patch;
+pub mod assistant_message;
 pub mod auth;
+pub mod background_event;
 pub mod bash;
 mod chat_completions;
 mod client;
diff --git a/codex-rs/core/src/model_family.rs b/codex-rs/core/src/model_family.rs
index a04443611..70cbf4239 100644
--- a/codex-rs/core/src/model_family.rs
+++ b/codex-rs/core/src/model_family.rs
@@ -57,6 +57,15 @@ pub struct ModelFamily {
 
     /// If the model family supports setting the verbosity level when using Responses API.
     pub support_verbosity: bool,
+
+    /// When true, force Chat Completions requests for this model to run in buffered
+    /// (non-streaming) mode so we can synthesize SSE events from the final payload.
+    pub prefer_non_stream: bool,
+
+    /// Whether we should aggressively issue follow-up prompts after tool calls
+    /// even when the model emits a short assistant message. Primarily for
+    /// non-agentic LiteLLM bridges that tend to stop early.
+    pub force_followups_after_tools: bool,
 }
 
 macro_rules! model_family {
@@ -77,6 +86,8 @@ macro_rules! model_family {
             experimental_supported_tools: Vec::new(),
             effective_context_window_percent: 95,
             support_verbosity: false,
+            prefer_non_stream: false,
+            force_followups_after_tools: false,
         };
         // apply overrides
         $(
@@ -113,8 +124,16 @@ pub fn find_family_for_model(slug: &str) -> Option<ModelFamily> {
             slug, "gpt-4.1",
             needs_special_apply_patch_instructions: true,
         )
-    } else if slug.starts_with("gpt-oss") || slug.starts_with("openai/gpt-oss") {
-        model_family!(slug, "gpt-oss", apply_patch_tool_type: Some(ApplyPatchToolType::Function))
+    } else if slug.starts_with("gpt-oss")
+        || slug.starts_with("openai/gpt-oss")
+        || slug.starts_with("vercel/gpt-oss")
+    {
+        model_family!(
+            slug, "gpt-oss",
+            apply_patch_tool_type: Some(ApplyPatchToolType::Function),
+            prefer_non_stream: true,
+            force_followups_after_tools: true,
+        )
     } else if slug.starts_with("gpt-4o") {
         model_family!(slug, "gpt-4o", needs_special_apply_patch_instructions: true)
     } else if slug.starts_with("gpt-3.5") {
@@ -188,5 +207,7 @@ pub fn derive_default_model_family(model: &str) -> ModelFamily {
         experimental_supported_tools: Vec::new(),
         effective_context_window_percent: 95,
         support_verbosity: false,
+        prefer_non_stream: false,
+        force_followups_after_tools: false,
     }
 }
diff --git a/codex-rs/core/src/model_provider_info.rs b/codex-rs/core/src/model_provider_info.rs
index 8dc252aa7..8ba901e71 100644
--- a/codex-rs/core/src/model_provider_info.rs
+++ b/codex-rs/core/src/model_provider_info.rs
@@ -261,15 +261,14 @@ impl ModelProviderInfo {
 const DEFAULT_OLLAMA_PORT: u32 = 11434;
 
 pub const BUILT_IN_OSS_MODEL_PROVIDER_ID: &str = "oss";
+pub const BUILT_IN_LITELLM_PROVIDER_ID: &str = "litellm";
 
 /// Built-in default provider list.
 pub fn built_in_model_providers() -> HashMap<String, ModelProviderInfo> {
     use ModelProviderInfo as P;
 
-    // We do not want to be in the business of adjucating which third-party
-    // providers are bundled with Codex CLI, so we only include the OpenAI and
-    // open source ("oss") providers by default. Users are encouraged to add to
-    // `model_providers` in config.toml to add their own providers.
+    // We include OpenAI, the upstream OSS provider, and a LiteLLM entry that
+    // our patches populate at runtime.
     [
         (
             "openai",
@@ -312,6 +311,7 @@ pub fn built_in_model_providers() -> HashMap<String, ModelProviderInfo> {
             },
         ),
         (BUILT_IN_OSS_MODEL_PROVIDER_ID, create_oss_provider()),
+        (BUILT_IN_LITELLM_PROVIDER_ID, create_litellm_provider()),
     ]
     .into_iter()
     .map(|(k, v)| (k.to_string(), v))
@@ -357,6 +357,24 @@ pub fn create_oss_provider_with_base_url(base_url: &str) -> ModelProviderInfo {
     }
 }
 
+pub fn create_litellm_provider() -> ModelProviderInfo {
+    ModelProviderInfo {
+        name: "LiteLLM Direct".into(),
+        base_url: None,
+        env_key: None,
+        env_key_instructions: None,
+        experimental_bearer_token: None,
+        wire_api: WireApi::Chat,
+        query_params: None,
+        http_headers: None,
+        env_http_headers: None,
+        request_max_retries: None,
+        stream_max_retries: None,
+        stream_idle_timeout_ms: None,
+        requires_openai_auth: false,
+    }
+}
+
 fn matches_azure_responses_base_url(base_url: &str) -> bool {
     let base = base_url.to_ascii_lowercase();
     const AZURE_MARKERS: [&str; 5] = [
diff --git a/codex-rs/core/src/response_processing.rs b/codex-rs/core/src/response_processing.rs
index d1767b74c..54c1d914e 100644
--- a/codex-rs/core/src/response_processing.rs
+++ b/codex-rs/core/src/response_processing.rs
@@ -1,8 +1,18 @@
+use crate::assistant_message::format_noise_reasoning;
+use crate::assistant_message::is_noise_assistant_message;
 use crate::codex::Session;
 use crate::codex::TurnContext;
+use codex_protocol::models::ContentItem;
 use codex_protocol::models::FunctionCallOutputPayload;
+use codex_protocol::models::ReasoningItemContent;
+use codex_protocol::models::ReasoningItemReasoningSummary;
 use codex_protocol::models::ResponseInputItem;
 use codex_protocol::models::ResponseItem;
+use codex_protocol::plan_tool::PlanItemArg;
+use codex_protocol::plan_tool::StepStatus;
+use codex_protocol::plan_tool::UpdatePlanArgs;
+use codex_protocol::protocol::AgentReasoningRawContentEvent;
+use codex_protocol::protocol::EventMsg;
 use tracing::warn;
 
 /// Process streamed `ResponseItem`s from the model into the pair of:
@@ -15,11 +25,33 @@ pub(crate) async fn process_items(
 ) -> (Vec<ResponseInputItem>, Vec<ResponseItem>) {
     let mut items_to_record_in_conversation_history = Vec::<ResponseItem>::new();
     let mut responses = Vec::<ResponseInputItem>::new();
+    let plan_tool_invoked = processed_items.iter().any(|processed| {
+        matches!(
+            processed.item,
+            ResponseItem::FunctionCall { ref name, .. } if name == "update_plan"
+        )
+    });
+
     for processed_response_item in processed_items {
         let crate::codex::ProcessedResponseItem { item, response } = processed_response_item;
         match (&item, &response) {
-            (ResponseItem::Message { role, .. }, None) if role == "assistant" => {
-                // If the model returned a message, we need to record it.
+            (ResponseItem::Message { role, content, .. }, None) if role == "assistant" => {
+                let assistant_text = extract_assistant_text(content);
+                let emitted_plan = !plan_tool_invoked
+                    && maybe_emit_plan_update(sess, turn_context, &assistant_text).await;
+                if emitted_plan {
+                    continue;
+                }
+                if is_noise_assistant_message(&assistant_text) {
+                    if let Some(formatted) = format_noise_reasoning(&assistant_text) {
+                        let event =
+                            EventMsg::AgentReasoningRawContent(AgentReasoningRawContentEvent {
+                                text: formatted,
+                            });
+                        sess.send_event(turn_context, event).await;
+                    }
+                    continue;
+                }
                 items_to_record_in_conversation_history.push(item);
             }
             (
@@ -95,6 +127,8 @@ pub(crate) async fn process_items(
         }
     }
 
+    ensure_assistant_summary(&mut items_to_record_in_conversation_history);
+
     // Only attempt to take the lock if there is something to record.
     if !items_to_record_in_conversation_history.is_empty() {
         sess.record_conversation_items(turn_context, &items_to_record_in_conversation_history)
@@ -102,3 +136,191 @@ pub(crate) async fn process_items(
     }
     (responses, items_to_record_in_conversation_history)
 }
+
+fn ensure_assistant_summary(items: &mut Vec<ResponseItem>) {
+    let has_assistant = items.iter().any(|item| {
+        matches!(
+            item,
+            ResponseItem::Message { role, .. } if role == "assistant"
+        )
+    });
+    if has_assistant {
+        return;
+    }
+
+    if let Some(text) = extract_reasoning_text(items) {
+        if !is_noise_assistant_message(&text) {
+            items.push(ResponseItem::Message {
+                id: None,
+                role: "assistant".to_string(),
+                content: vec![ContentItem::OutputText { text }],
+            });
+        }
+    }
+}
+
+async fn maybe_emit_plan_update(sess: &Session, turn_context: &TurnContext, text: &str) -> bool {
+    if text.is_empty() {
+        return false;
+    }
+    if let Some(plan) = parse_plan_from_text(text) {
+        sess.send_event(turn_context, EventMsg::PlanUpdate(plan))
+            .await;
+        return true;
+    }
+    false
+}
+
+fn extract_assistant_text(content: &[ContentItem]) -> String {
+    let mut text = String::new();
+    for item in content {
+        if let ContentItem::OutputText { text: t } = item {
+            if !text.is_empty() {
+                text.push('\n');
+            }
+            text.push_str(t);
+        }
+    }
+    text.trim().to_string()
+}
+
+fn parse_plan_from_text(text: &str) -> Option<UpdatePlanArgs> {
+    let plan_idx = text.find("Plan:")?;
+    let (before, after) = text.split_at(plan_idx);
+    let plan_body = after.trim_start_matches("Plan:").trim();
+    let mut steps: Vec<String> = Vec::new();
+    let mut collecting = false;
+    for line in plan_body.lines() {
+        let normalized = normalize_plan_line(line);
+        match normalized {
+            Some(step) => {
+                collecting = true;
+                steps.push(step);
+            }
+            None if collecting => break,
+            None => continue,
+        }
+    }
+    if steps.is_empty() {
+        return None;
+    }
+    let plan = steps
+        .into_iter()
+        .map(|step| PlanItemArg {
+            step,
+            status: StepStatus::Pending,
+        })
+        .collect();
+    let explanation = before.trim();
+    Some(UpdatePlanArgs {
+        explanation: if explanation.is_empty() {
+            None
+        } else {
+            Some(explanation.to_string())
+        },
+        plan,
+    })
+}
+
+fn normalize_plan_line(line: &str) -> Option<String> {
+    let trimmed = line.trim();
+    if trimmed.is_empty() {
+        return None;
+    }
+    if let Some(rest) = trimmed
+        .strip_prefix('-')
+        .or_else(|| trimmed.strip_prefix('*'))
+        .or_else(|| trimmed.strip_prefix('•'))
+    {
+        return non_empty_step(rest);
+    }
+
+    let mut chars = trimmed.chars();
+    if chars.next().map(|c| c.is_ascii_digit()).unwrap_or(false) {
+        let remainder = trimmed
+            .trim_start_matches(|c: char| c.is_ascii_digit())
+            .trim_start_matches(|c| matches!(c, '.' | ')' | ':' | '-'));
+        return non_empty_step(remainder);
+    }
+    None
+}
+
+fn non_empty_step(input: &str) -> Option<String> {
+    let cleaned = input.trim();
+    if cleaned.is_empty() {
+        None
+    } else {
+        Some(cleaned.to_string())
+    }
+}
+
+fn extract_reasoning_text(items: &[ResponseItem]) -> Option<String> {
+    let mut reasoning_blocks: Vec<(
+        Vec<ReasoningItemReasoningSummary>,
+        Option<Vec<ReasoningItemContent>>,
+    )> = items
+        .iter()
+        .filter_map(|item| {
+            if let ResponseItem::Reasoning {
+                summary, content, ..
+            } = item
+            {
+                Some((summary.clone(), content.clone()))
+            } else {
+                None
+            }
+        })
+        .collect();
+
+    reasoning_blocks.reverse();
+
+    reasoning_blocks.into_iter().find_map(|(summary, content)| {
+        reasoning_summary_text(&summary).or_else(|| {
+            content
+                .as_ref()
+                .and_then(|entries| reasoning_content_text(entries))
+        })
+    })
+}
+
+fn reasoning_summary_text(entries: &[ReasoningItemReasoningSummary]) -> Option<String> {
+    let buffer = entries
+        .iter()
+        .filter_map(|entry| match entry {
+            ReasoningItemReasoningSummary::SummaryText { text } => {
+                let trimmed = text.trim();
+                if trimmed.is_empty() {
+                    None
+                } else {
+                    Some(trimmed.to_string())
+                }
+            }
+        })
+        .collect::<Vec<_>>();
+    if buffer.is_empty() {
+        None
+    } else {
+        Some(buffer.join("\n"))
+    }
+}
+
+fn reasoning_content_text(entries: &[ReasoningItemContent]) -> Option<String> {
+    let buffer = entries
+        .iter()
+        .filter_map(|entry| match entry {
+            ReasoningItemContent::ReasoningText { text } | ReasoningItemContent::Text { text } => {
+                let trimmed = text.trim();
+                if trimmed.is_empty() {
+                    None
+                } else {
+                    Some(trimmed.to_string())
+                }
+            }
+        })
+        .collect::<Vec<_>>();
+    if buffer.is_empty() {
+        None
+    } else {
+        Some(buffer.join("\n"))
+    }
+}
diff --git a/codex-rs/core/src/rollout/policy.rs b/codex-rs/core/src/rollout/policy.rs
index e00883264..43fd0c236 100644
--- a/codex-rs/core/src/rollout/policy.rs
+++ b/codex-rs/core/src/rollout/policy.rs
@@ -44,7 +44,8 @@ pub(crate) fn should_persist_event_msg(ev: &EventMsg) -> bool {
         | EventMsg::EnteredReviewMode(_)
         | EventMsg::ExitedReviewMode(_)
         | EventMsg::UndoCompleted(_)
-        | EventMsg::TurnAborted(_) => true,
+        | EventMsg::TurnAborted(_)
+        | EventMsg::BackgroundEvent(_) => true,
         EventMsg::Error(_)
         | EventMsg::Warning(_)
         | EventMsg::TaskStarted(_)
@@ -64,7 +65,6 @@ pub(crate) fn should_persist_event_msg(ev: &EventMsg) -> bool {
         | EventMsg::ExecCommandEnd(_)
         | EventMsg::ExecApprovalRequest(_)
         | EventMsg::ApplyPatchApprovalRequest(_)
-        | EventMsg::BackgroundEvent(_)
         | EventMsg::StreamError(_)
         | EventMsg::PatchApplyBegin(_)
         | EventMsg::PatchApplyEnd(_)
diff --git a/codex-rs/core/src/tasks/mod.rs b/codex-rs/core/src/tasks/mod.rs
index 9bda02c34..9f1f03204 100644
--- a/codex-rs/core/src/tasks/mod.rs
+++ b/codex-rs/core/src/tasks/mod.rs
@@ -129,7 +129,8 @@ impl Session {
                     )
                     .await;
                 session_ctx.clone_session().flush_rollout().await;
-                if !task_cancellation_token.is_cancelled() {
+                let completed_normally = !task_cancellation_token.is_cancelled();
+                if completed_normally || last_agent_message.is_some() {
                     // Emit completion uniformly from spawn site so all tasks share the same lifecycle.
                     let sess = session_ctx.clone_session();
                     sess.on_task_finished(ctx_for_finish, last_agent_message)
@@ -162,10 +163,11 @@ impl Session {
         last_agent_message: Option<String>,
     ) {
         let mut active = self.active_turn.lock().await;
-        if let Some(at) = active.as_mut()
-            && at.remove_task(&turn_context.sub_id)
-        {
-            *active = None;
+        if let Some(at) = active.as_mut() {
+            if at.remove_task(&turn_context.sub_id) {
+                at.clear_pending().await;
+                *active = None;
+            }
         }
         drop(active);
         let event = EventMsg::TaskComplete(TaskCompleteEvent { last_agent_message });
diff --git a/codex-rs/core/src/tools/handlers/mcp_resource.rs b/codex-rs/core/src/tools/handlers/mcp_resource.rs
index b601591ac..f084766b9 100644
--- a/codex-rs/core/src/tools/handlers/mcp_resource.rs
+++ b/codex-rs/core/src/tools/handlers/mcp_resource.rs
@@ -18,7 +18,7 @@ use mcp_types::TextContent;
 use serde::Deserialize;
 use serde::Serialize;
 use serde::de::DeserializeOwned;
-use serde_json::Value;
+use serde_json::{Value, json};
 
 use crate::codex::Session;
 use crate::codex::TurnContext;
@@ -331,6 +331,20 @@ async fn handle_list_resources(
             }
         },
         Err(err) => {
+            if let Some(server) = extract_unknown_server(&err) {
+                let duration = start.elapsed();
+                let output = missing_server_tool_output(&server);
+                emit_tool_call_end(
+                    &session,
+                    turn.as_ref(),
+                    &call_id,
+                    invocation,
+                    duration,
+                    Ok(call_tool_result_from_output(&output)),
+                )
+                .await;
+                return Ok(output);
+            }
             let duration = start.elapsed();
             let message = err.to_string();
             emit_tool_call_end(
@@ -440,6 +454,20 @@ async fn handle_list_resource_templates(
             }
         },
         Err(err) => {
+            if let Some(server) = extract_unknown_server(&err) {
+                let duration = start.elapsed();
+                let output = missing_server_tool_output(&server);
+                emit_tool_call_end(
+                    &session,
+                    turn.as_ref(),
+                    &call_id,
+                    invocation,
+                    duration,
+                    Ok(call_tool_result_from_output(&output)),
+                )
+                .await;
+                return Ok(output);
+            }
             let duration = start.elapsed();
             let message = err.to_string();
             emit_tool_call_end(
@@ -529,6 +557,20 @@ async fn handle_read_resource(
             }
         },
         Err(err) => {
+            if let Some(server) = extract_unknown_server(&err) {
+                let duration = start.elapsed();
+                let output = missing_server_tool_output(&server);
+                emit_tool_call_end(
+                    &session,
+                    turn.as_ref(),
+                    &call_id,
+                    invocation,
+                    duration,
+                    Ok(call_tool_result_from_output(&output)),
+                )
+                .await;
+                return Ok(output);
+            }
             let duration = start.elapsed();
             let message = err.to_string();
             emit_tool_call_end(
@@ -557,6 +599,41 @@ fn call_tool_result_from_content(content: &str, success: Option<bool>) -> CallTo
     }
 }
 
+fn call_tool_result_from_output(output: &ToolOutput) -> CallToolResult {
+    match output {
+        ToolOutput::Function {
+            content, success, ..
+        } => call_tool_result_from_content(content, *success),
+        ToolOutput::Mcp { .. } => call_tool_result_from_content("{}", Some(false)),
+    }
+}
+
+fn missing_server_tool_output(server: &str) -> ToolOutput {
+    ToolOutput::Function {
+        content: json!({
+            "error": format!(
+                "MCP server '{server}' is not configured for this workspace. Add it to config.toml [mcp_servers.{server}] or disable the tool call."
+            )
+        })
+        .to_string(),
+        content_items: None,
+        success: Some(false),
+    }
+}
+
+fn extract_unknown_server(err: &FunctionCallError) -> Option<String> {
+    match err {
+        FunctionCallError::RespondToModel(message) => {
+            let needle = "unknown MCP server '";
+            let start = message.find(needle)? + needle.len();
+            let rest = &message[start..];
+            let end = rest.find('\'')?;
+            Some(rest[..end].to_string())
+        }
+        _ => None,
+    }
+}
+
 async fn emit_tool_call_begin(
     session: &Arc<Session>,
     turn: &TurnContext,
diff --git a/codex-rs/core/src/tools/handlers/shell.rs b/codex-rs/core/src/tools/handlers/shell.rs
index b97242a9a..f67a6ab73 100644
--- a/codex-rs/core/src/tools/handlers/shell.rs
+++ b/codex-rs/core/src/tools/handlers/shell.rs
@@ -22,10 +22,47 @@ use crate::tools::runtimes::apply_patch::ApplyPatchRuntime;
 use crate::tools::runtimes::shell::ShellRequest;
 use crate::tools::runtimes::shell::ShellRuntime;
 use crate::tools::sandboxing::ToolCtx;
+use serde_json::Value;
 
 pub struct ShellHandler;
 
 impl ShellHandler {
+    fn parse_shell_params(arguments: &str) -> Result<ShellToolCallParams, serde_json::Error> {
+        match serde_json::from_str(arguments) {
+            Ok(params) => Ok(params),
+            Err(primary_err) => {
+                let mut value: Value = serde_json::from_str(arguments)?;
+                let mut mutated = false;
+                if let Some(obj) = value.as_object_mut() {
+                    if let Some(cmd_value) = obj.remove("cmd") {
+                        obj.insert("command".to_string(), cmd_value);
+                        mutated = true;
+                    }
+
+                    if let Some(command_value) = obj.get("command").cloned() {
+                        if let Value::String(cmd_str) = command_value {
+                            obj.insert(
+                                "command".to_string(),
+                                Value::Array(vec![
+                                    Value::String("bash".to_string()),
+                                    Value::String("-lc".to_string()),
+                                    Value::String(cmd_str),
+                                ]),
+                            );
+                            mutated = true;
+                        }
+                    }
+                }
+
+                if mutated {
+                    serde_json::from_value(value).or(Err(primary_err))
+                } else {
+                    Err(primary_err)
+                }
+            }
+        }
+    }
+
     fn to_exec_params(params: ShellToolCallParams, turn_context: &TurnContext) -> ExecParams {
         ExecParams {
             command: params.command,
@@ -64,12 +101,11 @@ impl ToolHandler for ShellHandler {
 
         match payload {
             ToolPayload::Function { arguments } => {
-                let params: ShellToolCallParams =
-                    serde_json::from_str(&arguments).map_err(|e| {
-                        FunctionCallError::RespondToModel(format!(
-                            "failed to parse function arguments: {e:?}"
-                        ))
-                    })?;
+                let params = Self::parse_shell_params(&arguments).map_err(|e| {
+                    FunctionCallError::RespondToModel(format!(
+                        "failed to parse function arguments: {e:?}"
+                    ))
+                })?;
                 let exec_params = Self::to_exec_params(params, turn.as_ref());
                 Self::run_exec_like(
                     tool_name.as_str(),
diff --git a/codex-rs/core/src/tools/spec.rs b/codex-rs/core/src/tools/spec.rs
index eba9fd517..f99a05806 100644
--- a/codex-rs/core/src/tools/spec.rs
+++ b/codex-rs/core/src/tools/spec.rs
@@ -663,6 +663,7 @@ fn create_read_mcp_resource_tool() -> ToolSpec {
 /// TODO(dylan): deprecate once we get rid of json tool
 #[derive(Serialize, Deserialize)]
 pub(crate) struct ApplyPatchToolArgs {
+    #[serde(alias = "patch")]
     pub(crate) input: String,
 }
 
diff --git a/codex-rs/core/tests/chat_completions_sse.rs b/codex-rs/core/tests/chat_completions_sse.rs
index 46378b082..05e86d9ff 100644
--- a/codex-rs/core/tests/chat_completions_sse.rs
+++ b/codex-rs/core/tests/chat_completions_sse.rs
@@ -28,10 +28,18 @@ fn network_disabled() -> bool {
 }
 
 async fn run_stream(sse_body: &str) -> Vec<ResponseEvent> {
-    run_stream_with_bytes(sse_body.as_bytes()).await
+    run_stream_with_mode_bytes(sse_body.as_bytes(), true).await
+}
+
+async fn run_stream_aggregated(sse_body: &str) -> Vec<ResponseEvent> {
+    run_stream_with_mode_bytes(sse_body.as_bytes(), false).await
 }
 
 async fn run_stream_with_bytes(sse_body: &[u8]) -> Vec<ResponseEvent> {
+    run_stream_with_mode_bytes(sse_body, true).await
+}
+
+async fn run_stream_with_mode_bytes(sse_body: &[u8], raw_reasoning: bool) -> Vec<ResponseEvent> {
     let server = MockServer::start().await;
 
     let template = ResponseTemplate::new(200)
@@ -68,7 +76,7 @@ async fn run_stream_with_bytes(sse_body: &[u8]) -> Vec<ResponseEvent> {
     let mut config = load_default_config_for_test(&codex_home);
     config.model_provider_id = provider.name.clone();
     config.model_provider = provider.clone();
-    config.show_raw_agent_reasoning = true;
+    config.show_raw_agent_reasoning = raw_reasoning;
     let effort = config.model_reasoning_effort;
     let summary = config.model_reasoning_summary;
     let config = Arc::new(config);
@@ -191,6 +199,164 @@ async fn streams_text_without_reasoning() {
     assert_matches!(events[3], ResponseEvent::Completed { .. });
 }
 
+#[tokio::test(flavor = "multi_thread", worker_threads = 2)]
+async fn aggregates_text_when_raw_reasoning_disabled() {
+    if network_disabled() {
+        println!(
+            "Skipping test because it cannot execute when network is disabled in a Codex sandbox."
+        );
+        return;
+    }
+
+    let sse = concat!(
+        "data: {\"choices\":[{\"delta\":{\"content\":\"hi\"}}]}\n\n",
+        "data: {\"choices\":[{\"delta\":{}}]}\n\n",
+        "data: [DONE]\n\n",
+    );
+
+    let events = run_stream_aggregated(sse).await;
+    assert_eq!(events.len(), 3, "unexpected events: {events:?}");
+
+    match &events[0] {
+        ResponseEvent::OutputItemAdded(ResponseItem::Message { .. }) => {}
+        other => panic!("expected initial message, got {other:?}"),
+    }
+
+    match &events[1] {
+        ResponseEvent::OutputItemDone(item) => assert_message(item, "hi"),
+        other => panic!("expected aggregated message, got {other:?}"),
+    }
+
+    assert_matches!(events[2], ResponseEvent::Completed { .. });
+}
+
+#[tokio::test(flavor = "multi_thread", worker_threads = 2)]
+async fn aggregates_reasoning_before_message() {
+    if network_disabled() {
+        println!(
+            "Skipping test because it cannot execute when network is disabled in a Codex sandbox."
+        );
+        return;
+    }
+
+    let sse = concat!(
+        "data: {\"choices\":[{\"delta\":{\"reasoning\":\"think1\"}}]}\n\n",
+        "data: {\"choices\":[{\"delta\":{\"content\":\"ok\"}}]}\n\n",
+        "data: {\"choices\":[{\"delta\":{} ,\"finish_reason\":\"stop\"}]}\n\n",
+    );
+
+    let events = run_stream_aggregated(sse).await;
+    assert_eq!(events.len(), 5, "unexpected events: {events:?}");
+
+    match &events[0] {
+        ResponseEvent::OutputItemAdded(ResponseItem::Reasoning { .. }) => {}
+        other => panic!("expected initial reasoning item, got {other:?}"),
+    }
+
+    match &events[1] {
+        ResponseEvent::OutputItemAdded(ResponseItem::Message { .. }) => {}
+        other => panic!("expected initial message item, got {other:?}"),
+    }
+
+    match &events[2] {
+        ResponseEvent::OutputItemDone(item) => assert_reasoning(item, "think1"),
+        other => panic!("expected reasoning item, got {other:?}"),
+    }
+
+    match &events[3] {
+        ResponseEvent::OutputItemDone(item) => assert_message(item, "ok"),
+        other => panic!("expected aggregated message, got {other:?}"),
+    }
+
+    assert_matches!(events[4], ResponseEvent::Completed { .. });
+}
+
+#[tokio::test(flavor = "multi_thread", worker_threads = 2)]
+async fn aggregated_events_preserve_history_order() {
+    if network_disabled() {
+        println!(
+            "Skipping test because it cannot execute when network is disabled in a Codex sandbox."
+        );
+        return;
+    }
+
+    let sse = concat!(
+        "data: {\"choices\":[{\"delta\":{\"reasoning\":\"think1\"}}]}\n\n",
+        "data: {\"choices\":[{\"delta\":{\"content\":\"ok\"}}]}\n\n",
+        "data: {\"choices\":[{\"delta\":{} ,\"finish_reason\":\"stop\"}]}\n\n",
+    );
+
+    let events = run_stream_aggregated(sse).await;
+    let mut items: Vec<ResponseItem> = Vec::new();
+    for event in events {
+        if let ResponseEvent::OutputItemDone(item) = event {
+            items.push(item);
+        }
+    }
+    assert_eq!(
+        items.len(),
+        2,
+        "expected reasoning + message, got {:?}",
+        items
+    );
+
+    match &items[0] {
+        ResponseItem::Reasoning { .. } => assert_reasoning(&items[0], "think1"),
+        other => panic!("expected reasoning first, got {other:?}"),
+    }
+    match &items[1] {
+        ResponseItem::Message { .. } => assert_message(&items[1], "ok"),
+        other => panic!("expected message second, got {other:?}"),
+    }
+}
+
+#[tokio::test(flavor = "multi_thread", worker_threads = 2)]
+async fn aggregated_tool_call_keeps_context_pairing() {
+    if network_disabled() {
+        println!(
+            "Skipping test because it cannot execute when network is disabled in a Codex sandbox."
+        );
+        return;
+    }
+
+    let sse = concat!(
+        "data: {\"choices\":[{\"delta\":{\"reasoning\":\"pre-tool\"}}]}\n\n",
+        "data: {\"choices\":[{\"delta\":{\"tool_calls\":[{\"id\":\"call_1\",\"type\":\"function\",\"function\":{\"name\":\"run\",\"arguments\":\"{}\"}}]},\"finish_reason\":\"tool_calls\"}]}\n\n",
+    );
+
+    let events = run_stream_aggregated(sse).await;
+    let mut items: Vec<ResponseItem> = Vec::new();
+    for event in events {
+        if let ResponseEvent::OutputItemDone(item) = event {
+            items.push(item);
+        }
+    }
+    assert_eq!(
+        items.len(),
+        2,
+        "expected reasoning + function call, got {:?}",
+        items
+    );
+
+    match &items[0] {
+        ResponseItem::Reasoning { .. } => assert_reasoning(&items[0], "pre-tool"),
+        other => panic!("expected reasoning first, got {other:?}"),
+    }
+    match &items[1] {
+        ResponseItem::FunctionCall {
+            name,
+            arguments,
+            call_id,
+            ..
+        } => {
+            assert_eq!(name, "run");
+            assert_eq!(arguments, "{}");
+            assert_eq!(call_id, "call_1");
+        }
+        other => panic!("expected function call second, got {other:?}"),
+    }
+}
+
 #[tokio::test(flavor = "multi_thread", worker_threads = 2)]
 async fn streams_reasoning_from_string_delta() {
     if network_disabled() {
diff --git a/codex-rs/exec/Cargo.toml b/codex-rs/exec/Cargo.toml
index 8fc1e3887..e32252966 100644
--- a/codex-rs/exec/Cargo.toml
+++ b/codex-rs/exec/Cargo.toml
@@ -23,6 +23,7 @@ codex-common = { workspace = true, features = [
     "elapsed",
     "sandbox_summary",
 ] }
+codex-build-info = { workspace = true }
 codex-core = { workspace = true }
 codex-ollama = { workspace = true }
 codex-protocol = { workspace = true }
diff --git a/codex-rs/exec/src/cli.rs b/codex-rs/exec/src/cli.rs
index f56d07dc3..b4b549b1e 100644
--- a/codex-rs/exec/src/cli.rs
+++ b/codex-rs/exec/src/cli.rs
@@ -1,3 +1,4 @@
+use clap::ArgAction;
 use clap::Parser;
 use clap::ValueEnum;
 use codex_common::CliConfigOverrides;
@@ -21,6 +22,14 @@ pub struct Cli {
     #[arg(long = "oss", default_value_t = false)]
     pub oss: bool,
 
+    /// Force-enable telemetry writers for this run (overrides config).
+    #[arg(long = "telemetry", action = ArgAction::SetTrue, conflicts_with = "no_telemetry")]
+    pub telemetry: bool,
+
+    /// Disable all telemetry writers for this run (overrides config).
+    #[arg(long = "no-telemetry", action = ArgAction::SetTrue)]
+    pub no_telemetry: bool,
+
     /// Select the sandbox policy to use when executing model-generated shell
     /// commands.
     #[arg(long = "sandbox", short = 's', value_enum)]
diff --git a/codex-rs/exec/src/event_processor_with_human_output.rs b/codex-rs/exec/src/event_processor_with_human_output.rs
index 93e0e493b..ba87b5140 100644
--- a/codex-rs/exec/src/event_processor_with_human_output.rs
+++ b/codex-rs/exec/src/event_processor_with_human_output.rs
@@ -1,5 +1,8 @@
+use codex_build_info::VERSION_WITH_COMMIT;
 use codex_common::elapsed::format_duration;
 use codex_common::elapsed::format_elapsed;
+use codex_core::assistant_message::is_noise_assistant_message;
+use codex_core::background_event::should_suppress_background_event;
 use codex_core::config::Config;
 use codex_core::protocol::AgentMessageEvent;
 use codex_core::protocol::AgentReasoningRawContentEvent;
@@ -133,11 +136,10 @@ impl EventProcessor for EventProcessorWithHumanOutput {
         prompt: &str,
         session_configured_event: &SessionConfiguredEvent,
     ) {
-        const VERSION: &str = env!("CARGO_PKG_VERSION");
         ts_msg!(
             self,
             "OpenAI Codex v{} (research preview)\n--------",
-            VERSION
+            VERSION_WITH_COMMIT
         );
 
         let mut entries = create_config_summary_entries(config);
@@ -183,6 +185,9 @@ impl EventProcessor for EventProcessorWithHumanOutput {
                 }
             }
             EventMsg::BackgroundEvent(BackgroundEventEvent { message }) => {
+                if should_suppress_background_event(&message) {
+                    return CodexStatus::Running;
+                }
                 ts_msg!(self, "{}", message.style(self.dimmed));
             }
             EventMsg::StreamError(StreamErrorEvent { message }) => {
@@ -222,6 +227,9 @@ impl EventProcessor for EventProcessorWithHumanOutput {
                 }
             }
             EventMsg::AgentMessage(AgentMessageEvent { message }) => {
+                if is_noise_assistant_message(&message) {
+                    return CodexStatus::Running;
+                }
                 ts_msg!(
                     self,
                     "{}\n{}",
diff --git a/codex-rs/exec/src/event_processor_with_jsonl_output.rs b/codex-rs/exec/src/event_processor_with_jsonl_output.rs
index 23dff015e..3dcc16043 100644
--- a/codex-rs/exec/src/event_processor_with_jsonl_output.rs
+++ b/codex-rs/exec/src/event_processor_with_jsonl_output.rs
@@ -33,6 +33,7 @@ use crate::exec_events::TurnFailedEvent;
 use crate::exec_events::TurnStartedEvent;
 use crate::exec_events::Usage;
 use crate::exec_events::WebSearchItem;
+use codex_core::assistant_message::is_noise_assistant_message;
 use codex_core::config::Config;
 use codex_core::protocol::AgentMessageEvent;
 use codex_core::protocol::AgentReasoningEvent;
@@ -173,6 +174,9 @@ impl EventProcessorWithJsonOutput {
     }
 
     fn handle_agent_message(&self, payload: &AgentMessageEvent) -> Vec<ThreadEvent> {
+        if is_noise_assistant_message(&payload.message) {
+            return Vec::new();
+        }
         let item = ThreadItem {
             id: self.get_next_item_id(),
 
diff --git a/codex-rs/exec/src/lib.rs b/codex-rs/exec/src/lib.rs
index 13d43f61e..420218062 100644
--- a/codex-rs/exec/src/lib.rs
+++ b/codex-rs/exec/src/lib.rs
@@ -69,6 +69,7 @@ pub async fn run_main(cli: Cli, codex_linux_sandbox_exe: Option<PathBuf>) -> any
         prompt,
         output_schema: output_schema_path,
         config_overrides,
+        ..
     } = cli;
 
     // Determine the prompt source (parent or subcommand) and read from stdin if needed.
@@ -181,6 +182,13 @@ pub async fn run_main(cli: Cli, codex_linux_sandbox_exe: Option<PathBuf>) -> any
         tools_web_search_request: None,
         experimental_sandbox_command_assessment: None,
         additional_writable_roots: Vec::new(),
+        telemetry_enabled: if cli.telemetry {
+            Some(true)
+        } else if cli.no_telemetry {
+            Some(false)
+        } else {
+            None
+        },
     };
     // Parse `-c` overrides.
     let cli_kv_overrides = match config_overrides.parse_overrides() {
diff --git a/codex-rs/feedback/Cargo.toml b/codex-rs/feedback/Cargo.toml
index b104f5122..a0f2f47d3 100644
--- a/codex-rs/feedback/Cargo.toml
+++ b/codex-rs/feedback/Cargo.toml
@@ -6,7 +6,14 @@ version.workspace = true
 [dependencies]
 anyhow = { workspace = true }
 codex-protocol = { workspace = true }
-sentry = { version = "0.34" }
+sentry = { version = "0.34", default-features = false, features = [
+    "backtrace",
+    "contexts",
+    "debug-images",
+    "panic",
+    "reqwest",
+    "rustls",
+] }
 tracing-subscriber = { workspace = true }
 
 [dev-dependencies]
diff --git a/codex-rs/keyring-store/Cargo.toml b/codex-rs/keyring-store/Cargo.toml
index f662e5d4f..94d3d5449 100644
--- a/codex-rs/keyring-store/Cargo.toml
+++ b/codex-rs/keyring-store/Cargo.toml
@@ -7,10 +7,5 @@ version = { workspace = true }
 workspace = true
 
 [dependencies]
-keyring = { workspace = true, features = [
-    "apple-native",
-    "crypto-rust",
-    "linux-native-async-persistent",
-    "windows-native",
-] }
+keyring = { workspace = true }
 tracing = { workspace = true }
diff --git a/codex-rs/mcp-server/src/codex_tool_config.rs b/codex-rs/mcp-server/src/codex_tool_config.rs
index 4e61bde02..ceb89a344 100644
--- a/codex-rs/mcp-server/src/codex_tool_config.rs
+++ b/codex-rs/mcp-server/src/codex_tool_config.rs
@@ -171,6 +171,7 @@ impl CodexToolCallParam {
             tools_web_search_request: None,
             experimental_sandbox_command_assessment: None,
             additional_writable_roots: Vec::new(),
+            telemetry_enabled: None,
         };
 
         let cli_overrides = cli_overrides
diff --git a/codex-rs/process-hardening/Cargo.toml b/codex-rs/process-hardening/Cargo.toml
index 7294b6e26..549d980a8 100644
--- a/codex-rs/process-hardening/Cargo.toml
+++ b/codex-rs/process-hardening/Cargo.toml
@@ -19,3 +19,12 @@ libc = { workspace = true }
 
 [target.'cfg(target_os = "macos")'.dependencies]
 libc = { workspace = true }
+
+[target.'cfg(target_os = "illumos")'.dependencies]
+libc = { workspace = true }
+
+[target.'cfg(target_os = "freebsd")'.dependencies]
+libc = { workspace = true }
+
+[target.'cfg(target_os = "openbsd")'.dependencies]
+libc = { workspace = true }
diff --git a/codex-rs/process-hardening/src/lib.rs b/codex-rs/process-hardening/src/lib.rs
index a787b4097..5194db405 100644
--- a/codex-rs/process-hardening/src/lib.rs
+++ b/codex-rs/process-hardening/src/lib.rs
@@ -10,6 +10,9 @@ pub fn pre_main_hardening() {
     #[cfg(target_os = "macos")]
     pre_main_hardening_macos();
 
+    #[cfg(any(target_os = "freebsd", target_os = "openbsd"))]
+    pre_main_hardening_bsd();
+
     #[cfg(windows)]
     pre_main_hardening_windows();
 }
@@ -20,7 +23,14 @@ const PRCTL_FAILED_EXIT_CODE: i32 = 5;
 #[cfg(target_os = "macos")]
 const PTRACE_DENY_ATTACH_FAILED_EXIT_CODE: i32 = 6;
 
-#[cfg(any(target_os = "linux", target_os = "android", target_os = "macos"))]
+#[cfg(any(
+    target_os = "linux",
+    target_os = "android",
+    target_os = "macos",
+    target_os = "illumos",
+    target_os = "freebsd",
+    target_os = "openbsd",
+))]
 const SET_RLIMIT_CORE_FAILED_EXIT_CODE: i32 = 7;
 
 #[cfg(any(target_os = "linux", target_os = "android"))]
@@ -108,6 +118,13 @@ fn set_core_file_size_limit_to_zero() {
     }
 }
 
+#[cfg(any(target_os = "freebsd", target_os = "openbsd"))]
+fn pre_main_hardening_bsd() {
+    // FreeBSD/OpenBSD do not expose ptrace toggles like PR_SET_DUMPABLE,
+    // but we can still disable core dumps for consistency across targets.
+    set_core_file_size_limit_to_zero();
+}
+
 #[cfg(windows)]
 pub(crate) fn pre_main_hardening_windows() {
     // TODO(mbolin): Perform the appropriate configuration for Windows.
diff --git a/codex-rs/protocol/src/models.rs b/codex-rs/protocol/src/models.rs
index a824ee91e..0b5351358 100644
--- a/codex-rs/protocol/src/models.rs
+++ b/codex-rs/protocol/src/models.rs
@@ -203,6 +203,7 @@ pub enum LocalShellAction {
 
 #[derive(Debug, Clone, Serialize, Deserialize, PartialEq, JsonSchema, TS)]
 pub struct LocalShellExecAction {
+    #[serde(rename = "command", alias = "cmd")]
     pub command: Vec<String>,
     pub timeout_ms: Option<u64>,
     pub working_directory: Option<String>,
@@ -295,6 +296,7 @@ impl From<Vec<UserInput>> for ResponseInputItem {
 /// or shell`, the `arguments` field should deserialize to this struct.
 #[derive(Deserialize, Debug, Clone, PartialEq, JsonSchema, TS)]
 pub struct ShellToolCallParams {
+    #[serde(rename = "command", alias = "cmd")]
     pub command: Vec<String>,
     pub workdir: Option<String>,
 
@@ -622,6 +624,27 @@ mod tests {
         Ok(())
     }
 
+    #[test]
+    fn deserialize_shell_tool_call_params_cmd_alias() -> Result<()> {
+        let json = r#"{
+            "cmd": ["bash", "-lc", "ls -R"],
+            "timeout_ms": 12345
+        }"#;
+
+        let params: ShellToolCallParams = serde_json::from_str(json)?;
+        assert_eq!(
+            params,
+            ShellToolCallParams {
+                command: vec!["bash".to_string(), "-lc".to_string(), "ls -R".to_string()],
+                workdir: None,
+                timeout_ms: Some(12345),
+                with_escalated_permissions: None,
+                justification: None,
+            }
+        );
+        Ok(())
+    }
+
     #[test]
     fn local_image_read_error_adds_placeholder() -> Result<()> {
         let dir = tempdir()?;
diff --git a/codex-rs/rmcp-client/Cargo.toml b/codex-rs/rmcp-client/Cargo.toml
index e9f832e65..996d7ecfc 100644
--- a/codex-rs/rmcp-client/Cargo.toml
+++ b/codex-rs/rmcp-client/Cargo.toml
@@ -16,12 +16,7 @@ codex-keyring-store = { workspace = true }
 codex-protocol = { workspace = true }
 dirs = { workspace = true }
 futures = { workspace = true, default-features = false, features = ["std"] }
-keyring = { workspace = true, features = [
-    "apple-native",
-    "crypto-rust",
-    "linux-native-async-persistent",
-    "windows-native",
-] }
+keyring = { workspace = true }
 mcp-types = { path = "../mcp-types" }
 oauth2 = "5"
 reqwest = { version = "0.12", default-features = false, features = [
diff --git a/codex-rs/tui/Cargo.toml b/codex-rs/tui/Cargo.toml
index c56aba0fc..afa4357c3 100644
--- a/codex-rs/tui/Cargo.toml
+++ b/codex-rs/tui/Cargo.toml
@@ -40,6 +40,7 @@ codex-ollama = { workspace = true }
 codex-protocol = { workspace = true }
 codex-app-server-protocol = { workspace = true }
 codex-feedback = { workspace = true }
+codex-litellm-model-session-telemetry = { workspace = true }
 color-eyre = { workspace = true }
 crossterm = { workspace = true, features = [
     "bracketed-paste",
diff --git a/codex-rs/tui/src/app.rs b/codex-rs/tui/src/app.rs
index 3c06113cb..1cdb9df42 100644
--- a/codex-rs/tui/src/app.rs
+++ b/codex-rs/tui/src/app.rs
@@ -14,6 +14,7 @@ use crate::tui;
 use crate::tui::TuiEvent;
 use crate::updates::UpdateAction;
 use codex_ansi_escape::ansi_escape_line;
+use codex_common::model_presets::ModelPreset;
 use codex_core::AuthManager;
 use codex_core::ConversationManager;
 use codex_core::config::Config;
@@ -78,6 +79,7 @@ pub(crate) struct App {
     pub(crate) feedback: codex_feedback::CodexFeedback,
     /// Set when the user confirms an update; propagated on exit.
     pub(crate) pending_update_action: Option<UpdateAction>,
+    model_presets: Vec<ModelPreset>,
 }
 
 impl App {
@@ -91,6 +93,9 @@ impl App {
         initial_images: Vec<PathBuf>,
         resume_selection: ResumeSelection,
         feedback: codex_feedback::CodexFeedback,
+        model_presets: Vec<ModelPreset>,
+        auto_open_model_selector: bool,
+        litellm_model_missing: bool,
     ) -> Result<AppExitInfo> {
         use tokio_stream::StreamExt;
         let (app_event_tx, mut app_event_rx) = unbounded_channel();
@@ -103,6 +108,12 @@ impl App {
 
         let enhanced_keys_supported = tui.enhanced_keys_supported();
 
+        let should_auto_open_selector = auto_open_model_selector
+            && matches!(
+                resume_selection,
+                ResumeSelection::StartFresh | ResumeSelection::Exit
+            );
+
         let chat_widget = match resume_selection {
             ResumeSelection::StartFresh | ResumeSelection::Exit => {
                 let init = crate::chatwidget::ChatWidgetInit {
@@ -114,6 +125,9 @@ impl App {
                     enhanced_keys_supported,
                     auth_manager: auth_manager.clone(),
                     feedback: feedback.clone(),
+                    model_presets: model_presets.clone(),
+                    auto_open_model_selector: should_auto_open_selector,
+                    litellm_model_missing,
                 };
                 ChatWidget::new(init, conversation_manager.clone())
             }
@@ -137,6 +151,9 @@ impl App {
                     enhanced_keys_supported,
                     auth_manager: auth_manager.clone(),
                     feedback: feedback.clone(),
+                    model_presets: model_presets.clone(),
+                    auto_open_model_selector: false,
+                    litellm_model_missing,
                 };
                 ChatWidget::new_from_existing(
                     init,
@@ -167,6 +184,7 @@ impl App {
             backtrack: BacktrackState::default(),
             feedback: feedback.clone(),
             pending_update_action: None,
+            model_presets,
         };
 
         #[cfg(not(debug_assertions))]
@@ -257,6 +275,9 @@ impl App {
                     enhanced_keys_supported: self.enhanced_keys_supported,
                     auth_manager: self.auth_manager.clone(),
                     feedback: self.feedback.clone(),
+                    model_presets: self.model_presets.clone(),
+                    auto_open_model_selector: false,
+                    litellm_model_missing: false,
                 };
                 self.chat_widget = ChatWidget::new(init, self.server.clone());
                 tui.frame_requester().schedule_frame();
@@ -380,6 +401,14 @@ impl App {
                     .await
                 {
                     Ok(()) => {
+                        let profile_label = profile.unwrap_or("default");
+                        tracing::info!(
+                            target: "codex_litellm_debug::model_selection",
+                            model = %model,
+                            effort = ?effort,
+                            profile = profile_label,
+                            "model_selection.persisted"
+                        );
                         let effort_label = effort
                             .map(|eff| format!(" with {eff} reasoning"))
                             .unwrap_or_else(|| " with default reasoning".to_string());
@@ -399,7 +428,11 @@ impl App {
                     }
                     Err(err) => {
                         tracing::error!(
+                            target: "codex_litellm_debug::model_selection",
                             error = %err,
+                            model = %model,
+                            effort = ?effort,
+                            profile = %profile.unwrap_or("default"),
                             "failed to persist model selection"
                         );
                         if let Some(profile) = profile {
@@ -591,6 +624,7 @@ mod tests {
             backtrack: BacktrackState::default(),
             feedback: codex_feedback::CodexFeedback::new(),
             pending_update_action: None,
+            model_presets: Vec::new(),
         }
     }
 
diff --git a/codex-rs/tui/src/app_backtrack.rs b/codex-rs/tui/src/app_backtrack.rs
index e2ded3d3e..ee7218c9a 100644
--- a/codex-rs/tui/src/app_backtrack.rs
+++ b/codex-rs/tui/src/app_backtrack.rs
@@ -347,6 +347,9 @@ impl App {
             enhanced_keys_supported: self.enhanced_keys_supported,
             auth_manager: self.auth_manager.clone(),
             feedback: self.feedback.clone(),
+            model_presets: Vec::new(),
+            auto_open_model_selector: false,
+            litellm_model_missing: false,
         };
         self.chat_widget =
             crate::chatwidget::ChatWidget::new_from_existing(init, conv, session_configured);
diff --git a/codex-rs/tui/src/bottom_pane/list_selection_view.rs b/codex-rs/tui/src/bottom_pane/list_selection_view.rs
index 44d7b2640..a5988a8e1 100644
--- a/codex-rs/tui/src/bottom_pane/list_selection_view.rs
+++ b/codex-rs/tui/src/bottom_pane/list_selection_view.rs
@@ -364,13 +364,32 @@ impl Renderable for ListSelectionView {
         let rows = self.build_rows();
         let rows_height =
             measure_rows_height(&rows, &self.state, MAX_POPUP_ROWS, content_area.width);
+
+        let inset_area = content_area.inset(Insets::vh(1, 2));
+        let mut remaining = inset_area.height;
+
+        let header_len = header_height.min(remaining);
+        remaining = remaining.saturating_sub(header_len);
+
+        let spacer_len = remaining.min(1);
+        remaining = remaining.saturating_sub(spacer_len);
+
+        let search_len = if self.is_searchable {
+            remaining.min(1)
+        } else {
+            0
+        };
+        remaining = remaining.saturating_sub(search_len);
+
+        let rows_len = rows_height.min(remaining);
+
         let [header_area, _, search_area, list_area] = Layout::vertical([
-            Constraint::Max(header_height),
-            Constraint::Max(1),
-            Constraint::Length(if self.is_searchable { 1 } else { 0 }),
-            Constraint::Length(rows_height),
+            Constraint::Length(header_len),
+            Constraint::Length(spacer_len),
+            Constraint::Length(search_len),
+            Constraint::Fill(1),
         ])
-        .areas(content_area.inset(Insets::vh(1, 2)));
+        .areas(inset_area);
 
         if header_area.height < header_height {
             let [header_area, elision_area] =
@@ -385,31 +404,35 @@ impl Renderable for ListSelectionView {
         }
 
         if self.is_searchable {
-            Line::from(self.search_query.clone()).render(search_area, buf);
-            let query_span: Span<'static> = if self.search_query.is_empty() {
-                self.search_placeholder
-                    .as_ref()
-                    .map(|placeholder| placeholder.clone().dim())
-                    .unwrap_or_else(|| "".into())
-            } else {
-                self.search_query.clone().into()
-            };
-            Line::from(query_span).render(search_area, buf);
+            if search_area.height > 0 {
+                Line::from(self.search_query.clone()).render(search_area, buf);
+                let query_span: Span<'static> = if self.search_query.is_empty() {
+                    self.search_placeholder
+                        .as_ref()
+                        .map(|placeholder| placeholder.clone().dim())
+                        .unwrap_or_else(|| "".into())
+                } else {
+                    self.search_query.clone().into()
+                };
+                Line::from(query_span).render(search_area, buf);
+            }
         }
 
-        if list_area.height > 0 {
-            let list_area = Rect {
-                x: list_area.x - 2,
+        let usable_rows = rows_len.min(list_area.height);
+
+        if usable_rows > 0 {
+            let render_area = Rect {
+                x: list_area.x,
                 y: list_area.y,
-                width: list_area.width + 2,
-                height: list_area.height,
+                width: list_area.width,
+                height: usable_rows,
             };
             render_rows(
-                list_area,
+                render_area,
                 buf,
                 &rows,
                 &self.state,
-                list_area.height as usize,
+                usable_rows as usize,
                 "no matches",
             );
         }
diff --git a/codex-rs/tui/src/bottom_pane/mod.rs b/codex-rs/tui/src/bottom_pane/mod.rs
index 38a5ecb6e..346db101e 100644
--- a/codex-rs/tui/src/bottom_pane/mod.rs
+++ b/codex-rs/tui/src/bottom_pane/mod.rs
@@ -407,6 +407,11 @@ impl BottomPane {
         self.request_redraw();
     }
 
+    #[cfg(test)]
+    pub(crate) fn context_window_percent_for_tests(&self) -> Option<i64> {
+        self.context_window_percent
+    }
+
     /// Show a generic list selection view with the provided items.
     pub(crate) fn show_selection_view(&mut self, params: list_selection_view::SelectionViewParams) {
         let view = list_selection_view::ListSelectionView::new(params, self.app_event_tx.clone());
diff --git a/codex-rs/tui/src/bottom_pane/snapshots/codex_tui__bottom_pane__list_selection_view__tests__list_selection_spacing_with_subtitle.snap b/codex-rs/tui/src/bottom_pane/snapshots/codex_tui__bottom_pane__list_selection_view__tests__list_selection_spacing_with_subtitle.snap
index 512f6bbca..5719caa95 100644
--- a/codex-rs/tui/src/bottom_pane/snapshots/codex_tui__bottom_pane__list_selection_view__tests__list_selection_spacing_with_subtitle.snap
+++ b/codex-rs/tui/src/bottom_pane/snapshots/codex_tui__bottom_pane__list_selection_view__tests__list_selection_spacing_with_subtitle.snap
@@ -1,12 +1,13 @@
 ---
 source: tui/src/bottom_pane/list_selection_view.rs
+assertion_line: 528
 expression: render_lines(&view)
 ---
                                                 
   Select Approval Mode                          
   Switch between Codex approval presets         
                                                 
-› 1. Read Only (current)  Codex can read files  
-  2. Full Access          Codex can edit files  
+  › 1. Read Only (current)  Codex can read      
+                            files               
                                                 
   Press enter to confirm or esc to go back
diff --git a/codex-rs/tui/src/bottom_pane/snapshots/codex_tui__bottom_pane__list_selection_view__tests__list_selection_spacing_without_subtitle.snap b/codex-rs/tui/src/bottom_pane/snapshots/codex_tui__bottom_pane__list_selection_view__tests__list_selection_spacing_without_subtitle.snap
index ddd0f90cd..3262b9f44 100644
--- a/codex-rs/tui/src/bottom_pane/snapshots/codex_tui__bottom_pane__list_selection_view__tests__list_selection_spacing_without_subtitle.snap
+++ b/codex-rs/tui/src/bottom_pane/snapshots/codex_tui__bottom_pane__list_selection_view__tests__list_selection_spacing_without_subtitle.snap
@@ -1,11 +1,12 @@
 ---
 source: tui/src/bottom_pane/list_selection_view.rs
+assertion_line: 519
 expression: render_lines(&view)
 ---
                                                 
   Select Approval Mode                          
                                                 
-› 1. Read Only (current)  Codex can read files  
-  2. Full Access          Codex can edit files  
+  › 1. Read Only (current)  Codex can read      
+                            files               
                                                 
   Press enter to confirm or esc to go back
diff --git a/codex-rs/tui/src/chatwidget.rs b/codex-rs/tui/src/chatwidget.rs
index 5239c66e8..6e6b56a71 100644
--- a/codex-rs/tui/src/chatwidget.rs
+++ b/codex-rs/tui/src/chatwidget.rs
@@ -3,6 +3,8 @@ use std::collections::VecDeque;
 use std::path::PathBuf;
 use std::sync::Arc;
 
+use codex_core::assistant_message::is_noise_assistant_message;
+use codex_core::background_event::should_suppress_background_event;
 use codex_core::config::Config;
 use codex_core::config::types::Notifications;
 use codex_core::git_info::current_branch_name;
@@ -103,22 +105,26 @@ use self::interrupts::InterruptManager;
 mod agent;
 use self::agent::spawn_agent;
 use self::agent::spawn_agent_from_existing;
-mod session_header;
-use self::session_header::SessionHeader;
 use crate::streaming::controller::StreamController;
 use std::path::Path;
 
+const DISPLAY_PREVIEW_LIMIT: usize = 160;
+
 use chrono::Local;
 use codex_common::approval_presets::ApprovalPreset;
 use codex_common::approval_presets::builtin_approval_presets;
-use codex_common::model_presets::ModelPreset;
-use codex_common::model_presets::builtin_model_presets;
+use codex_common::model_presets::{
+    ModelPreset, builtin_model_presets, fetch_litellm_model_presets, is_litellm_provider_id,
+};
 use codex_core::AuthManager;
 use codex_core::ConversationManager;
+use codex_core::config::read_litellm_provider_state;
+use codex_core::config::types::ReasoningSummaryFormat;
 use codex_core::protocol::AskForApproval;
 use codex_core::protocol::SandboxPolicy;
 use codex_core::protocol_config_types::ReasoningEffort as ReasoningEffortConfig;
 use codex_file_search::FileMatch;
+use codex_litellm_model_session_telemetry as session_telemetry;
 use codex_protocol::plan_tool::UpdatePlanArgs;
 use strum::IntoEnumIterator;
 
@@ -228,6 +234,9 @@ pub(crate) struct ChatWidgetInit {
     pub(crate) enhanced_keys_supported: bool,
     pub(crate) auth_manager: Arc<AuthManager>,
     pub(crate) feedback: codex_feedback::CodexFeedback,
+    pub(crate) model_presets: Vec<ModelPreset>,
+    pub(crate) auto_open_model_selector: bool,
+    pub(crate) litellm_model_missing: bool,
 }
 
 pub(crate) struct ChatWidget {
@@ -237,7 +246,6 @@ pub(crate) struct ChatWidget {
     active_cell: Option<Box<dyn HistoryCell>>,
     config: Config,
     auth_manager: Arc<AuthManager>,
-    session_header: SessionHeader,
     initial_user_message: Option<UserMessage>,
     token_info: Option<TokenUsageInfo>,
     rate_limit_snapshot: Option<RateLimitSnapshotDisplay>,
@@ -269,14 +277,25 @@ pub(crate) struct ChatWidget {
     pending_notification: Option<Notification>,
     // Simple review mode flag; used to adjust layout and banners.
     is_review_mode: bool,
-    // Whether to add a final message separator after the last message
-    needs_final_message_separator: bool,
+    // Whether to insert an interstitial divider before the next streamed chunk
+    needs_history_divider: bool,
+    suppress_next_history_divider: bool,
+    reasoning_summary_inserted: bool,
+    final_separator_inserted: bool,
+    pending_final_separator: Option<Option<u64>>,
+    capture_agentic_reasoning: bool,
 
     last_rendered_width: std::cell::Cell<Option<usize>>,
     // Feedback sink for /feedback
     feedback: codex_feedback::CodexFeedback,
     // Current session rollout path (if known)
     current_rollout_path: Option<PathBuf>,
+    model_presets: Vec<ModelPreset>,
+    auto_open_model_selector: bool,
+    litellm_model_missing: bool,
+    litellm_model_missing_notified: bool,
+    resumed_session: bool,
+    resume_overflow_warned: bool,
 }
 
 struct UserMessage {
@@ -293,6 +312,12 @@ impl From<String> for UserMessage {
     }
 }
 
+fn is_fallback_notification(message: &str) -> bool {
+    let trimmed = message.trim_start();
+    trimmed.starts_with("The upstream model did not respond within")
+        || trimmed.starts_with("The upstream model paused before returning an answer.")
+}
+
 fn create_initial_user_message(text: String, image_paths: Vec<PathBuf>) -> Option<UserMessage> {
     if text.is_empty() && image_paths.is_empty() {
         None
@@ -302,28 +327,102 @@ fn create_initial_user_message(text: String, image_paths: Vec<PathBuf>) -> Optio
 }
 
 impl ChatWidget {
+    fn display_preview(text: &str) -> String {
+        let mut preview = String::new();
+        let mut chars = text.chars();
+        for _ in 0..DISPLAY_PREVIEW_LIMIT {
+            match chars.next() {
+                Some(ch) => preview.push(ch),
+                None => return preview,
+            }
+        }
+        if chars.next().is_some() {
+            preview.push('…');
+        }
+        preview
+    }
+
+    fn log_display_event(&self, element: &str, event: &str, detail: Option<&str>) {
+        if let Some(detail) = detail {
+            tracing::info!(
+                target: "codex_litellm_debug::display",
+                element,
+                event,
+                detail = %Self::display_preview(detail),
+                "display.event"
+            );
+        } else {
+            tracing::info!(
+                target: "codex_litellm_debug::display",
+                element,
+                event,
+                "display.event"
+            );
+        }
+    }
+
+    fn should_capture_agentic_reasoning(&self) -> bool {
+        self.capture_agentic_reasoning
+            && matches!(
+                self.config.model_family.reasoning_summary_format,
+                ReasoningSummaryFormat::Experimental
+            )
+            && !self.reasoning_summary_inserted
+    }
+
+    fn finish_agentic_reasoning_capture(&mut self) {
+        if self.should_capture_agentic_reasoning() && !self.reasoning_buffer.is_empty() {
+            self.on_agent_reasoning_final();
+        }
+        self.capture_agentic_reasoning = false;
+    }
+
     fn flush_answer_stream_with_separator(&mut self) {
         if let Some(mut controller) = self.stream_controller.take()
             && let Some(cell) = controller.finalize()
         {
+            if let Some(elapsed) = self.pending_final_separator.take() {
+                let detail = elapsed
+                    .map(super::status_indicator_widget::fmt_elapsed_compact)
+                    .unwrap_or_else(|| "unknown".to_string());
+                self.log_display_event("history", "final_separator_inserted", Some(&detail));
+                self.push_history_cell_direct(Box::new(history_cell::FinalMessageSeparator::new(
+                    elapsed,
+                )));
+                self.needs_history_divider = false;
+                self.suppress_next_history_divider = true;
+            }
             self.add_boxed_history(cell);
         }
     }
 
     fn set_status_header(&mut self, header: String) {
+        self.log_display_event("status", "header_update", Some(&header));
         self.current_status_header = header.clone();
         self.bottom_pane.update_status_header(header);
     }
 
+    fn mark_litellm_model_missing(&mut self) {
+        if self.litellm_model_missing_notified {
+            return;
+        }
+        self.add_error_message("Select a LiteLLM model (run /model)".to_string());
+        self.litellm_model_missing_notified = true;
+    }
+
     // --- Small event handlers ---
     fn on_session_configured(&mut self, event: codex_core::protocol::SessionConfiguredEvent) {
+        let session_id_str = event.session_id.to_string();
         self.bottom_pane
             .set_history_metadata(event.history_log_id, event.history_entry_count);
         self.conversation_id = Some(event.session_id);
         self.current_rollout_path = Some(event.rollout_path.clone());
         let initial_messages = event.initial_messages.clone();
-        let model_for_header = event.model.clone();
-        self.session_header.set_model(&model_for_header);
+        let resumed = initial_messages.is_some();
+        self.resumed_session = resumed;
+        self.resume_overflow_warned = false;
+        self.config.model = event.model.clone();
+        session_telemetry::clear_session(Some(&session_id_str));
         self.add_to_history(history_cell::new_session_info(
             &self.config,
             event,
@@ -334,6 +433,10 @@ impl ChatWidget {
         }
         // Ask codex-core to enumerate custom prompts for this session.
         self.submit_op(Op::ListCustomPrompts);
+        if self.auto_open_model_selector {
+            self.auto_open_model_selector = false;
+            self.open_model_popup();
+        }
         if let Some(user_message) = self.initial_user_message.take() {
             self.submit_user_message(user_message);
         }
@@ -376,44 +479,82 @@ impl ChatWidget {
     }
 
     fn on_agent_message(&mut self, message: String) {
+        if is_fallback_notification(&message) {
+            self.add_to_history(history_cell::new_warning_event(message));
+            self.request_redraw();
+            return;
+        }
+        if is_noise_assistant_message(&message) {
+            self.log_display_event("history", "agent_message_suppressed", None);
+            return;
+        }
+        if self.should_capture_agentic_reasoning() {
+            self.on_agent_reasoning_delta(message);
+            self.finish_agentic_reasoning_capture();
+            self.handle_stream_finished();
+            self.request_redraw();
+            return;
+        }
+        self.log_display_event("history", "agent_message_final", Some(&message));
         // If we have a stream_controller, then the final agent message is redundant and will be a
         // duplicate of what has already been streamed.
         if self.stream_controller.is_none() {
             self.handle_streaming_delta(message);
         }
+        self.insert_final_separator();
         self.flush_answer_stream_with_separator();
         self.handle_stream_finished();
         self.request_redraw();
     }
 
     fn on_agent_message_delta(&mut self, delta: String) {
+        if self.should_capture_agentic_reasoning() {
+            self.on_agent_reasoning_delta(delta);
+            return;
+        }
         self.handle_streaming_delta(delta);
     }
 
     fn on_agent_reasoning_delta(&mut self, delta: String) {
+        self.capture_agentic_reasoning = false;
         // For reasoning deltas, do not stream to history. Accumulate the
         // current reasoning block and extract the first bold element
         // (between **/**) as the chunk header. Show this header as status.
+        self.log_display_event("status", "reasoning_delta", Some(&delta));
         self.reasoning_buffer.push_str(&delta);
 
         if let Some(header) = extract_first_bold(&self.reasoning_buffer) {
             // Update the shimmer header to the extracted reasoning chunk header.
             self.set_status_header(header);
-        } else {
-            // Fallback while we don't yet have a bold header: leave existing header as-is.
+        } else if let Some(summary) = summarize_reasoning_snippet(&delta) {
+            self.set_status_header(summary);
         }
         self.request_redraw();
     }
 
     fn on_agent_reasoning_final(&mut self) {
         // At the end of a reasoning block, record transcript-only content.
+        if !self.reasoning_buffer.is_empty() {
+            self.log_display_event(
+                "history",
+                "reasoning_complete",
+                Some(&self.reasoning_buffer),
+            );
+        } else {
+            self.log_display_event("history", "reasoning_complete", None);
+        }
         self.full_reasoning_buffer.push_str(&self.reasoning_buffer);
         if !self.full_reasoning_buffer.is_empty() {
+            let transcript_only = self.reasoning_summary_inserted;
             let cell = history_cell::new_reasoning_summary_block(
                 self.full_reasoning_buffer.clone(),
                 &self.config,
+                transcript_only,
             );
             self.add_boxed_history(cell);
+            if !self.reasoning_summary_inserted {
+                self.reasoning_summary_inserted = true;
+            }
         }
         self.reasoning_buffer.clear();
         self.full_reasoning_buffer.clear();
@@ -430,6 +571,7 @@ impl ChatWidget {
     // Raw reasoning uses the same flow as summarized reasoning
 
     fn on_task_started(&mut self) {
+        self.log_display_event("status", "task_started", None);
         self.bottom_pane.clear_ctrl_c_quit_hint();
         self.bottom_pane.set_task_running(true);
         self.retry_status_header = None;
@@ -437,11 +579,26 @@ impl ChatWidget {
         self.set_status_header(String::from("Working"));
         self.full_reasoning_buffer.clear();
         self.reasoning_buffer.clear();
+        self.reasoning_summary_inserted = false;
+        self.final_separator_inserted = false;
+        self.needs_history_divider = false;
+        self.pending_final_separator = None;
+        self.capture_agentic_reasoning = matches!(
+            self.config.model_family.reasoning_summary_format,
+            ReasoningSummaryFormat::Experimental
+        );
         self.request_redraw();
     }
 
     fn on_task_complete(&mut self, last_agent_message: Option<String>) {
+        self.log_display_event(
+            "history",
+            "task_complete_event",
+            last_agent_message.as_deref(),
+        );
         // If a stream is currently active, finalize it.
+        self.finish_agentic_reasoning_capture();
+        self.insert_final_separator();
         self.flush_answer_stream_with_separator();
         // Mark task stopped and request redraw now that all content is in history.
         self.bottom_pane.set_task_running(false);
@@ -451,6 +608,10 @@ impl ChatWidget {
         // If there is a queued user message, send exactly one now to begin the next turn.
         self.maybe_send_next_queued_input();
         // Emit a notification when the turn completes (suppressed if focused).
+        if let Some(message) = last_agent_message.as_ref() {
+            self.log_display_event("history", "task_complete_message", Some(message));
+        }
+        self.log_display_event("status", "task_complete", None);
         self.notify(Notification::AgentTurnComplete {
             response: last_agent_message.unwrap_or_default(),
         });
@@ -458,15 +619,71 @@ impl ChatWidget {
 
     pub(crate) fn set_token_info(&mut self, info: Option<TokenUsageInfo>) {
         if let Some(info) = info {
+            let usage_detail = format!(
+                "prompt={}, completion={}, total={}",
+                info.last_token_usage.input_tokens + info.last_token_usage.cached_input_tokens,
+                info.last_token_usage.output_tokens,
+                info.last_token_usage.total_tokens
+            );
+            self.log_display_event("status", "token_usage_update", Some(&usage_detail));
             let context_window = info
                 .model_context_window
                 .or(self.config.model_context_window);
-            let percent = context_window.map(|window| {
-                info.last_token_usage
-                    .percent_of_context_window_remaining(window)
-            });
+            if self.resumed_session && !self.resume_overflow_warned {
+                if let Some(window) = context_window {
+                    let tokens_in_context = info.total_token_usage.tokens_in_context_window();
+                    if tokens_in_context >= window {
+                        self.add_to_history(history_cell::new_warning_event(
+                            "Saved history exceeds the configured context window. Run /compact before continuing or start a new session with /new."
+                                .to_string(),
+                        ));
+                        self.set_status_header(
+                            "History exceeds context — run /compact".to_string(),
+                        );
+                        self.resume_overflow_warned = true;
+                    }
+                }
+            }
+            let last_usage = info.last_token_usage.clone();
+            let total_usage = info.total_token_usage.clone();
+            let percent = context_window
+                .map(|window| total_usage.percent_of_context_window_remaining(window));
+            let prompt_tokens = last_usage.input_tokens + last_usage.cached_input_tokens;
+            let detail_string = if let Some(p) = percent {
+                format!(
+                    "prompt={prompt_tokens}, completion={}, reasoning={}, total={}, context_remaining={}%",
+                    last_usage.output_tokens,
+                    last_usage.reasoning_output_tokens,
+                    last_usage.total_tokens,
+                    p
+                )
+            } else {
+                format!(
+                    "prompt={prompt_tokens}, completion={}, reasoning={}, total={}",
+                    last_usage.output_tokens,
+                    last_usage.reasoning_output_tokens,
+                    last_usage.total_tokens
+                )
+            };
+            self.log_display_event("status", "token_usage_updated", Some(&detail_string));
             self.bottom_pane.set_context_window_percent(percent);
             self.token_info = Some(info);
+
+            let session_id = self.conversation_id.as_ref().map(|id| id.to_string());
+            let reasoning_effort = self
+                .config
+                .model_reasoning_effort
+                .as_ref()
+                .map(ToString::to_string);
+            session_telemetry::record_turn(
+                session_id.as_deref(),
+                self.config.model.as_str(),
+                reasoning_effort.as_deref(),
+                prompt_tokens,
+                last_usage.output_tokens,
+                last_usage.reasoning_output_tokens,
+                last_usage.total_tokens,
+            );
         }
     }
 
@@ -489,10 +706,27 @@ impl ChatWidget {
             );
 
             let display = crate::status::rate_limit_snapshot_display(&snapshot, Local::now());
+            let fmt_percent = |value: Option<f64>| -> String {
+                value
+                    .map(|v| format!("{v:.1}"))
+                    .unwrap_or_else(|| "n/a".to_string())
+            };
+            let detail = format!(
+                "primary={}%, secondary={}%",
+                fmt_percent(snapshot.primary.as_ref().map(|window| window.used_percent)),
+                fmt_percent(
+                    snapshot
+                        .secondary
+                        .as_ref()
+                        .map(|window| window.used_percent)
+                )
+            );
+            self.log_display_event("status", "rate_limit_update", Some(&detail));
             self.rate_limit_snapshot = Some(display);
 
             if !warnings.is_empty() {
                 for warning in warnings {
+                    self.log_display_event("history", "rate_limit_warning", Some(&warning));
                     self.add_to_history(history_cell::new_warning_event(warning));
                 }
                 self.request_redraw();
@@ -512,6 +746,7 @@ impl ChatWidget {
     }
 
     fn on_error(&mut self, message: String) {
+        self.log_display_event("history", "error_event", Some(&message));
         self.finalize_turn();
         self.add_to_history(history_cell::new_error_event(message));
         self.request_redraw();
@@ -530,6 +765,8 @@ impl ChatWidget {
     /// separated by newlines rather than auto‑submitting the next one.
     fn on_interrupted_turn(&mut self, reason: TurnAbortReason) {
         // Finalize, log a gentle prompt, and clear running state.
+        let reason_detail = format!("{reason:?}");
+        self.log_display_event("history", "turn_interrupted", Some(&reason_detail));
         self.finalize_turn();
 
         if reason != TurnAbortReason::ReviewEnded {
@@ -564,6 +801,7 @@ impl ChatWidget {
     }
 
     fn on_plan_update(&mut self, update: UpdatePlanArgs) {
+        self.log_display_event("history", "plan_update", None);
         self.add_to_history(history_cell::new_plan_update(update));
     }
 
@@ -586,6 +824,7 @@ impl ChatWidget {
     }
 
     fn on_exec_command_begin(&mut self, ev: ExecCommandBeginEvent) {
+        self.finish_agentic_reasoning_capture();
         self.flush_answer_stream_with_separator();
         let ev2 = ev.clone();
         self.defer_or_handle(|q| q.push_exec_begin(ev), |s| s.handle_exec_begin_now(ev2));
@@ -678,6 +917,11 @@ impl ChatWidget {
 
     fn on_background_event(&mut self, message: String) {
         debug!("BackgroundEvent: {message}");
+        if should_suppress_background_event(&message) {
+            return;
+        }
+        self.add_to_history(history_cell::new_info_event(message, None));
+        self.request_redraw();
     }
 
     fn on_undo_started(&mut self, event: UndoStartedEvent) {
@@ -761,17 +1005,22 @@ impl ChatWidget {
 
     #[inline]
     fn handle_streaming_delta(&mut self, delta: String) {
+        if self.should_capture_agentic_reasoning() {
+            self.on_agent_reasoning_delta(delta);
+            return;
+        }
         // Before streaming agent content, flush any active exec cell group.
         self.flush_active_cell();
 
         if self.stream_controller.is_none() {
-            if self.needs_final_message_separator {
-                let elapsed_seconds = self
-                    .bottom_pane
-                    .status_widget()
-                    .map(super::status_indicator_widget::StatusIndicatorWidget::elapsed_seconds);
-                self.add_to_history(history_cell::FinalMessageSeparator::new(elapsed_seconds));
-                self.needs_final_message_separator = false;
+            self.log_display_event("history", "agent_stream_start", Some(&delta));
+        } else {
+            self.log_display_event("history", "agent_stream_delta", Some(&delta));
+        }
+
+        if self.stream_controller.is_none() {
+            if self.needs_history_divider {
+                self.insert_history_divider();
             }
             self.stream_controller = Some(StreamController::new(
                 self.last_rendered_width.get().map(|w| w.saturating_sub(2)),
@@ -985,12 +1234,15 @@ impl ChatWidget {
             enhanced_keys_supported,
             auth_manager,
             feedback,
+            model_presets,
+            auto_open_model_selector,
+            litellm_model_missing,
         } = common;
         let mut rng = rand::rng();
         let placeholder = EXAMPLE_PROMPTS[rng.random_range(0..EXAMPLE_PROMPTS.len())].to_string();
         let codex_op_tx = spawn_agent(config.clone(), app_event_tx.clone(), conversation_manager);
 
-        Self {
+        let mut this = Self {
             app_event_tx: app_event_tx.clone(),
             frame_requester: frame_requester.clone(),
             codex_op_tx,
@@ -1005,7 +1257,6 @@ impl ChatWidget {
             active_cell: None,
             config: config.clone(),
             auth_manager,
-            session_header: SessionHeader::new(config.model),
             initial_user_message: create_initial_user_message(
                 initial_prompt.unwrap_or_default(),
                 initial_images,
@@ -1027,11 +1278,31 @@ impl ChatWidget {
             suppress_session_configured_redraw: false,
             pending_notification: None,
             is_review_mode: false,
-            needs_final_message_separator: false,
+            needs_history_divider: false,
+            suppress_next_history_divider: false,
+            reasoning_summary_inserted: false,
+            final_separator_inserted: false,
+            pending_final_separator: None,
+            capture_agentic_reasoning: matches!(
+                config.model_family.reasoning_summary_format,
+                ReasoningSummaryFormat::Experimental
+            ),
             last_rendered_width: std::cell::Cell::new(None),
             feedback,
             current_rollout_path: None,
+            model_presets,
+            auto_open_model_selector,
+            litellm_model_missing,
+            litellm_model_missing_notified: false,
+            resumed_session: false,
+            resume_overflow_warned: false,
+        };
+
+        if this.litellm_model_missing {
+            this.mark_litellm_model_missing();
         }
+
+        this
     }
 
     /// Create a ChatWidget attached to an existing conversation (e.g., a fork).
@@ -1049,6 +1320,9 @@ impl ChatWidget {
             enhanced_keys_supported,
             auth_manager,
             feedback,
+            model_presets,
+            auto_open_model_selector,
+            litellm_model_missing,
         } = common;
         let mut rng = rand::rng();
         let placeholder = EXAMPLE_PROMPTS[rng.random_range(0..EXAMPLE_PROMPTS.len())].to_string();
@@ -1056,7 +1330,7 @@ impl ChatWidget {
         let codex_op_tx =
             spawn_agent_from_existing(conversation, session_configured, app_event_tx.clone());
 
-        Self {
+        let mut this = Self {
             app_event_tx: app_event_tx.clone(),
             frame_requester: frame_requester.clone(),
             codex_op_tx,
@@ -1071,7 +1345,6 @@ impl ChatWidget {
             active_cell: None,
             config: config.clone(),
             auth_manager,
-            session_header: SessionHeader::new(config.model),
             initial_user_message: create_initial_user_message(
                 initial_prompt.unwrap_or_default(),
                 initial_images,
@@ -1093,11 +1366,31 @@ impl ChatWidget {
             suppress_session_configured_redraw: true,
             pending_notification: None,
             is_review_mode: false,
-            needs_final_message_separator: false,
+            needs_history_divider: false,
+            suppress_next_history_divider: false,
+            reasoning_summary_inserted: false,
+            final_separator_inserted: false,
+            pending_final_separator: None,
+            capture_agentic_reasoning: matches!(
+                config.model_family.reasoning_summary_format,
+                ReasoningSummaryFormat::Experimental
+            ),
             last_rendered_width: std::cell::Cell::new(None),
             feedback,
             current_rollout_path: None,
+            model_presets,
+            auto_open_model_selector,
+            litellm_model_missing,
+            litellm_model_missing_notified: false,
+            resumed_session: false,
+            resume_overflow_warned: false,
+        };
+
+        if this.litellm_model_missing {
+            this.mark_litellm_model_missing();
         }
+
+        this
     }
 
     pub fn desired_height(&self, width: u16) -> u16 {
@@ -1350,7 +1643,9 @@ impl ChatWidget {
 
     fn flush_active_cell(&mut self) {
         if let Some(active) = self.active_cell.take() {
-            self.needs_final_message_separator = true;
+            if !self.suppress_next_history_divider {
+                self.needs_history_divider = true;
+            }
             self.app_event_tx.send(AppEvent::InsertHistoryCell(active));
         }
     }
@@ -1360,12 +1655,67 @@ impl ChatWidget {
     }
 
     fn add_boxed_history(&mut self, cell: Box<dyn HistoryCell>) {
+        if let Some(elapsed) = self.pending_final_separator.take() {
+            self.app_event_tx.send(AppEvent::InsertHistoryCell(Box::new(
+                history_cell::FinalMessageSeparator::new(elapsed),
+            )));
+            self.needs_history_divider = false;
+            self.suppress_next_history_divider = true;
+        }
         if !cell.display_lines(u16::MAX).is_empty() {
             // Only break exec grouping if the cell renders visible lines.
             self.flush_active_cell();
-            self.needs_final_message_separator = true;
         }
         self.app_event_tx.send(AppEvent::InsertHistoryCell(cell));
+        self.suppress_next_history_divider = false;
+    }
+
+    fn push_history_cell_direct(&mut self, cell: Box<dyn HistoryCell>) {
+        self.app_event_tx.send(AppEvent::InsertHistoryCell(cell));
+    }
+
+    fn insert_history_divider(&mut self) {
+        if self.suppress_next_history_divider {
+            self.needs_history_divider = false;
+            return;
+        }
+        self.needs_history_divider = false;
+        self.app_event_tx.send(AppEvent::InsertHistoryCell(Box::new(
+            history_cell::HistoryDivider::new(),
+        )));
+    }
+
+    fn insert_final_separator(&mut self) {
+        if self.final_separator_inserted {
+            return;
+        }
+        let elapsed_seconds = self
+            .bottom_pane
+            .status_widget()
+            .map(super::status_indicator_widget::StatusIndicatorWidget::elapsed_seconds)
+            .or(Some(0));
+        if let Some(elapsed) = elapsed_seconds {
+            let detail = super::status_indicator_widget::fmt_elapsed_compact(elapsed as u64);
+            self.log_display_event("history", "final_separator_pending", Some(&detail));
+        } else {
+            self.log_display_event("history", "final_separator_pending", None);
+        }
+        self.pending_final_separator = Some(elapsed_seconds);
+        self.final_separator_inserted = true;
+        self.needs_history_divider = false;
+        if self.stream_controller.is_none() {
+            if let Some(elapsed) = self.pending_final_separator.take() {
+                let detail = elapsed
+                    .map(super::status_indicator_widget::fmt_elapsed_compact)
+                    .unwrap_or_else(|| "unknown".to_string());
+                self.log_display_event("history", "final_separator_inserted", Some(&detail));
+                self.push_history_cell_direct(Box::new(history_cell::FinalMessageSeparator::new(
+                    elapsed,
+                )));
+                self.needs_history_divider = false;
+                self.suppress_next_history_divider = true;
+            }
+        }
     }
 
     fn submit_user_message(&mut self, user_message: UserMessage) {
@@ -1375,6 +1725,7 @@ impl ChatWidget {
         }
 
         let mut items: Vec<UserInput> = Vec::new();
+        let image_count = image_paths.len();
 
         // Special-case: "!cmd" executes a local shell command instead of sending to the model.
         if let Some(stripped) = text.strip_prefix('!') {
@@ -1395,9 +1746,19 @@ impl ChatWidget {
         }
 
         if !text.is_empty() {
+            self.log_display_event("history", "user_prompt_submitted", Some(&text));
             items.push(UserInput::Text { text: text.clone() });
         }
 
+        if image_count > 0 {
+            let image_detail = if image_count == 1 {
+                "1 image".to_string()
+            } else {
+                format!("{image_count} images")
+            };
+            self.log_display_event("history", "user_images_attached", Some(&image_detail));
+        }
+
         for path in image_paths {
             items.push(UserInput::LocalImage { path });
         }
@@ -1421,7 +1782,7 @@ impl ChatWidget {
         if !text.is_empty() {
             self.add_to_history(history_cell::new_user_prompt(text));
         }
-        self.needs_final_message_separator = false;
+        self.needs_history_divider = false;
     }
 
     /// Replay a subset of initial events into the UI to seed the transcript when
@@ -1600,7 +1961,12 @@ impl ChatWidget {
         }
     }
 
-    fn request_exit(&self) {
+    fn request_exit(&mut self) {
+        if let Some(conversation_id) = self.conversation_id.as_ref() {
+            let resume_cell = history_cell::new_resume_hint(conversation_id.to_string());
+            self.add_to_history(resume_cell);
+            self.request_redraw();
+        }
         self.app_event_tx.send(AppEvent::ExitRequest);
     }
 
@@ -1668,15 +2034,18 @@ impl ChatWidget {
     pub(crate) fn add_status_output(&mut self) {
         let default_usage = TokenUsage::default();
         let (total_usage, context_usage) = if let Some(ti) = &self.token_info {
-            (&ti.total_token_usage, Some(&ti.last_token_usage))
+            (&ti.total_token_usage, Some(&ti.total_token_usage))
         } else {
             (&default_usage, Some(&default_usage))
         };
+        let session_id = self.conversation_id.as_ref().map(|id| id.to_string());
+        let session_snapshot = session_telemetry::snapshot(session_id.as_deref());
         self.add_to_history(crate::status::new_status_output(
             &self.config,
             total_usage,
             context_usage,
             &self.conversation_id,
+            session_snapshot,
             self.rate_limit_snapshot.as_ref(),
             Local::now(),
         ));
@@ -1687,20 +2056,87 @@ impl ChatWidget {
     pub(crate) fn open_model_popup(&mut self) {
         let current_model = self.config.model.clone();
         let auth_mode = self.auth_manager.auth().map(|auth| auth.mode);
-        let presets: Vec<ModelPreset> = builtin_model_presets(auth_mode);
-
-        let mut items: Vec<SelectionItem> = Vec::new();
-        for preset in presets.into_iter() {
-            let description = if preset.description.is_empty() {
-                None
+        let (mut presets, preset_source): (Vec<ModelPreset>, &'static str) =
+            if is_litellm_provider_id(&self.config.model_provider_id) {
+                match read_litellm_provider_state(&self.config.codex_home)
+                    .map_err(|err| err.to_string())
+                    .and_then(|state| {
+                        let base_url = state
+                            .base_url
+                            .ok_or_else(|| "LiteLLM endpoint URL missing in config".to_string())?;
+                        fetch_litellm_model_presets(
+                            &base_url,
+                            state.api_key.as_deref(),
+                            &current_model,
+                        )
+                    }) {
+                    Ok(list) => {
+                        tracing::info!(
+                            target: "codex_litellm_debug::model_selection",
+                            provider = %self.config.model_provider_id,
+                            count = list.len(),
+                            "model_popup.presets_loaded"
+                        );
+                        (list, "litellm")
+                    }
+                    Err(err) => {
+                        tracing::warn!(
+                            target: "codex_litellm_debug::model_selection",
+                            provider = %self.config.model_provider_id,
+                            error = %err,
+                            "model_popup.presets_failed"
+                        );
+                        self.add_error_message(format!(
+                            "Unable to fetch LiteLLM model list ({err}). Falling back to defaults."
+                        ));
+                        (builtin_model_presets(auth_mode), "builtin")
+                    }
+                }
+            } else if self.model_presets.is_empty() {
+                let builtin = builtin_model_presets(auth_mode);
+                tracing::info!(
+                    target: "codex_litellm_debug::model_selection",
+                    provider = %self.config.model_provider_id,
+                    count = builtin.len(),
+                    "model_popup.presets_loaded"
+                );
+                (builtin, "builtin")
             } else {
-                Some(preset.description.to_string())
+                (self.model_presets.clone(), "cached")
             };
+
+        if let Some(default_index) = presets
+            .iter()
+            .position(|preset| preset.model == current_model)
+        {
+            for (idx, preset) in presets.iter_mut().enumerate() {
+                preset.is_default = idx == default_index;
+            }
+        } else if !presets.is_empty() {
+            presets[0].is_default = true;
+        }
+
+        self.model_presets = presets.clone();
+        tracing::info!(
+            target: "codex_litellm_debug::model_selection",
+            provider = %self.config.model_provider_id,
+            current = %current_model,
+            source = preset_source,
+            count = self.model_presets.len(),
+            "model_popup.opened"
+        );
+
+        let mut items: Vec<SelectionItem> = Vec::new();
+        for preset in presets.iter() {
+            let description = preset
+                .description
+                .as_ref()
+                .and_then(|text| (!text.is_empty()).then(|| text.to_string()));
             let is_current = preset.model == current_model;
-            let preset_for_action = preset;
+            let preset_for_action = preset.clone();
             let actions: Vec<SelectionAction> = vec![Box::new(move |tx| {
                 tx.send(AppEvent::OpenReasoningPopup {
-                    model: preset_for_action,
+                    model: preset_for_action.clone(),
                 });
             })];
             items.push(SelectionItem {
@@ -1754,6 +2190,13 @@ impl ChatWidget {
             .flatten()
             .or_else(|| choices.iter().find_map(|choice| choice.stored))
             .or(Some(default_effort));
+        tracing::info!(
+            target: "codex_litellm_debug::model_selection",
+            model = %preset.model,
+            total_options = choices.len(),
+            default = ?default_choice,
+            "reasoning_popup.opened"
+        );
 
         let model_slug = preset.model.to_string();
         let is_current_model = self.config.model == preset.model;
@@ -1810,11 +2253,10 @@ impl ChatWidget {
                     effort: effort_for_action,
                 });
                 tracing::info!(
-                    "Selected model: {}, Selected effort: {}",
-                    model_for_action,
-                    effort_for_action
-                        .map(|e| e.to_string())
-                        .unwrap_or_else(|| "default".to_string())
+                    target: "codex_litellm_debug::model_selection",
+                    model = %model_for_action,
+                    effort = ?effort_for_action,
+                    "model_selection.choice"
                 );
             })];
 
@@ -2066,16 +2508,24 @@ impl ChatWidget {
 
     /// Set the model in the widget's config copy.
     pub(crate) fn set_model(&mut self, model: &str) {
-        self.session_header.set_model(model);
         self.config.model = model.to_string();
+        for preset in self.model_presets.iter_mut() {
+            preset.is_default = preset.model == model;
+        }
+        self.request_redraw();
     }
 
     pub(crate) fn add_info_message(&mut self, message: String, hint: Option<String>) {
+        self.log_display_event("history", "info_message", Some(&message));
+        if let Some(hint) = hint.as_ref() {
+            self.log_display_event("history", "info_hint", Some(hint));
+        }
         self.add_to_history(history_cell::new_info_event(message, hint));
         self.request_redraw();
     }
 
     pub(crate) fn add_error_message(&mut self, message: String) {
+        self.log_display_event("history", "error_message", Some(&message));
         self.add_to_history(history_cell::new_error_event(message));
         self.request_redraw();
     }
@@ -2365,7 +2815,7 @@ impl ChatWidget {
 
 impl WidgetRef for &ChatWidget {
     fn render_ref(&self, area: Rect, buf: &mut Buffer) {
-        let [_, active_cell_area, bottom_pane_area] = self.layout_areas(area);
+        let [_header_area, active_cell_area, bottom_pane_area] = self.layout_areas(area);
         (&self.bottom_pane).render(bottom_pane_area, buf);
         if !active_cell_area.is_empty()
             && let Some(cell) = &self.active_cell
@@ -2458,6 +2908,30 @@ const EXAMPLE_PROMPTS: [&str; 6] = [
 
 // Extract the first bold (Markdown) element in the form **...** from `s`.
 // Returns the inner text if found; otherwise `None`.
+fn summarize_reasoning_snippet(delta: &str) -> Option<String> {
+    let trimmed = delta.trim();
+    if trimmed.is_empty() {
+        return None;
+    }
+    let mut words = trimmed.split_whitespace();
+    let mut summary_words = Vec::new();
+    for _ in 0..10 {
+        if let Some(word) = words.next() {
+            summary_words.push(word);
+        } else {
+            break;
+        }
+    }
+    if summary_words.is_empty() {
+        return None;
+    }
+    let mut summary = summary_words.join(" ");
+    if words.next().is_some() {
+        summary.push('…');
+    }
+    Some(summary)
+}
+
 fn extract_first_bold(s: &str) -> Option<String> {
     let bytes = s.as_bytes();
     let mut i = 0usize;
diff --git a/codex-rs/tui/src/chatwidget/session_header.rs b/codex-rs/tui/src/chatwidget/session_header.rs
deleted file mode 100644
index 32e31b668..000000000
--- a/codex-rs/tui/src/chatwidget/session_header.rs
+++ /dev/null
@@ -1,16 +0,0 @@
-pub(crate) struct SessionHeader {
-    model: String,
-}
-
-impl SessionHeader {
-    pub(crate) fn new(model: String) -> Self {
-        Self { model }
-    }
-
-    /// Updates the header's model text.
-    pub(crate) fn set_model(&mut self, model: &str) {
-        if self.model != model {
-            self.model = model.to_string();
-        }
-    }
-}
diff --git a/codex-rs/tui/src/chatwidget/snapshots/codex_tui__chatwidget__tests__approval_modal_exec.snap b/codex-rs/tui/src/chatwidget/snapshots/codex_tui__chatwidget__tests__approval_modal_exec.snap
index d0990fa93..6d873dcd5 100644
--- a/codex-rs/tui/src/chatwidget/snapshots/codex_tui__chatwidget__tests__approval_modal_exec.snap
+++ b/codex-rs/tui/src/chatwidget/snapshots/codex_tui__chatwidget__tests__approval_modal_exec.snap
@@ -1,5 +1,6 @@
 ---
 source: tui/src/chatwidget/tests.rs
+assertion_line: 1716
 expression: terminal.backend().vt100().screen().contents()
 ---
   Would you like to run the following command?
@@ -9,8 +10,8 @@ expression: terminal.backend().vt100().screen().contents()
 
   $ echo hello world
 
-› 1. Yes, proceed
-  2. Yes, and don't ask again for this command
-  3. No, and tell Codex what to do differently esc
+  › 1. Yes, proceed
+    2. Yes, and don't ask again for this command
+    3. No, and tell Codex what to do differently esc
 
   Press enter to confirm or esc to cancel
diff --git a/codex-rs/tui/src/chatwidget/snapshots/codex_tui__chatwidget__tests__approval_modal_exec_no_reason.snap b/codex-rs/tui/src/chatwidget/snapshots/codex_tui__chatwidget__tests__approval_modal_exec_no_reason.snap
index 3a557bf6c..96f46b98a 100644
--- a/codex-rs/tui/src/chatwidget/snapshots/codex_tui__chatwidget__tests__approval_modal_exec_no_reason.snap
+++ b/codex-rs/tui/src/chatwidget/snapshots/codex_tui__chatwidget__tests__approval_modal_exec_no_reason.snap
@@ -1,13 +1,14 @@
 ---
 source: tui/src/chatwidget/tests.rs
+assertion_line: 1749
 expression: terminal.backend().vt100().screen().contents()
 ---
   Would you like to run the following command?
 
   $ echo hello world
 
-› 1. Yes, proceed
-  2. Yes, and don't ask again for this command
-  3. No, and tell Codex what to do differently esc
+  › 1. Yes, proceed
+    2. Yes, and don't ask again for this command
+    3. No, and tell Codex what to do differently esc
 
   Press enter to confirm or esc to cancel
diff --git a/codex-rs/tui/src/chatwidget/snapshots/codex_tui__chatwidget__tests__approval_modal_patch.snap b/codex-rs/tui/src/chatwidget/snapshots/codex_tui__chatwidget__tests__approval_modal_patch.snap
index 96dde8fb9..d796fb160 100644
--- a/codex-rs/tui/src/chatwidget/snapshots/codex_tui__chatwidget__tests__approval_modal_patch.snap
+++ b/codex-rs/tui/src/chatwidget/snapshots/codex_tui__chatwidget__tests__approval_modal_patch.snap
@@ -1,5 +1,6 @@
 ---
 source: tui/src/chatwidget/tests.rs
+assertion_line: 1788
 expression: terminal.backend().vt100().screen().contents()
 ---
   Would you like to make the following edits?
@@ -11,7 +12,7 @@ expression: terminal.backend().vt100().screen().contents()
     1 +hello
     2 +world
 
-› 1. Yes, proceed
-  2. No, and tell Codex what to do differently esc
+  › 1. Yes, proceed
+    2. No, and tell Codex what to do differently esc
 
   Press enter to confirm or esc to cancel
diff --git a/codex-rs/tui/src/chatwidget/snapshots/codex_tui__chatwidget__tests__approvals_selection_popup.snap b/codex-rs/tui/src/chatwidget/snapshots/codex_tui__chatwidget__tests__approvals_selection_popup.snap
index 190594b1b..76fa30f7b 100644
--- a/codex-rs/tui/src/chatwidget/snapshots/codex_tui__chatwidget__tests__approvals_selection_popup.snap
+++ b/codex-rs/tui/src/chatwidget/snapshots/codex_tui__chatwidget__tests__approvals_selection_popup.snap
@@ -1,17 +1,18 @@
 ---
 source: tui/src/chatwidget/tests.rs
+assertion_line: 1347
 expression: popup
 ---
   Select Approval Mode
 
-› 1. Read Only (current)  Codex can read files and answer questions. Codex
-                          requires approval to make edits, run commands, or
-                          access network.
-  2. Auto                 Codex can read files, make edits, and run commands
-                          in the workspace. Codex requires approval to work
-                          outside the workspace or access network.
-  3. Full Access          Codex can read files, make edits, and run commands
-                          with network access, without approval. Exercise
-                          caution.
+  › 1. Read Only (current)  Codex can read files and answer questions. Codex
+                            requires approval to make edits, run commands, or
+                            access network.
+    2. Auto                 Codex can read files, make edits, and run commands
+                            in the workspace. Codex requires approval to work
+                            outside the workspace or access network.
+    3. Full Access          Codex can read files, make edits, and run commands
+                            with network access, without approval. Exercise
+                            caution.
 
   Press enter to confirm or esc to go back
diff --git a/codex-rs/tui/src/chatwidget/snapshots/codex_tui__chatwidget__tests__binary_size_ideal_response.snap b/codex-rs/tui/src/chatwidget/snapshots/codex_tui__chatwidget__tests__binary_size_ideal_response.snap
index 77738439a..8f0da610d 100644
--- a/codex-rs/tui/src/chatwidget/snapshots/codex_tui__chatwidget__tests__binary_size_ideal_response.snap
+++ b/codex-rs/tui/src/chatwidget/snapshots/codex_tui__chatwidget__tests__binary_size_ideal_response.snap
@@ -1,16 +1,8 @@
 ---
 source: tui/src/chatwidget/tests.rs
+assertion_line: 1738
 expression: "lines[start_idx..].join(\"\\n\")"
 ---
-• I need to check the codex-rs repository to explain why the project's binaries
-  are large. The user is likely seeking specifics about the setup: are Rust
-  builds static, what features are enabled, and is debug information included?
-  It could be due to static linking, included OpenSSL, or how panic handling
-  is set up. I should look into the Cargo.toml file to confirm features and
-  profiles without needing to edit any code. Let's get started on this!
-
-─ Worked for 0s ────────────────────────────────────────────────────────────────
-
 • I’m going to scan the workspace and Cargo manifests to see build profiles and
   dependencies that impact binary size. Then I’ll summarize the main causes.
 
@@ -18,14 +10,6 @@ expression: "lines[start_idx..].join(\"\\n\")"
   └ List ls -la
     Read Cargo.toml
 
-• I'm reviewing the workspace's release profile, which has settings like
-  lto=fat, strip=symbols, and codegen-units=1 to reduce binary size. However, it
-  seems that "strip = symbols" only removes symbols, not debug information. In
-  Rust 1.73+, both "symbols" and "debuginfo" stripping options are available. I
-  should check whether using these results in a smaller binary. I’ll also skim
-  through crates for heavy dependencies in Cargo.toml, including cli, core,
-  exec, linux-sandbox, tui, login, ollama, and mcp.
-
 • Ran for d in ansi-escape apply-patch arg0 cli common core exec execpolicy
   │ file-search linux-sandbox login mcp-client mcp-server mcp-types ollama
   │ tui; do echo "--- $d/Cargo.toml"; sed -n '1,200p' $d/Cargo.toml; echo;
@@ -39,79 +23,6 @@ expression: "lines[start_idx..].join(\"\\n\")"
 • Explored
   └ Read Cargo.toml
 
-• I’m evaluating how binary size is influenced by various factors. There are
-  many binaries, such as codex and codex-exec, which include dependencies by
-  default, pulling in features like Tokio and hyper. For instance, reqwest
-  includes default features like TLS through rustls. If default features remain
-  enabled, it inflates the size with many additional components. I notice
-  that for musl targets, using vendored builds of OpenSSL could significantly
-  increase binary size. This is quite the intricate web of dependencies!
-
-  Analyzing musl and binary size
-
-  I’m considering the implications of building for musl with static linking.
-  In this case, OpenSSL might come into play through some crates, but reqwest
-  defaults to using rustls, although openssl-sys vendored could increase
-  binary size. It’s interesting to think about whether they’re targeting musl,
-  especially since it seems they might often be building for Mac and Linux with
-  dynamic linking instead.
-
-  I also realize there’s a difference between debug and release builds — release
-  should be smaller due to optimizations, but if they’re defaulting to debug
-  builds, that could lead to much larger binaries.
-
-  Assessing crate impacts on size
-
-  I need to be precise when analyzing binary sizes from various crates. Some
-  crates, like ratatui-image, include heavy features such as libjpeg decoders,
-  which can really bulk things up. Then there are the tree-sitter crates that
-  contain large static grammar tables, adding significant size. The starlark
-  crate and tracing-subscriber also contribute to size increases.
-
-  With static linking per binary being standard in Rust, it leads to multiple
-  copies of the same dependencies unless cdylibs are used. That makes the
-  compile times longer, but using LTO helps reduce bloat somewhat through
-  deduplication.
-
-  Evaluating binary size factors
-
-  Having multiple binaries indeed increases the overall size significantly.
-  I'm considering options like upgrading to "opt-level = 'z'" or using
-  "strip=debuginfo" for reducing size. It's interesting that while the profile
-  only sets symbols for stripping, not debuginfo, this could inflate sizes if
-  retained in release builds.
-
-  The distinction between "symbols" and "debuginfo" is critical; "symbols"
-  keeps debug information which may not align with the goal of minimizing
-  size. I’d like to verify with the Rust documentation, but my network access
-  is restricted.
-
-  Explaining large binary sizes
-
-  I’m analyzing why binaries in this project are so large. The primary reasons
-  include monolithic static linking, where each binary includes the full
-  dependency graph without shared libraries. Heavy dependencies such as reqwest,
-  hyper, and TLS components also contribute significantly to size, along with
-  image decoders and other libraries.
-
-  Using MUSL targets means vendored OpenSSL can drastically increase the
-  size due to static linking. Furthermore, if the release profile keeps debug
-  information due to using "strip = symbols," that can lead to larger binaries
-  as well. It’s a mix of several factors, including panic strategies and
-  platform-specific linking behavior.
-
-  Identifying binary size contributors
-
-  I’m considering why binaries are still large even without static linking. The
-  tracing-subscriber crate potentially adds weight because it includes regex,
-  though it can sometimes use a lighter version. References from files like
-  "core/Cargo.toml" show dependencies like reqwest and tokio, while the tui uses
-  the image crate.
-
-  To present this clearly, I’ll format the final answer with two main sections:
-  "Main Causes" and "Build-Mode Notes." I can also include brief suggestions for
-  reducing size, but I want to stay focused on answering the user's question.
-
 ─ Worked for 0s ────────────────────────────────────────────────────────────────
 
 • Here’s what’s driving size in this workspace’s binaries.
@@ -151,3 +62,6 @@ expression: "lines[start_idx..].join(\"\\n\")"
   If you want, I can outline targeted trims (e.g., strip = "debuginfo", opt-
   level = "z", panic abort, tighter tokio/reqwest features) and estimate impact
   per binary.
+
+Session paused.
+To continue this session, run codex resume c7df96da-daec-4fe9-aed9-3cd19b7a6192
diff --git a/codex-rs/tui/src/chatwidget/snapshots/codex_tui__chatwidget__tests__exec_approval_modal_exec.snap b/codex-rs/tui/src/chatwidget/snapshots/codex_tui__chatwidget__tests__exec_approval_modal_exec.snap
index f0cf3a2d3..18088c97d 100644
--- a/codex-rs/tui/src/chatwidget/snapshots/codex_tui__chatwidget__tests__exec_approval_modal_exec.snap
+++ b/codex-rs/tui/src/chatwidget/snapshots/codex_tui__chatwidget__tests__exec_approval_modal_exec.snap
@@ -1,6 +1,6 @@
 ---
 source: tui/src/chatwidget/tests.rs
-assertion_line: 409
+assertion_line: 432
 expression: "format!(\"{buf:?}\")"
 ---
 Buffer {
@@ -15,9 +15,9 @@ Buffer {
         "                                                                                ",
         "  $ echo hello world                                                            ",
         "                                                                                ",
-        "› 1. Yes, proceed                                                               ",
-        "  2. Yes, and don't ask again for this command                                  ",
-        "  3. No, and tell Codex what to do differently esc                              ",
+        "  › 1. Yes, proceed                                                             ",
+        "    2. Yes, and don't ask again for this command                                ",
+        "    3. No, and tell Codex what to do differently esc                            ",
         "                                                                                ",
         "  Press enter to confirm or esc to cancel                                       ",
     ],
@@ -29,10 +29,10 @@ Buffer {
         x: 73, y: 4, fg: Reset, bg: Reset, underline: Reset, modifier: NONE,
         x: 2, y: 5, fg: Reset, bg: Reset, underline: Reset, modifier: ITALIC,
         x: 7, y: 5, fg: Reset, bg: Reset, underline: Reset, modifier: NONE,
-        x: 0, y: 9, fg: Cyan, bg: Reset, underline: Reset, modifier: BOLD,
-        x: 17, y: 9, fg: Reset, bg: Reset, underline: Reset, modifier: NONE,
-        x: 47, y: 11, fg: Reset, bg: Reset, underline: Reset, modifier: DIM,
-        x: 50, y: 11, fg: Reset, bg: Reset, underline: Reset, modifier: NONE,
+        x: 2, y: 9, fg: Cyan, bg: Reset, underline: Reset, modifier: BOLD,
+        x: 19, y: 9, fg: Reset, bg: Reset, underline: Reset, modifier: NONE,
+        x: 49, y: 11, fg: Reset, bg: Reset, underline: Reset, modifier: DIM,
+        x: 52, y: 11, fg: Reset, bg: Reset, underline: Reset, modifier: NONE,
         x: 2, y: 13, fg: Reset, bg: Reset, underline: Reset, modifier: DIM,
     ]
 }
diff --git a/codex-rs/tui/src/chatwidget/snapshots/codex_tui__chatwidget__tests__feedback_selection_popup.snap b/codex-rs/tui/src/chatwidget/snapshots/codex_tui__chatwidget__tests__feedback_selection_popup.snap
index 4a9824202..39bfe7212 100644
--- a/codex-rs/tui/src/chatwidget/snapshots/codex_tui__chatwidget__tests__feedback_selection_popup.snap
+++ b/codex-rs/tui/src/chatwidget/snapshots/codex_tui__chatwidget__tests__feedback_selection_popup.snap
@@ -1,11 +1,12 @@
 ---
 source: tui/src/chatwidget/tests.rs
+assertion_line: 1425
 expression: popup
 ---
   How was this?
 
-› 1. bug          Crash, error message, hang, or broken UI/behavior.
-  2. bad result   Output was off-target, incorrect, incomplete, or unhelpful.
-  3. good result  Helpful, correct, high‑quality, or delightful result worth
-                  celebrating.
-  4. other        Slowness, feature suggestion, UX feedback, or anything else.
+  › 1. bug          Crash, error message, hang, or broken UI/behavior.
+    2. bad result   Output was off-target, incorrect, incomplete, or
+                    unhelpful.
+    3. good result  Helpful, correct, high‑quality, or delightful result worth
+                    celebrating.
diff --git a/codex-rs/tui/src/chatwidget/snapshots/codex_tui__chatwidget__tests__feedback_upload_consent_popup.snap b/codex-rs/tui/src/chatwidget/snapshots/codex_tui__chatwidget__tests__feedback_upload_consent_popup.snap
index cc3d8e375..69d586d4f 100644
--- a/codex-rs/tui/src/chatwidget/snapshots/codex_tui__chatwidget__tests__feedback_upload_consent_popup.snap
+++ b/codex-rs/tui/src/chatwidget/snapshots/codex_tui__chatwidget__tests__feedback_upload_consent_popup.snap
@@ -1,5 +1,6 @@
 ---
 source: tui/src/chatwidget/tests.rs
+assertion_line: 1436
 expression: popup
 ---
   Upload logs?
@@ -7,8 +8,8 @@ expression: popup
   The following files will be sent:
     • codex-logs.log
 
-› 1. Yes  Share the current Codex session logs with the team for
-          troubleshooting.
-  2. No
+  › 1. Yes  Share the current Codex session logs with the team for
+            troubleshooting.
+    2. No
 
   Press enter to confirm or esc to go back
diff --git a/codex-rs/tui/src/chatwidget/snapshots/codex_tui__chatwidget__tests__full_access_confirmation_popup.snap b/codex-rs/tui/src/chatwidget/snapshots/codex_tui__chatwidget__tests__full_access_confirmation_popup.snap
index 71dac5f59..0f6eed5d2 100644
--- a/codex-rs/tui/src/chatwidget/snapshots/codex_tui__chatwidget__tests__full_access_confirmation_popup.snap
+++ b/codex-rs/tui/src/chatwidget/snapshots/codex_tui__chatwidget__tests__full_access_confirmation_popup.snap
@@ -1,5 +1,6 @@
 ---
 source: tui/src/chatwidget/tests.rs
+assertion_line: 1383
 expression: popup
 ---
   Enable full access?
@@ -8,8 +9,8 @@ expression: popup
   enabling full access. This significantly increases the risk of data loss,
   leaks, or unexpected behavior.
 
-› 1. Yes, continue anyway      Apply full access for this session
-  2. Yes, and don't ask again  Enable full access and remember this choice
-  3. Cancel                    Go back without enabling full access
+  › 1. Yes, continue anyway      Apply full access for this session
+    2. Yes, and don't ask again  Enable full access and remember this choice
+    3. Cancel                    Go back without enabling full access
 
   Press enter to confirm or esc to go back
diff --git a/codex-rs/tui/src/chatwidget/snapshots/codex_tui__chatwidget__tests__model_reasoning_selection_popup.snap b/codex-rs/tui/src/chatwidget/snapshots/codex_tui__chatwidget__tests__model_reasoning_selection_popup.snap
index d2ef858a6..045ebbdf6 100644
--- a/codex-rs/tui/src/chatwidget/snapshots/codex_tui__chatwidget__tests__model_reasoning_selection_popup.snap
+++ b/codex-rs/tui/src/chatwidget/snapshots/codex_tui__chatwidget__tests__model_reasoning_selection_popup.snap
@@ -1,14 +1,15 @@
 ---
 source: tui/src/chatwidget/tests.rs
+assertion_line: 1414
 expression: popup
 ---
   Select Reasoning Level for gpt-5-codex
 
-  1. Low               Fastest responses with limited reasoning
-  2. Medium (default)  Dynamically adjusts reasoning based on the task
-› 3. High (current)    Maximizes reasoning depth for complex or ambiguous
-                       problems
-                       ⚠ High reasoning effort can quickly consume Plus plan
-                       rate limits.
+    1. Low               Fastest responses with limited reasoning
+    2. Medium (default)  Dynamically adjusts reasoning based on the task
+  › 3. High (current)    Maximizes reasoning depth for complex or ambiguous
+                         problems
+                         ⚠ High reasoning effort can quickly consume Plus plan
+                         rate limits.
 
   Press enter to confirm or esc to go back
diff --git a/codex-rs/tui/src/chatwidget/snapshots/codex_tui__chatwidget__tests__model_selection_popup.snap b/codex-rs/tui/src/chatwidget/snapshots/codex_tui__chatwidget__tests__model_selection_popup.snap
index d4ca0491e..0727ec904 100644
--- a/codex-rs/tui/src/chatwidget/snapshots/codex_tui__chatwidget__tests__model_selection_popup.snap
+++ b/codex-rs/tui/src/chatwidget/snapshots/codex_tui__chatwidget__tests__model_selection_popup.snap
@@ -1,12 +1,13 @@
 ---
 source: tui/src/chatwidget/tests.rs
+assertion_line: 1331
 expression: popup
 ---
   Select Model and Effort
   Switch the model for this and future Codex CLI sessions
 
-› 1. gpt-5-codex (current)  Optimized for coding tasks with many tools.
-  2. gpt-5                  Broad world knowledge with strong general
-                            reasoning.
+  › 1. gpt-5-codex (current)  Optimized for coding tasks with many tools.
+    2. gpt-5                  Broad world knowledge with strong general
+                              reasoning.
 
   Press enter to select reasoning effort, or esc to dismiss.
diff --git a/codex-rs/tui/src/chatwidget/snapshots/codex_tui__chatwidget__tests__status_widget_and_approval_modal.snap b/codex-rs/tui/src/chatwidget/snapshots/codex_tui__chatwidget__tests__status_widget_and_approval_modal.snap
index 086a3a8c7..4be372e79 100644
--- a/codex-rs/tui/src/chatwidget/snapshots/codex_tui__chatwidget__tests__status_widget_and_approval_modal.snap
+++ b/codex-rs/tui/src/chatwidget/snapshots/codex_tui__chatwidget__tests__status_widget_and_approval_modal.snap
@@ -1,6 +1,6 @@
 ---
 source: tui/src/chatwidget/tests.rs
-assertion_line: 1548
+assertion_line: 1960
 expression: terminal.backend()
 ---
 "                                                                                "
@@ -12,8 +12,8 @@ expression: terminal.backend()
 "                                                                                "
 "  $ echo 'hello world'                                                          "
 "                                                                                "
-"› 1. Yes, proceed                                                               "
-"  2. Yes, and don't ask again for this command                                  "
-"  3. No, and tell Codex what to do differently esc                              "
+"  › 1. Yes, proceed                                                             "
+"    2. Yes, and don't ask again for this command                                "
+"    3. No, and tell Codex what to do differently esc                            "
 "                                                                                "
 "  Press enter to confirm or esc to cancel                                       "
diff --git a/codex-rs/tui/src/chatwidget/tests.rs b/codex-rs/tui/src/chatwidget/tests.rs
index 135dd2491..d68424733 100644
--- a/codex-rs/tui/src/chatwidget/tests.rs
+++ b/codex-rs/tui/src/chatwidget/tests.rs
@@ -5,12 +5,14 @@ use crate::test_backend::VT100Backend;
 use crate::tui::FrameRequester;
 use assert_matches::assert_matches;
 use codex_common::approval_presets::builtin_approval_presets;
+use codex_common::model_presets::builtin_model_presets;
 use codex_core::AuthManager;
 use codex_core::CodexAuth;
 use codex_core::config::Config;
 use codex_core::config::ConfigOverrides;
 use codex_core::config::ConfigToml;
 use codex_core::config::OPENAI_DEFAULT_MODEL;
+use codex_core::config::types::ReasoningSummaryFormat;
 use codex_core::protocol::AgentMessageDeltaEvent;
 use codex_core::protocol::AgentMessageEvent;
 use codex_core::protocol::AgentReasoningDeltaEvent;
@@ -34,6 +36,8 @@ use codex_core::protocol::ReviewRequest;
 use codex_core::protocol::StreamErrorEvent;
 use codex_core::protocol::TaskCompleteEvent;
 use codex_core::protocol::TaskStartedEvent;
+use codex_core::protocol::TokenUsage;
+use codex_core::protocol::TokenUsageInfo;
 use codex_core::protocol::UndoCompletedEvent;
 use codex_core::protocol::UndoStartedEvent;
 use codex_core::protocol::ViewImageToolCallEvent;
@@ -248,6 +252,9 @@ async fn helpers_are_available_and_do_not_panic() {
         enhanced_keys_supported: false,
         auth_manager,
         feedback: codex_feedback::CodexFeedback::new(),
+        model_presets: builtin_model_presets(None),
+        auto_open_model_selector: false,
+        litellm_model_missing: false,
     };
     let mut w = ChatWidget::new(init, conversation_manager);
     // Basic construction sanity.
@@ -280,7 +287,6 @@ fn make_chatwidget_manual() -> (
         active_cell: None,
         config: cfg.clone(),
         auth_manager,
-        session_header: SessionHeader::new(cfg.model),
         initial_user_message: None,
         token_info: None,
         rate_limit_snapshot: None,
@@ -300,10 +306,23 @@ fn make_chatwidget_manual() -> (
         suppress_session_configured_redraw: false,
         pending_notification: None,
         is_review_mode: false,
-        needs_final_message_separator: false,
+        needs_history_divider: false,
+        reasoning_summary_inserted: false,
+        final_separator_inserted: false,
+        pending_final_separator: None,
+        capture_agentic_reasoning: matches!(
+            cfg.model_family.reasoning_summary_format,
+            ReasoningSummaryFormat::Experimental
+        ),
         last_rendered_width: std::cell::Cell::new(None),
         feedback: codex_feedback::CodexFeedback::new(),
         current_rollout_path: None,
+        model_presets: builtin_model_presets(None),
+        auto_open_model_selector: false,
+        litellm_model_missing: false,
+        litellm_model_missing_notified: false,
+        resumed_session: false,
+        resume_overflow_warned: false,
     };
     (widget, rx, op_rx)
 }
@@ -863,6 +882,73 @@ fn slash_quit_requests_exit() {
     assert_matches!(rx.try_recv(), Ok(AppEvent::ExitRequest));
 }
 
+#[test]
+fn slash_quit_inserts_resume_hint_when_session_active() {
+    let (mut chat, mut rx, _op_rx) = make_chatwidget_manual();
+    chat.conversation_id = Some(codex_protocol::ConversationId::new());
+
+    chat.dispatch_command(SlashCommand::Quit);
+
+    match rx.try_recv() {
+        Ok(AppEvent::InsertHistoryCell(cell)) => {
+            let lines = cell.display_lines(80);
+            let rendered = lines_to_single_string(&lines);
+            assert!(
+                rendered.contains("codex resume"),
+                "resume hint should mention resume command: {rendered}"
+            );
+        }
+        other => panic!("expected resume hint event before exit, got {other:?}"),
+    }
+    assert_matches!(rx.try_recv(), Ok(AppEvent::ExitRequest));
+}
+
+#[test]
+fn resume_history_overflow_prompts_compact_once() {
+    let (mut chat, mut rx, _op_rx) = make_chatwidget_manual();
+    chat.conversation_id = Some(ConversationId::new());
+    chat.resumed_session = true;
+
+    let info = TokenUsageInfo {
+        total_token_usage: TokenUsage {
+            total_tokens: 140_000,
+            ..TokenUsage::default()
+        },
+        last_token_usage: TokenUsage {
+            total_tokens: 140_000,
+            ..TokenUsage::default()
+        },
+        model_context_window: Some(130_000),
+    };
+
+    chat.set_token_info(Some(info.clone()));
+
+    let cells = drain_insert_history(&mut rx);
+    assert!(
+        cells
+            .iter()
+            .any(|lines| lines_to_single_string(lines).contains("/compact")),
+        "expected resume warning urging /compact"
+    );
+    assert!(chat.resume_overflow_warned);
+    assert_eq!(
+        chat.bottom_pane.context_window_percent_for_tests(),
+        Some(0),
+        "context indicator should show 0% remaining after overflow warning"
+    );
+
+    chat.set_token_info(Some(info));
+    assert!(
+        rx.try_recv().is_err(),
+        "second overflow update should not emit duplicate warning"
+    );
+    assert_eq!(
+        chat.bottom_pane.context_window_percent_for_tests(),
+        Some(0),
+        "context indicator should stay at 0% after subsequent updates"
+    );
+}
+
 #[test]
 fn slash_exit_requests_exit() {
     let (mut chat, mut rx, _op_rx) = make_chatwidget_manual();
diff --git a/codex-rs/tui/src/cli.rs b/codex-rs/tui/src/cli.rs
index d86040b5c..0367bfc7b 100644
--- a/codex-rs/tui/src/cli.rs
+++ b/codex-rs/tui/src/cli.rs
@@ -1,3 +1,4 @@
+use clap::ArgAction;
 use clap::Parser;
 use clap::ValueHint;
 use codex_common::ApprovalModeCliArg;
@@ -38,6 +39,14 @@ pub struct Cli {
     #[arg(long = "oss", default_value_t = false)]
     pub oss: bool,
 
+    /// Force-enable telemetry writers for this run (overrides config).
+    #[arg(long = "telemetry", action = ArgAction::SetTrue, conflicts_with = "no_telemetry")]
+    pub telemetry: bool,
+
+    /// Disable all telemetry writers for this run (overrides config).
+    #[arg(long = "no-telemetry", action = ArgAction::SetTrue)]
+    pub no_telemetry: bool,
+
     /// Configuration profile from config.toml to specify default options.
     #[arg(long = "profile", short = 'p')]
     pub config_profile: Option<String>,
diff --git a/codex-rs/tui/src/history_cell.rs b/codex-rs/tui/src/history_cell.rs
index 73d61dc4e..c9e5fd012 100644
--- a/codex-rs/tui/src/history_cell.rs
+++ b/codex-rs/tui/src/history_cell.rs
@@ -556,14 +556,6 @@ pub(crate) fn new_session_info(
         rollout_path: _,
     } = event;
     if is_first_event {
-        // Header box rendered as history (so it appears at the very top)
-        let header = SessionHeaderHistoryCell::new(
-            model,
-            reasoning_effort,
-            config.cwd.clone(),
-            crate::version::CODEX_CLI_VERSION,
-        );
-
         // Help lines below the header (new copy and list)
         let help_lines: Vec<Line<'static>> = vec![
             "  To get started, describe a task or try one of these commands:"
@@ -597,6 +589,13 @@ pub(crate) fn new_session_info(
             ]),
         ];
 
+        let header = SessionHeaderHistoryCell::new(
+            model,
+            reasoning_effort,
+            config.cwd.clone(),
+            codex_common::litellm::decorate_version(crate::version::CODEX_CLI_VERSION),
+        );
+
         CompositeHistoryCell {
             parts: vec![
                 Box::new(header),
@@ -623,7 +622,7 @@ pub(crate) fn new_user_prompt(message: String) -> UserHistoryCell {
 
 #[derive(Debug)]
 struct SessionHeaderHistoryCell {
-    version: &'static str,
+    version: String,
     model: String,
     reasoning_effort: Option<ReasoningEffortConfig>,
     directory: PathBuf,
@@ -634,7 +633,7 @@ impl SessionHeaderHistoryCell {
         model: String,
         reasoning_effort: Option<ReasoningEffortConfig>,
         directory: PathBuf,
-        version: &'static str,
+        version: String,
     ) -> Self {
         Self {
             version,
@@ -1229,6 +1228,19 @@ pub(crate) fn new_info_event(message: String, hint: Option<String>) -> PlainHist
     PlainHistoryCell { lines }
 }
 
+pub(crate) fn new_resume_hint(session_id: String) -> PlainHistoryCell {
+    let resume_cmd = format!("codex resume {session_id}");
+    let lines: Vec<Line<'static>> = vec![
+        vec!["Session paused.".bold().yellow()].into(),
+        vec![
+            "To continue this session, run ".dim(),
+            resume_cmd.clone().cyan(),
+        ]
+        .into(),
+    ];
+    PlainHistoryCell { lines }
+}
+
 pub(crate) fn new_error_event(message: String) -> PlainHistoryCell {
     // Use a hair space (U+200A) to create a subtle, near-invisible separation
     // before the text. VS16 is intentionally omitted to keep spacing tighter
@@ -1356,6 +1368,7 @@ pub(crate) fn new_view_image_tool_call(path: PathBuf, cwd: &Path) -> PlainHistor
 pub(crate) fn new_reasoning_summary_block(
     full_reasoning_buffer: String,
     config: &Config,
+    transcript_only: bool,
 ) -> Box<dyn HistoryCell> {
     if config.model_family.reasoning_summary_format == ReasoningSummaryFormat::Experimental {
         // Experimental format is following:
@@ -1377,7 +1390,7 @@ pub(crate) fn new_reasoning_summary_block(
                     return Box::new(ReasoningSummaryCell::new(
                         header_buffer,
                         summary_buffer,
-                        false,
+                        transcript_only,
                     ));
                 }
             }
@@ -1386,7 +1399,7 @@ pub(crate) fn new_reasoning_summary_block(
     Box::new(ReasoningSummaryCell::new(
         "".to_string(),
         full_reasoning_buffer,
-        true,
+        transcript_only,
     ))
 }
 
@@ -1420,6 +1433,33 @@ impl HistoryCell for FinalMessageSeparator {
     }
 }
 
+#[derive(Debug)]
+pub struct HistoryDivider;
+
+impl HistoryDivider {
+    pub(crate) fn new() -> Self {
+        Self
+    }
+}
+
+impl HistoryCell for HistoryDivider {
+    fn display_lines(&self, width: u16) -> Vec<Line<'static>> {
+        vec![Line::from_iter(["─".repeat(width as usize).dim()])]
+    }
+
+    fn desired_height(&self, width: u16) -> u16 {
+        self.display_lines(width).len() as u16
+    }
+
+    fn transcript_lines(&self, width: u16) -> Vec<Line<'static>> {
+        self.display_lines(width)
+    }
+
+    fn desired_transcript_height(&self, width: u16) -> u16 {
+        self.display_lines(width).len() as u16
+    }
+}
+
 fn format_mcp_invocation<'a>(invocation: McpInvocation) -> Line<'a> {
     let args_str = invocation
         .arguments
@@ -1797,7 +1837,7 @@ mod tests {
             "gpt-4o".to_string(),
             Some(ReasoningEffortConfig::High),
             std::env::temp_dir(),
-            "test",
+            "test".to_string(),
         );
 
         let lines = render_lines(&cell.display_lines(80));
@@ -2218,6 +2258,7 @@ mod tests {
         let cell = new_reasoning_summary_block(
             "**High level reasoning**\n\nDetailed reasoning goes here.".to_string(),
             &config,
+            false,
         );
 
         let rendered_display = render_lines(&cell.display_lines(80));
@@ -2232,8 +2273,11 @@ mod tests {
         let mut config = test_config();
         config.model_family.reasoning_summary_format = ReasoningSummaryFormat::Experimental;
 
-        let cell =
-            new_reasoning_summary_block("Detailed reasoning goes here.".to_string(), &config);
+        let cell = new_reasoning_summary_block(
+            "Detailed reasoning goes here.".to_string(),
+            &config,
+            false,
+        );
 
         let rendered = render_transcript(cell.as_ref());
         assert_eq!(rendered, vec!["• Detailed reasoning goes here."]);
@@ -2247,6 +2291,7 @@ mod tests {
         let cell = new_reasoning_summary_block(
             "**High level reasoning without closing".to_string(),
             &config,
+            false,
         );
 
         let rendered = render_transcript(cell.as_ref());
@@ -2261,6 +2306,7 @@ mod tests {
         let cell = new_reasoning_summary_block(
             "**High level reasoning without closing**".to_string(),
             &config,
+            false,
         );
 
         let rendered = render_transcript(cell.as_ref());
@@ -2269,6 +2315,7 @@ mod tests {
         let cell = new_reasoning_summary_block(
             "**High level reasoning without closing**\n\n  ".to_string(),
             &config,
+            false,
         );
 
         let rendered = render_transcript(cell.as_ref());
@@ -2283,6 +2330,7 @@ mod tests {
         let cell = new_reasoning_summary_block(
             "**High level plan**\n\nWe should fix the bug next.".to_string(),
             &config,
+            false,
         );
 
         let rendered_display = render_lines(&cell.display_lines(80));
diff --git a/codex-rs/tui/src/lib.rs b/codex-rs/tui/src/lib.rs
index 99312ec19..717c28d89 100644
--- a/codex-rs/tui/src/lib.rs
+++ b/codex-rs/tui/src/lib.rs
@@ -7,6 +7,7 @@ use additional_dirs::add_dir_warning_message;
 use app::App;
 pub use app::AppExitInfo;
 use codex_app_server_protocol::AuthMode;
+use codex_common::model_presets::{ModelPreset, is_litellm_provider_id};
 use codex_core::AuthManager;
 use codex_core::BUILT_IN_OSS_MODEL_PROVIDER_ID;
 use codex_core::CodexAuth;
@@ -18,16 +19,19 @@ use codex_core::config::ConfigOverrides;
 use codex_core::find_conversation_path_by_id_str;
 use codex_core::get_platform_sandbox;
 use codex_core::protocol::AskForApproval;
+use codex_litellm_model_session_telemetry as session_telemetry;
 use codex_ollama::DEFAULT_OSS_MODEL;
 use codex_protocol::config_types::SandboxMode;
 use opentelemetry_appender_tracing::layer::OpenTelemetryTracingBridge;
 use std::fs::OpenOptions;
 use std::path::PathBuf;
-use tracing::error;
+use tracing::{debug, error, warn};
 use tracing_appender::non_blocking;
+use tracing_appender::non_blocking::{NonBlocking, WorkerGuard};
 use tracing_subscriber::EnvFilter;
-use tracing_subscriber::filter::Targets;
+use tracing_subscriber::fmt;
 use tracing_subscriber::prelude::*;
+use tracing_subscriber::registry::LookupSpan;
 
 mod additional_dirs;
 mod app;
@@ -159,6 +163,13 @@ pub async fn run_main(
         tools_web_search_request: None,
         experimental_sandbox_command_assessment: None,
         additional_writable_roots: additional_dirs,
+        telemetry_enabled: if cli.telemetry {
+            Some(true)
+        } else if cli.no_telemetry {
+            Some(false)
+        } else {
+            None
+        },
     };
     let raw_overrides = cli.config_overrides.raw_overrides.clone();
     let overrides_cli = codex_common::CliConfigOverrides { raw_overrides };
@@ -188,48 +199,142 @@ pub async fn run_main(
     }
 
     let active_profile = config.active_profile.clone();
-    let log_dir = codex_core::config::log_dir(&config)?;
-    std::fs::create_dir_all(&log_dir)?;
-    // Open (or create) your log file, appending to it.
-    let mut log_file_opts = OpenOptions::new();
-    log_file_opts.create(true).append(true);
-
-    // Ensure the file is only readable and writable by the current user.
-    // Doing the equivalent to `chmod 600` on Windows is quite a bit more code
-    // and requires the Windows API crates, so we can reconsider that when
-    // Codex CLI is officially supported on Windows.
-    #[cfg(unix)]
-    {
-        use std::os::unix::fs::OpenOptionsExt;
-        log_file_opts.mode(0o600);
-    }
+    let mut tracing_guards: Vec<WorkerGuard> = Vec::new();
+    let (file_writer, debug_writer) = if config.telemetry.enabled {
+        if let Err(err) = std::fs::create_dir_all(&config.telemetry.dir) {
+            warn!(
+                "telemetry: failed to create telemetry directory {}: {err}",
+                config.telemetry.dir.display()
+            );
+        }
 
-    let log_file = log_file_opts.open(log_dir.join("codex-tui.log"))?;
+        let file_writer = codex_core::config::telemetry_log_config(&config, "tui")
+            .cloned()
+            .and_then(|log_cfg| {
+                if !log_cfg.enabled {
+                    return None;
+                }
+                if let Some(parent) = log_cfg.path.parent() {
+                    if let Err(err) = std::fs::create_dir_all(parent) {
+                        warn!(
+                            "telemetry: failed to create TUI log directory {}: {err}",
+                            parent.display()
+                        );
+                        return None;
+                    }
+                }
+                let mut log_file_opts = OpenOptions::new();
+                log_file_opts.create(true).append(true);
+                #[cfg(unix)]
+                {
+                    use std::os::unix::fs::OpenOptionsExt;
+                    log_file_opts.mode(0o600);
+                }
+                match log_file_opts.open(&log_cfg.path) {
+                    Ok(log_file) => {
+                        let (writer, guard) = non_blocking(log_file);
+                        tracing_guards.push(guard);
+                        debug!(
+                            "telemetry: writing Codex TUI log to {}",
+                            log_cfg.path.display()
+                        );
+                        Some(writer)
+                    }
+                    Err(err) => {
+                        warn!(
+                            "telemetry: failed to open TUI log {}: {err}",
+                            log_cfg.path.display()
+                        );
+                        None
+                    }
+                }
+            });
 
-    // Wrap file in non‑blocking writer.
-    let (non_blocking, _guard) = non_blocking(log_file);
+        let debug_writer = codex_core::config::telemetry_log_config(&config, "debug")
+            .cloned()
+            .and_then(|log_cfg| {
+                if !log_cfg.enabled {
+                    return None;
+                }
+                if let Some(parent) = log_cfg.path.parent() {
+                    if let Err(err) = std::fs::create_dir_all(parent) {
+                        warn!(
+                            "telemetry: failed to create debug log directory {}: {err}",
+                            parent.display()
+                        );
+                        return None;
+                    }
+                }
+                let mut log_file_opts = OpenOptions::new();
+                log_file_opts.create(true).append(true);
+                #[cfg(unix)]
+                {
+                    use std::os::unix::fs::OpenOptionsExt;
+                    log_file_opts.mode(0o600);
+                }
+                match log_file_opts.open(&log_cfg.path) {
+                    Ok(log_file) => {
+                        let (writer, guard) = non_blocking(log_file);
+                        tracing_guards.push(guard);
+                        debug!(
+                            "telemetry: recording debug events to {}",
+                            log_cfg.path.display()
+                        );
+                        Some(writer)
+                    }
+                    Err(err) => {
+                        warn!(
+                            "telemetry: failed to open debug log {}: {err}",
+                            log_cfg.path.display()
+                        );
+                        None
+                    }
+                }
+            });
 
-    // use RUST_LOG env var, default to info for codex crates.
-    let env_filter = || {
-        EnvFilter::try_from_default_env().unwrap_or_else(|_| {
-            EnvFilter::new("codex_core=info,codex_tui=info,codex_rmcp_client=info")
-        })
+        if let Some(session_cfg) =
+            codex_core::config::telemetry_log_config(&config, "session").cloned()
+        {
+            if session_cfg.enabled {
+                if let Some(parent) = session_cfg.path.parent() {
+                    if let Err(err) = std::fs::create_dir_all(parent) {
+                        warn!(
+                            "telemetry: failed to create session log directory {}: {err}",
+                            parent.display()
+                        );
+                    }
+                }
+                match session_telemetry::configure_log_file(Some(session_cfg.path.clone())) {
+                    Ok(()) => debug!(
+                        "telemetry: recording LiteLLM session usage to {}",
+                        session_cfg.path.display()
+                    ),
+                    Err(err) => warn!(
+                        "telemetry: failed to open LiteLLM session log {}: {err}",
+                        session_cfg.path.display()
+                    ),
+                }
+            } else if let Err(err) = session_telemetry::configure_log_file(None) {
+                warn!("telemetry: failed to disable session telemetry logging: {err}");
+            }
+        } else if let Err(err) = session_telemetry::configure_log_file(None) {
+            warn!("telemetry: failed to disable session telemetry logging: {err}");
+        }
+
+        (file_writer, debug_writer)
+    } else {
+        if let Err(err) = session_telemetry::configure_log_file(None) {
+            warn!("telemetry: failed to disable session telemetry logging: {err}");
+        }
+        (None, None)
     };
 
-    let file_layer = tracing_subscriber::fmt::layer()
-        .with_writer(non_blocking)
-        .with_target(false)
-        .with_span_events(tracing_subscriber::fmt::format::FmtSpan::CLOSE)
-        .with_filter(env_filter());
+    // use RUST_LOG env var, default to info for codex crates.
+    let env_filter = EnvFilter::try_from_default_env().unwrap_or_else(|_| {
+        EnvFilter::new("codex_core=info,codex_tui=info,codex_rmcp_client=info")
+    });
 
     let feedback = codex_feedback::CodexFeedback::new();
-    let targets = Targets::new().with_default(tracing::Level::TRACE);
-
-    let feedback_layer = tracing_subscriber::fmt::layer()
-        .with_writer(feedback.make_writer())
-        .with_ansi(false)
-        .with_target(false)
-        .with_filter(targets);
 
     if cli.oss {
         codex_ollama::ensure_oss_ready(&config)
@@ -248,22 +353,72 @@ pub async fn run_main(
         }
     };
 
-    if let Some(provider) = otel.as_ref() {
-        let otel_layer = OpenTelemetryTracingBridge::new(&provider.logger).with_filter(
-            tracing_subscriber::filter::filter_fn(codex_core::otel_init::codex_export_filter),
-        );
-
-        let _ = tracing_subscriber::registry()
-            .with(file_layer)
-            .with(feedback_layer)
-            .with(otel_layer)
-            .try_init();
-    } else {
-        let _ = tracing_subscriber::registry()
-            .with(file_layer)
-            .with(feedback_layer)
-            .try_init();
-    };
+    match (file_writer, debug_writer) {
+        (Some(file_writer), Some(debug_writer)) => {
+            let base = tracing_subscriber::registry()
+                .with(env_filter.clone())
+                .with(tui_file_layer_for(file_writer))
+                .with(debug_layer_for(debug_writer))
+                .with(feedback_layer_for(&feedback));
+            if let Some(provider) = otel.as_ref() {
+                let otel_layer = OpenTelemetryTracingBridge::new(&provider.logger).with_filter(
+                    tracing_subscriber::filter::filter_fn(
+                        codex_core::otel_init::codex_export_filter,
+                    ),
+                );
+                let _ = base.with(otel_layer).try_init();
+            } else {
+                let _ = base.try_init();
+            }
+        }
+        (Some(file_writer), None) => {
+            let base = tracing_subscriber::registry()
+                .with(env_filter.clone())
+                .with(tui_file_layer_for(file_writer))
+                .with(feedback_layer_for(&feedback));
+            if let Some(provider) = otel.as_ref() {
+                let otel_layer = OpenTelemetryTracingBridge::new(&provider.logger).with_filter(
+                    tracing_subscriber::filter::filter_fn(
+                        codex_core::otel_init::codex_export_filter,
+                    ),
+                );
+                let _ = base.with(otel_layer).try_init();
+            } else {
+                let _ = base.try_init();
+            }
+        }
+        (None, Some(debug_writer)) => {
+            let base = tracing_subscriber::registry()
+                .with(env_filter.clone())
+                .with(debug_layer_for(debug_writer))
+                .with(feedback_layer_for(&feedback));
+            if let Some(provider) = otel.as_ref() {
+                let otel_layer = OpenTelemetryTracingBridge::new(&provider.logger).with_filter(
+                    tracing_subscriber::filter::filter_fn(
+                        codex_core::otel_init::codex_export_filter,
+                    ),
+                );
+                let _ = base.with(otel_layer).try_init();
+            } else {
+                let _ = base.try_init();
+            }
+        }
+        (None, None) => {
+            let base = tracing_subscriber::registry()
+                .with(env_filter.clone())
+                .with(feedback_layer_for(&feedback));
+            if let Some(provider) = otel.as_ref() {
+                let otel_layer = OpenTelemetryTracingBridge::new(&provider.logger).with_filter(
+                    tracing_subscriber::filter::filter_fn(
+                        codex_core::otel_init::codex_export_filter,
+                    ),
+                );
+                let _ = base.with(otel_layer).try_init();
+            } else {
+                let _ = base.try_init();
+            }
+        }
+    }
 
     run_ratatui_app(
         cli,
@@ -277,6 +432,47 @@ pub async fn run_main(
     .map_err(|err| std::io::Error::other(err.to_string()))
 }
 
+fn feedback_layer_for<S>(
+    feedback: &codex_feedback::CodexFeedback,
+) -> fmt::Layer<
+    S,
+    fmt::format::DefaultFields,
+    fmt::format::Format<fmt::format::Full>,
+    codex_feedback::FeedbackMakeWriter,
+>
+where
+    S: tracing::Subscriber + for<'a> LookupSpan<'a>,
+{
+    fmt::Layer::default()
+        .with_writer(feedback.make_writer())
+        .with_ansi(false)
+        .with_target(false)
+}
+
+fn tui_file_layer_for<S>(
+    writer: NonBlocking,
+) -> fmt::Layer<S, fmt::format::DefaultFields, fmt::format::Format<fmt::format::Full>, NonBlocking>
+where
+    S: tracing::Subscriber + for<'a> LookupSpan<'a>,
+{
+    fmt::Layer::default()
+        .with_writer(writer)
+        .with_target(false)
+        .with_span_events(tracing_subscriber::fmt::format::FmtSpan::CLOSE)
+}
+
+fn debug_layer_for<S>(
+    writer: NonBlocking,
+) -> fmt::Layer<S, fmt::format::DefaultFields, fmt::format::Format<fmt::format::Full>, NonBlocking>
+where
+    S: tracing::Subscriber + for<'a> LookupSpan<'a>,
+{
+    fmt::Layer::default()
+        .with_writer(writer)
+        .with_ansi(false)
+        .with_target(true)
+}
+
 async fn run_ratatui_app(
     cli: Cli,
     initial_config: Config,
@@ -339,6 +535,17 @@ async fn run_ratatui_app(
         should_show_trust_screen,
         should_show_windows_wsl_screen,
     );
+    tracing::info!(
+        target: "codex_litellm_debug::onboarding",
+        provider = %initial_config.model_provider_id,
+        model = %initial_config.model,
+        litellm_setup_required = initial_config.litellm_setup_required,
+        should_show = should_show_onboarding,
+        "onboarding.preflight"
+    );
+
+    let mut litellm_credentials_updated = false;
+    let mut litellm_model_selected = false;
 
     let config = if should_show_onboarding {
         let onboarding_result = run_onboarding_app(
@@ -366,12 +573,17 @@ async fn run_ratatui_app(
                 update_action: None,
             });
         }
+        litellm_credentials_updated = onboarding_result.litellm_credentials_updated;
+        litellm_model_selected = onboarding_result.litellm_model_selected;
         // if the user acknowledged windows or made an explicit decision ato trust the directory, reload the config accordingly
+        let reload_for_litellm = onboarding_result.litellm_credentials_updated
+            || onboarding_result.litellm_model_selected;
         if should_show_windows_wsl_screen
             || onboarding_result
                 .directory_trust_decision
                 .map(|d| d == TrustDirectorySelection::Trust)
                 .unwrap_or(false)
+            || reload_for_litellm
         {
             load_config_or_exit(cli_kv_overrides, overrides).await
         } else {
@@ -380,6 +592,33 @@ async fn run_ratatui_app(
     } else {
         initial_config
     };
+    tracing::info!(
+        target: "codex_litellm_debug::onboarding",
+        provider = %config.model_provider_id,
+        model = %config.model,
+        litellm_setup_required = config.litellm_setup_required,
+        "onboarding.post_config_load"
+    );
+
+    let auth_mode = auth_manager.auth().map(|auth| auth.mode);
+    let mut model_presets: Vec<ModelPreset> = codex_common::model_presets::presets_for_provider(
+        &config.model_provider_id,
+        &config.model,
+        auth_mode,
+    );
+
+    if is_litellm_provider_id(&config.model_provider_id)
+        && config.litellm_setup_required
+        && model_presets.is_empty()
+    {
+        model_presets.push(codex_common::model_presets::make_litellm_model_preset(
+            &config.model,
+            true,
+        ));
+    }
+    let auto_open_model_selector =
+        !litellm_model_selected && (litellm_credentials_updated || config.litellm_setup_required);
+    let litellm_model_missing = false;
 
     // Determine resume behavior: explicit id, then resume last, then picker.
     let resume_selection = if let Some(id_str) = cli.resume_session_id.as_deref() {
@@ -456,6 +695,9 @@ async fn run_ratatui_app(
         images,
         resume_selection,
         feedback,
+        model_presets,
+        auto_open_model_selector,
+        litellm_model_missing,
     )
     .await;
 
@@ -546,12 +788,21 @@ fn should_show_onboarding(
         return true;
     }
 
+    if is_litellm_provider_id(&config.model_provider_id)
+        && (config.litellm_setup_required || config.model.trim().is_empty())
+    {
+        return true;
+    }
+
     should_show_login_screen(login_status, config)
 }
 
 fn should_show_login_screen(login_status: LoginStatus, config: &Config) -> bool {
     // Only show the login screen for providers that actually require OpenAI auth
     // (OpenAI or equivalents). For OSS/other providers, skip login entirely.
+    if config.litellm_setup_required {
+        return true;
+    }
     if !config.model_provider.requires_openai_auth {
         return false;
     }
diff --git a/codex-rs/tui/src/onboarding/auth.rs b/codex-rs/tui/src/onboarding/auth.rs
index 06e1c629e..1f61df946 100644
--- a/codex-rs/tui/src/onboarding/auth.rs
+++ b/codex-rs/tui/src/onboarding/auth.rs
@@ -1,10 +1,12 @@
 #![allow(clippy::unwrap_used)]
 
+use codex_common::litellm::{LITELLM_API_KEY_ENV, LITELLM_BASE_URL_ENV};
 use codex_core::AuthManager;
 use codex_core::auth::AuthCredentialsStoreMode;
 use codex_core::auth::CLIENT_ID;
-use codex_core::auth::login_with_api_key;
-use codex_core::auth::read_openai_api_key_from_env;
+use codex_core::config::{
+    LiteLlmProviderUpdate, read_litellm_provider_state, write_litellm_provider_state,
+};
 use codex_login::ServerOptions;
 use codex_login::ShutdownHandle;
 use codex_login::run_login_server;
@@ -31,6 +33,7 @@ use ratatui::widgets::Wrap;
 
 use codex_app_server_protocol::AuthMode;
 use codex_protocol::config_types::ForcedLoginMethod;
+use std::env;
 use std::sync::RwLock;
 
 use crate::LoginStatus;
@@ -46,19 +49,32 @@ use super::onboarding_screen::StepState;
 #[derive(Clone)]
 pub(crate) enum SignInState {
     PickMode,
+    #[allow(dead_code)]
     ChatGptContinueInBrowser(ContinueInBrowserState),
+    #[allow(dead_code)]
     ChatGptSuccessMessage,
+    #[allow(dead_code)]
     ChatGptSuccess,
     ApiKeyEntry(ApiKeyInputState),
     ApiKeyConfigured,
 }
 
-const API_KEY_DISABLED_MESSAGE: &str = "API key login is disabled.";
+const API_KEY_DISABLED_MESSAGE: &str = "LiteLLM configuration is disabled.";
 
 #[derive(Clone, Default)]
 pub(crate) struct ApiKeyInputState {
-    value: String,
-    prepopulated_from_env: bool,
+    base_url: String,
+    api_key: String,
+    stage: ApiKeyEntryStage,
+    base_url_prepopulated: bool,
+    api_key_prepopulated: bool,
+}
+
+#[derive(Clone, Copy, Default, PartialEq, Eq)]
+enum ApiKeyEntryStage {
+    #[default]
+    BaseUrl,
+    ApiKey,
 }
 
 #[derive(Clone)]
@@ -78,54 +94,26 @@ impl Drop for ContinueInBrowserState {
 
 impl KeyboardHandler for AuthModeWidget {
     fn handle_key_event(&mut self, key_event: KeyEvent) {
+        if key_event.kind != KeyEventKind::Press && key_event.kind != KeyEventKind::Repeat {
+            return;
+        }
         if self.handle_api_key_entry_key_event(&key_event) {
             return;
         }
 
         match key_event.code {
-            KeyCode::Up | KeyCode::Char('k') => {
-                if self.is_chatgpt_login_allowed() {
-                    self.highlighted_mode = AuthMode::ChatGPT;
-                }
-            }
-            KeyCode::Down | KeyCode::Char('j') => {
+            KeyCode::Up | KeyCode::Char('k') | KeyCode::Down | KeyCode::Char('j') => {
                 if self.is_api_login_allowed() {
                     self.highlighted_mode = AuthMode::ApiKey;
                 }
             }
-            KeyCode::Char('1') => {
-                if self.is_chatgpt_login_allowed() {
-                    self.start_chatgpt_login();
-                }
-            }
-            KeyCode::Char('2') => {
+            KeyCode::Char('1') | KeyCode::Char('2') | KeyCode::Enter => {
                 if self.is_api_login_allowed() {
                     self.start_api_key_entry();
                 } else {
                     self.disallow_api_login();
                 }
             }
-            KeyCode::Enter => {
-                let sign_in_state = { (*self.sign_in_state.read().unwrap()).clone() };
-                match sign_in_state {
-                    SignInState::PickMode => match self.highlighted_mode {
-                        AuthMode::ChatGPT if self.is_chatgpt_login_allowed() => {
-                            self.start_chatgpt_login();
-                        }
-                        AuthMode::ApiKey if self.is_api_login_allowed() => {
-                            self.start_api_key_entry();
-                        }
-                        AuthMode::ChatGPT => {}
-                        AuthMode::ApiKey => {
-                            self.disallow_api_login();
-                        }
-                    },
-                    SignInState::ChatGptSuccessMessage => {
-                        *self.sign_in_state.write().unwrap() = SignInState::ChatGptSuccess;
-                    }
-                    _ => {}
-                }
-            }
             KeyCode::Esc => {
                 tracing::info!("Esc pressed");
                 let sign_in_state = { (*self.sign_in_state.read().unwrap()).clone() };
@@ -150,11 +138,13 @@ pub(crate) struct AuthModeWidget {
     pub error: Option<String>,
     pub sign_in_state: Arc<RwLock<SignInState>>,
     pub codex_home: PathBuf,
-    pub cli_auth_credentials_store_mode: AuthCredentialsStoreMode,
     pub login_status: LoginStatus,
+    #[allow(dead_code)]
     pub auth_manager: Arc<AuthManager>,
+    #[allow(dead_code)]
     pub forced_chatgpt_workspace_id: Option<String>,
     pub forced_login_method: Option<ForcedLoginMethod>,
+    pub cli_auth_credentials_store_mode: AuthCredentialsStoreMode,
 }
 
 impl AuthModeWidget {
@@ -162,12 +152,13 @@ impl AuthModeWidget {
         !matches!(self.forced_login_method, Some(ForcedLoginMethod::Chatgpt))
     }
 
+    #[allow(dead_code)]
     fn is_chatgpt_login_allowed(&self) -> bool {
         !matches!(self.forced_login_method, Some(ForcedLoginMethod::Api))
     }
 
     fn disallow_api_login(&mut self) {
-        self.highlighted_mode = AuthMode::ChatGPT;
+        self.highlighted_mode = AuthMode::ApiKey;
         self.error = Some(API_KEY_DISABLED_MESSAGE.to_string());
         *self.sign_in_state.write().unwrap() = SignInState::PickMode;
         self.request_frame.schedule_frame();
@@ -177,11 +168,11 @@ impl AuthModeWidget {
         let mut lines: Vec<Line> = vec![
             Line::from(vec![
                 "  ".into(),
-                "Sign in with ChatGPT to use Codex as part of your paid plan".into(),
+                "codex-litellm connects directly to your LiteLLM backend.".into(),
             ]),
             Line::from(vec![
                 "  ".into(),
-                "or connect an API key for usage-based billing".into(),
+                "Provide the endpoint URL and API key to configure this workspace.".into(),
             ]),
             "".into(),
         ];
@@ -215,39 +206,23 @@ impl AuthModeWidget {
             vec![line1, line2]
         };
 
-        let chatgpt_description = if self.is_chatgpt_login_allowed() {
-            "Usage included with Plus, Pro, and Team plans"
-        } else {
-            "ChatGPT login is disabled"
-        };
-        lines.extend(create_mode_item(
-            0,
-            AuthMode::ChatGPT,
-            "Sign in with ChatGPT",
-            chatgpt_description,
-        ));
-        lines.push("".into());
         if self.is_api_login_allowed() {
             lines.extend(create_mode_item(
-                1,
+                0,
                 AuthMode::ApiKey,
-                "Provide your own API key",
-                "Pay for what you use",
+                "Configure LiteLLM credentials",
+                "Paste the LiteLLM endpoint URL and API key (env: LITELLM_BASE_URL / LITELLM_API_KEY)",
             ));
             lines.push("".into());
         } else {
             lines.push(
-                "  API key login is disabled by this workspace. Sign in with ChatGPT to continue."
+                "  LiteLLM configuration is disabled for this workspace. Ask your admin to enable API key login."
                     .dim()
                     .into(),
             );
             lines.push("".into());
         }
-        lines.push(
-            // AE: Following styles.md, this should probably be Cyan because it's a user input tip.
-            //     But leaving this for a future cleanup.
-            "  Press Enter to continue".dim().into(),
-        );
+        lines.push("  Press Enter to continue".dim().into());
         if let Some(err) = &self.error {
             lines.push("".into());
             lines.push(err.as_str().red().into());
@@ -327,9 +302,9 @@ impl AuthModeWidget {
 
     fn render_api_key_configured(&self, area: Rect, buf: &mut Buffer) {
         let lines = vec![
-            "✓ API key configured".fg(Color::Green).into(),
+            "✓ LiteLLM credentials configured".fg(Color::Green).into(),
             "".into(),
-            "  Codex will use usage-based billing with your API key.".into(),
+            "  codex-litellm will reuse these settings for future sessions.".into(),
         ];
 
         Paragraph::new(lines)
@@ -345,48 +320,83 @@ impl AuthModeWidget {
         ])
         .areas(area);
 
-        let mut intro_lines: Vec<Line> = vec![
-            Line::from(vec![
-                "> ".into(),
-                "Use your own OpenAI API key for usage-based billing".bold(),
-            ]),
-            "".into(),
-            "  Paste or type your API key below. It will be stored locally in auth.json.".into(),
-            "".into(),
-        ];
-        if state.prepopulated_from_env {
-            intro_lines.push("  Detected OPENAI_API_KEY environment variable.".into());
-            intro_lines.push(
-                "  Paste a different key if you prefer to use another account."
-                    .dim()
-                    .into(),
-            );
-            intro_lines.push("".into());
-        }
+        let (title, placeholder, value, intro_lines) = match state.stage {
+            ApiKeyEntryStage::BaseUrl => {
+                let mut intro_lines: Vec<Line> = vec![
+                    Line::from(vec![
+                        "> ".into(),
+                        "Step 1 of 2: LiteLLM endpoint URL".bold(),
+                    ]),
+                    "".into(),
+                    "  Paste the base URL of your LiteLLM server (for example, https://litellm.example.com/v1)."
+                        .into(),
+                    "".into(),
+                ];
+                if state.base_url_prepopulated {
+                    intro_lines.push(format!("  Prefilled from ${LITELLM_BASE_URL_ENV}.").into());
+                    intro_lines.push("  Press Enter to accept or type to override.".dim().into());
+                    intro_lines.push("".into());
+                }
+                (
+                    "LiteLLM endpoint",
+                    "Paste or type the LiteLLM endpoint URL".to_string(),
+                    state.base_url.clone(),
+                    intro_lines,
+                )
+            }
+            ApiKeyEntryStage::ApiKey => {
+                let mut intro_lines: Vec<Line> = vec![
+                    Line::from(vec!["> ".into(), "Step 2 of 2: LiteLLM API key".bold()]),
+                    "".into(),
+                    "  Paste the API token used by your LiteLLM deployment.".into(),
+                    "  Stored locally in config.toml as a Bearer token."
+                        .dim()
+                        .into(),
+                    "".into(),
+                ];
+                if state.api_key_prepopulated {
+                    intro_lines.push(format!("  Prefilled from ${LITELLM_API_KEY_ENV}.").into());
+                    intro_lines.push("  Press Enter to accept or type to override.".dim().into());
+                    intro_lines.push("".into());
+                }
+                (
+                    "LiteLLM API key",
+                    "Paste or type your LiteLLM API key".to_string(),
+                    state.api_key.clone(),
+                    intro_lines,
+                )
+            }
+        };
         Paragraph::new(intro_lines)
             .wrap(Wrap { trim: false })
             .render(intro_area, buf);
 
-        let content_line: Line = if state.value.is_empty() {
-            vec!["Paste or type your API key".dim()].into()
+        let content_line: Line = if value.trim().is_empty() {
+            vec![placeholder.dim()].into()
         } else {
-            Line::from(state.value.clone())
+            Line::from(value)
         };
         Paragraph::new(content_line)
             .wrap(Wrap { trim: false })
             .block(
                 Block::default()
-                    .title("API key")
+                    .title(title)
                     .borders(Borders::ALL)
                     .border_type(BorderType::Rounded)
                     .border_style(Style::default().fg(Color::Cyan)),
             )
             .render(input_area, buf);
 
-        let mut footer_lines: Vec<Line> = vec![
-            "  Press Enter to save".dim().into(),
-            "  Press Esc to go back".dim().into(),
-        ];
+        let mut footer_lines: Vec<Line> = match state.stage {
+            ApiKeyEntryStage::BaseUrl => vec![
+                "  Press Enter to continue".dim().into(),
+                "  Press Esc to cancel".dim().into(),
+            ],
+            ApiKeyEntryStage::ApiKey => vec![
+                "  Press Enter to save".dim().into(),
+                "  Press Esc to go back".dim().into(),
+            ],
+        };
         if let Some(error) = &self.error {
             footer_lines.push("".into());
             footer_lines.push(error.as_str().red().into());
@@ -397,7 +407,10 @@ impl AuthModeWidget {
     }
 
     fn handle_api_key_entry_key_event(&mut self, key_event: &KeyEvent) -> bool {
-        let mut should_save: Option<String> = None;
+        if key_event.kind != KeyEventKind::Press && key_event.kind != KeyEventKind::Repeat {
+            return false;
+        }
+        let mut should_save: Option<(String, String)> = None;
         let mut should_request_frame = false;
 
         {
@@ -405,25 +418,59 @@ impl AuthModeWidget {
             if let SignInState::ApiKeyEntry(state) = &mut *guard {
                 match key_event.code {
                     KeyCode::Esc => {
-                        *guard = SignInState::PickMode;
+                        match state.stage {
+                            ApiKeyEntryStage::BaseUrl => {
+                                *guard = SignInState::PickMode;
+                            }
+                            ApiKeyEntryStage::ApiKey => {
+                                state.stage = ApiKeyEntryStage::BaseUrl;
+                            }
+                        }
                         self.error = None;
                         should_request_frame = true;
                     }
-                    KeyCode::Enter => {
-                        let trimmed = state.value.trim().to_string();
-                        if trimmed.is_empty() {
-                            self.error = Some("API key cannot be empty".to_string());
-                            should_request_frame = true;
-                        } else {
-                            should_save = Some(trimmed);
+                    KeyCode::Enter => match state.stage {
+                        ApiKeyEntryStage::BaseUrl => {
+                            let trimmed = state.base_url.trim().to_string();
+                            if trimmed.is_empty() {
+                                self.error =
+                                    Some("LiteLLM endpoint URL cannot be empty".to_string());
+                                should_request_frame = true;
+                            } else {
+                                state.base_url = trimmed;
+                                state.stage = ApiKeyEntryStage::ApiKey;
+                                self.error = None;
+                                should_request_frame = true;
+                            }
                         }
-                    }
+                        ApiKeyEntryStage::ApiKey => {
+                            let trimmed = state.api_key.trim().to_string();
+                            if trimmed.is_empty() {
+                                self.error = Some("LiteLLM API key cannot be empty".to_string());
+                                should_request_frame = true;
+                            } else {
+                                should_save = Some((state.base_url.clone(), trimmed));
+                            }
+                        }
+                    },
                     KeyCode::Backspace => {
-                        if state.prepopulated_from_env {
-                            state.value.clear();
-                            state.prepopulated_from_env = false;
-                        } else {
-                            state.value.pop();
+                        match state.stage {
+                            ApiKeyEntryStage::BaseUrl => {
+                                if state.base_url_prepopulated {
+                                    state.base_url.clear();
+                                    state.base_url_prepopulated = false;
+                                } else {
+                                    state.base_url.pop();
+                                }
+                            }
+                            ApiKeyEntryStage::ApiKey => {
+                                if state.api_key_prepopulated {
+                                    state.api_key.clear();
+                                    state.api_key_prepopulated = false;
+                                } else {
+                                    state.api_key.pop();
+                                }
+                            }
                         }
                         self.error = None;
                         should_request_frame = true;
@@ -434,24 +481,34 @@ impl AuthModeWidget {
                             && !key_event.modifiers.contains(KeyModifiers::CONTROL)
                             && !key_event.modifiers.contains(KeyModifiers::ALT) =>
                     {
-                        if state.prepopulated_from_env {
-                            state.value.clear();
-                            state.prepopulated_from_env = false;
+                        match state.stage {
+                            ApiKeyEntryStage::BaseUrl => {
+                                if state.base_url_prepopulated {
+                                    state.base_url.clear();
+                                    state.base_url_prepopulated = false;
+                                }
+                                state.base_url.push(c);
+                            }
+                            ApiKeyEntryStage::ApiKey => {
+                                if state.api_key_prepopulated {
+                                    state.api_key.clear();
+                                    state.api_key_prepopulated = false;
+                                }
+                                state.api_key.push(c);
+                            }
                         }
-                        state.value.push(c);
                         self.error = None;
                         should_request_frame = true;
                     }
                     _ => {}
                 }
-                // handled; let guard drop before potential save
             } else {
                 return false;
             }
         }
 
-        if let Some(api_key) = should_save {
-            self.save_api_key(api_key);
+        if let Some((base_url, api_key)) = should_save {
+            self.save_litellm_credentials(base_url, api_key);
         } else if should_request_frame {
             self.request_frame.schedule_frame();
         }
@@ -466,11 +523,22 @@ impl AuthModeWidget {
 
         let mut guard = self.sign_in_state.write().unwrap();
         if let SignInState::ApiKeyEntry(state) = &mut *guard {
-            if state.prepopulated_from_env {
-                state.value = trimmed.to_string();
-                state.prepopulated_from_env = false;
-            } else {
-                state.value.push_str(trimmed);
+            match state.stage {
+                ApiKeyEntryStage::BaseUrl => {
+                    if state.base_url_prepopulated {
+                        state.base_url.clear();
+                        state.base_url_prepopulated = false;
+                    }
+                    state.base_url.push_str(trimmed);
+                }
+                ApiKeyEntryStage::ApiKey => {
+                    if state.api_key_prepopulated {
+                        state.api_key = trimmed.to_string();
+                        state.api_key_prepopulated = false;
+                    } else {
+                        state.api_key.push_str(trimmed);
+                    }
+                }
             }
             self.error = None;
         } else {
@@ -482,72 +550,100 @@ impl AuthModeWidget {
         true
     }
 
-    fn start_api_key_entry(&mut self) {
+    pub(crate) fn start_api_key_entry(&mut self) {
         if !self.is_api_login_allowed() {
             self.disallow_api_login();
             return;
         }
         self.error = None;
-        let prefill_from_env = read_openai_api_key_from_env();
-        let mut guard = self.sign_in_state.write().unwrap();
-        match &mut *guard {
-            SignInState::ApiKeyEntry(state) => {
-                if state.value.is_empty() {
-                    if let Some(prefill) = prefill_from_env {
-                        state.value = prefill;
-                        state.prepopulated_from_env = true;
-                    } else {
-                        state.prepopulated_from_env = false;
-                    }
-                }
-            }
-            _ => {
-                *guard = SignInState::ApiKeyEntry(ApiKeyInputState {
-                    value: prefill_from_env.clone().unwrap_or_default(),
-                    prepopulated_from_env: prefill_from_env.is_some(),
-                });
+        let base_url_env = read_env_var(LITELLM_BASE_URL_ENV);
+        let api_key_env = read_env_var(LITELLM_API_KEY_ENV);
+
+        let provider_state = match read_litellm_provider_state(&self.codex_home) {
+            Ok(state) => state,
+            Err(err) => {
+                self.error = Some(format!("Failed to read LiteLLM configuration: {err}"));
+                self.request_frame.schedule_frame();
+                return;
             }
-        }
+        };
+
+        let (base_url, base_url_prepopulated) =
+            match provider_state.base_url.filter(|v| !v.trim().is_empty()) {
+                Some(existing) => (existing, false),
+                None => match base_url_env {
+                    Some(env_value) => (env_value, true),
+                    None => (String::new(), false),
+                },
+            };
+
+        let (api_key, api_key_prepopulated) =
+            match provider_state.api_key.filter(|v| !v.trim().is_empty()) {
+                Some(existing) => (strip_bearer(existing), false),
+                None => match api_key_env {
+                    Some(env_value) => (env_value, true),
+                    None => (String::new(), false),
+                },
+            };
+
+        let stage = if base_url.trim().is_empty() {
+            ApiKeyEntryStage::BaseUrl
+        } else {
+            ApiKeyEntryStage::ApiKey
+        };
+
+        let mut guard = self.sign_in_state.write().unwrap();
+        *guard = SignInState::ApiKeyEntry(ApiKeyInputState {
+            base_url,
+            api_key,
+            stage,
+            base_url_prepopulated,
+            api_key_prepopulated,
+        });
         drop(guard);
         self.request_frame.schedule_frame();
     }
 
-    fn save_api_key(&mut self, api_key: String) {
+    fn save_litellm_credentials(&mut self, base_url: String, api_key: String) {
         if !self.is_api_login_allowed() {
             self.disallow_api_login();
             return;
         }
-        match login_with_api_key(
-            &self.codex_home,
-            &api_key,
-            self.cli_auth_credentials_store_mode,
-        ) {
+        let trimmed_url = base_url.trim().to_string();
+        let trimmed_key = api_key.trim().to_string();
+        if trimmed_url.is_empty() {
+            self.error = Some("LiteLLM endpoint URL cannot be empty".to_string());
+            self.request_frame.schedule_frame();
+            return;
+        }
+        if trimmed_key.is_empty() {
+            self.error = Some("LiteLLM API key cannot be empty".to_string());
+            self.request_frame.schedule_frame();
+            return;
+        }
+
+        let update = LiteLlmProviderUpdate {
+            base_url: Some(trimmed_url),
+            api_key: Some(format_bearer(&trimmed_key)),
+        };
+
+        match write_litellm_provider_state(&self.codex_home, update) {
             Ok(()) => {
                 self.error = None;
-                self.login_status = LoginStatus::AuthMode(AuthMode::ApiKey);
+                self.login_status = LoginStatus::NotAuthenticated;
                 self.auth_manager.reload();
                 *self.sign_in_state.write().unwrap() = SignInState::ApiKeyConfigured;
             }
             Err(err) => {
-                self.error = Some(format!("Failed to save API key: {err}"));
-                let mut guard = self.sign_in_state.write().unwrap();
-                if let SignInState::ApiKeyEntry(existing) = &mut *guard {
-                    if existing.value.is_empty() {
-                        existing.value.push_str(&api_key);
-                    }
-                    existing.prepopulated_from_env = false;
-                } else {
-                    *guard = SignInState::ApiKeyEntry(ApiKeyInputState {
-                        value: api_key,
-                        prepopulated_from_env: false,
-                    });
-                }
+                self.error = Some(format!("Failed to write LiteLLM configuration: {err}"));
+                // fall through to reshow the form with existing input
             }
         }
 
         self.request_frame.schedule_frame();
     }
 
+    #[allow(dead_code)]
     fn start_chatgpt_login(&mut self) {
         // If we're already authenticated with ChatGPT, don't start a new login –
         // just proceed to the success message flow.
@@ -590,7 +686,6 @@ impl AuthModeWidget {
                         }
                         _ => {
                             *sign_in_state.write().unwrap() = SignInState::PickMode;
-                            // self.error = Some(e.to_string());
                             request_frame.schedule_frame();
                         }
                     }
@@ -644,14 +739,43 @@ impl WidgetRef for AuthModeWidget {
     }
 }
 
+fn read_env_var(name: &str) -> Option<String> {
+    env::var(name)
+        .ok()
+        .map(|value| value.trim().to_string())
+        .filter(|value| !value.is_empty())
+}
+
+fn strip_bearer(value: String) -> String {
+    let trimmed = value.trim();
+    if let Some(rest) = trimmed.strip_prefix("Bearer") {
+        rest.trim_start().to_string()
+    } else if let Some(rest) = trimmed.strip_prefix("bearer") {
+        rest.trim_start().to_string()
+    } else {
+        trimmed.to_string()
+    }
+}
+
+fn format_bearer(token: &str) -> String {
+    if token.len() >= 6 && token[..6].eq_ignore_ascii_case("bearer") {
+        let rest = token[6..].trim_start();
+        if rest.is_empty() {
+            "Bearer".to_string()
+        } else {
+            format!("Bearer {rest}")
+        }
+    } else {
+        format!("Bearer {token}")
+    }
+}
+
 #[cfg(test)]
 mod tests {
     use super::*;
     use pretty_assertions::assert_eq;
     use tempfile::TempDir;
 
-    use codex_core::auth::AuthCredentialsStoreMode;
-
     fn widget_forced_chatgpt() -> (AuthModeWidget, TempDir) {
         let codex_home = TempDir::new().unwrap();
         let codex_home_path = codex_home.path().to_path_buf();
@@ -661,15 +785,15 @@ mod tests {
             error: None,
             sign_in_state: Arc::new(RwLock::new(SignInState::PickMode)),
             codex_home: codex_home_path.clone(),
-            cli_auth_credentials_store_mode: AuthCredentialsStoreMode::File,
             login_status: LoginStatus::NotAuthenticated,
             auth_manager: AuthManager::shared(
                 codex_home_path,
                 false,
-                AuthCredentialsStoreMode::File,
+                AuthCredentialsStoreMode::default(),
             ),
             forced_chatgpt_workspace_id: None,
             forced_login_method: Some(ForcedLoginMethod::Chatgpt),
+            cli_auth_credentials_store_mode: AuthCredentialsStoreMode::default(),
         };
         (widget, codex_home)
     }
@@ -691,7 +815,8 @@ mod tests {
     fn saving_api_key_is_blocked_when_chatgpt_forced() {
         let (mut widget, _tmp) = widget_forced_chatgpt();
 
-        widget.save_api_key("sk-test".to_string());
+        widget
+            .save_litellm_credentials("https://example.com/v1".to_string(), "sk-test".to_string());
 
         assert_eq!(widget.error.as_deref(), Some(API_KEY_DISABLED_MESSAGE));
         assert!(matches!(
diff --git a/codex-rs/tui/src/onboarding/mod.rs b/codex-rs/tui/src/onboarding/mod.rs
index 6c420dae5..23d79a965 100644
--- a/codex-rs/tui/src/onboarding/mod.rs
+++ b/codex-rs/tui/src/onboarding/mod.rs
@@ -1,4 +1,5 @@
 mod auth;
+pub mod model;
 pub mod onboarding_screen;
 mod trust_directory;
 pub use trust_directory::TrustDirectorySelection;
diff --git a/codex-rs/tui/src/onboarding/onboarding_screen.rs b/codex-rs/tui/src/onboarding/onboarding_screen.rs
index 709f158b8..f5383af20 100644
--- a/codex-rs/tui/src/onboarding/onboarding_screen.rs
+++ b/codex-rs/tui/src/onboarding/onboarding_screen.rs
@@ -12,11 +12,13 @@ use ratatui::widgets::Clear;
 use ratatui::widgets::WidgetRef;
 
 use codex_app_server_protocol::AuthMode;
+use codex_common::model_presets::is_litellm_provider_id;
 use codex_protocol::config_types::ForcedLoginMethod;
 
 use crate::LoginStatus;
 use crate::onboarding::auth::AuthModeWidget;
 use crate::onboarding::auth::SignInState;
+use crate::onboarding::model::ModelSelectionWidget;
 use crate::onboarding::trust_directory::TrustDirectorySelection;
 use crate::onboarding::trust_directory::TrustDirectoryWidget;
 use crate::onboarding::welcome::WelcomeWidget;
@@ -33,6 +35,7 @@ enum Step {
     Windows(WindowsSetupWidget),
     Welcome(WelcomeWidget),
     Auth(AuthModeWidget),
+    Model(ModelSelectionWidget),
     TrustDirectory(TrustDirectoryWidget),
 }
 
@@ -57,6 +60,8 @@ pub(crate) struct OnboardingScreen {
     steps: Vec<Step>,
     is_done: bool,
     windows_install_selected: bool,
+    litellm_credentials_updated: bool,
+    litellm_model_selected: bool,
 }
 
 pub(crate) struct OnboardingScreenArgs {
@@ -71,6 +76,8 @@ pub(crate) struct OnboardingScreenArgs {
 pub(crate) struct OnboardingResult {
     pub directory_trust_decision: Option<TrustDirectorySelection>,
     pub windows_install_selected: bool,
+    pub litellm_credentials_updated: bool,
+    pub litellm_model_selected: bool,
 }
 
 impl OnboardingScreen {
@@ -86,8 +93,7 @@ impl OnboardingScreen {
         let cwd = config.cwd.clone();
         let forced_chatgpt_workspace_id = config.forced_chatgpt_workspace_id.clone();
         let forced_login_method = config.forced_login_method;
-        let codex_home = config.codex_home;
-        let cli_auth_credentials_store_mode = config.cli_auth_credentials_store_mode;
+        let codex_home = config.codex_home.clone();
         let mut steps: Vec<Step> = Vec::new();
         if show_windows_wsl_screen {
             steps.push(Step::Windows(WindowsSetupWidget::new(codex_home.clone())));
@@ -98,21 +104,29 @@ impl OnboardingScreen {
         )));
         if show_login_screen {
             let highlighted_mode = match forced_login_method {
-                Some(ForcedLoginMethod::Api) => AuthMode::ApiKey,
-                _ => AuthMode::ChatGPT,
+                Some(ForcedLoginMethod::Api) | None => AuthMode::ApiKey,
+                Some(ForcedLoginMethod::Chatgpt) => AuthMode::ChatGPT,
             };
-            steps.push(Step::Auth(AuthModeWidget {
+            let mut auth_widget = AuthModeWidget {
                 request_frame: tui.frame_requester(),
                 highlighted_mode,
                 error: None,
                 sign_in_state: Arc::new(RwLock::new(SignInState::PickMode)),
                 codex_home: codex_home.clone(),
-                cli_auth_credentials_store_mode,
                 login_status,
+                cli_auth_credentials_store_mode: config.cli_auth_credentials_store_mode,
                 auth_manager,
                 forced_chatgpt_workspace_id,
                 forced_login_method,
-            }))
+            };
+            if config.litellm_setup_required {
+                auth_widget.start_api_key_entry();
+            }
+            steps.push(Step::Auth(auth_widget))
+        }
+        if is_litellm_provider_id(&config.model_provider_id) {
+            let model_widget = ModelSelectionWidget::new(tui.frame_requester(), config.clone());
+            steps.push(Step::Model(model_widget));
         }
         let is_git_repo = get_git_repo_root(&cwd).is_some();
         let highlighted = if is_git_repo {
@@ -132,12 +146,30 @@ impl OnboardingScreen {
             }))
         }
         // TODO: add git warning.
-        Self {
+        let is_litellm = is_litellm_provider_id(&config.model_provider_id);
+        let credentials_ready = if is_litellm {
+            !config.litellm_setup_required
+        } else {
+            true
+        };
+
+        let mut screen = Self {
             request_frame: tui.frame_requester(),
             steps,
             is_done: false,
             windows_install_selected: false,
-        }
+            litellm_credentials_updated: credentials_ready,
+            litellm_model_selected: !is_litellm,
+        };
+        screen.refresh_litellm_state();
+        tracing::info!(
+            target: "codex_litellm_debug::onboarding",
+            provider = %config.model_provider_id,
+            credentials_ready = %credentials_ready,
+            model_selected = screen.litellm_model_selected,
+            "onboarding_screen.created"
+        );
+        screen
     }
 
     fn current_steps_mut(&mut self) -> Vec<&mut Step> {
@@ -194,6 +226,72 @@ impl OnboardingScreen {
     pub fn windows_install_selected(&self) -> bool {
         self.windows_install_selected
     }
+
+    pub fn litellm_credentials_updated(&self) -> bool {
+        self.litellm_credentials_updated
+    }
+
+    pub fn litellm_model_selected(&self) -> bool {
+        self.litellm_model_selected
+    }
+
+    fn refresh_litellm_state(&mut self) {
+        #[cfg(debug_assertions)]
+        tracing::debug!(
+            credentials_updated = self.litellm_credentials_updated,
+            model_selected = self.litellm_model_selected,
+            "onboarding.refresh_litellm_state:start"
+        );
+
+        let credentials_ready = {
+            if let Some(Step::Auth(widget)) =
+                self.steps.iter().find(|step| matches!(step, Step::Auth(_)))
+            {
+                widget
+                    .sign_in_state
+                    .read()
+                    .map(|guard| matches!(&*guard, super::auth::SignInState::ApiKeyConfigured))
+                    .unwrap_or(false)
+            } else {
+                false
+            }
+        };
+
+        if credentials_ready && !self.litellm_credentials_updated {
+            self.litellm_credentials_updated = true;
+            #[cfg(debug_assertions)]
+            tracing::debug!("onboarding: LiteLLM credentials captured");
+            tracing::info!(
+                target: "codex_litellm_debug::onboarding",
+                "onboarding.credentials_ready"
+            );
+        }
+
+        if let Some(Step::Model(widget)) = self
+            .steps
+            .iter_mut()
+            .find(|step| matches!(step, Step::Model(_)))
+        {
+            if self.litellm_credentials_updated {
+                #[cfg(debug_assertions)]
+                tracing::debug!("onboarding: triggering model preset load");
+                tracing::info!(
+                    target: "codex_litellm_debug::onboarding",
+                    "onboarding.model_widget.credentials_ready"
+                );
+                widget.credentials_ready();
+            }
+            if widget.selection().is_some() {
+                self.litellm_model_selected = true;
+                #[cfg(debug_assertions)]
+                tracing::debug!("onboarding: LiteLLM model selected");
+                tracing::info!(
+                    target: "codex_litellm_debug::onboarding",
+                    "onboarding.model_selected"
+                );
+            }
+        }
+    }
 }
 
 impl KeyboardHandler for OnboardingScreen {
@@ -240,6 +338,7 @@ impl KeyboardHandler for OnboardingScreen {
             self.is_done = true;
         }
         self.request_frame.schedule_frame();
+        self.refresh_litellm_state();
     }
 
     fn handle_paste(&mut self, pasted: String) {
@@ -251,6 +350,7 @@ impl KeyboardHandler for OnboardingScreen {
             active_step.handle_paste(pasted);
         }
         self.request_frame.schedule_frame();
+        self.refresh_litellm_state();
     }
 }
 
@@ -323,6 +423,7 @@ impl KeyboardHandler for Step {
             Step::Windows(widget) => widget.handle_key_event(key_event),
             Step::Welcome(widget) => widget.handle_key_event(key_event),
             Step::Auth(widget) => widget.handle_key_event(key_event),
+            Step::Model(widget) => widget.handle_key_event(key_event),
             Step::TrustDirectory(widget) => widget.handle_key_event(key_event),
         }
     }
@@ -332,6 +433,7 @@ impl KeyboardHandler for Step {
             Step::Windows(_) => {}
             Step::Welcome(_) => {}
             Step::Auth(widget) => widget.handle_paste(pasted),
+            Step::Model(_) => {}
             Step::TrustDirectory(widget) => widget.handle_paste(pasted),
         }
     }
@@ -343,6 +445,7 @@ impl StepStateProvider for Step {
             Step::Windows(w) => w.get_step_state(),
             Step::Welcome(w) => w.get_step_state(),
             Step::Auth(w) => w.get_step_state(),
+            Step::Model(w) => w.get_step_state(),
             Step::TrustDirectory(w) => w.get_step_state(),
         }
     }
@@ -360,6 +463,9 @@ impl WidgetRef for Step {
             Step::Auth(widget) => {
                 widget.render_ref(area, buf);
             }
+            Step::Model(widget) => {
+                widget.render_ref(area, buf);
+            }
             Step::TrustDirectory(widget) => {
                 widget.render_ref(area, buf);
             }
@@ -434,5 +540,7 @@ pub(crate) async fn run_onboarding_app(
     Ok(OnboardingResult {
         directory_trust_decision: onboarding_screen.directory_trust_decision(),
         windows_install_selected: onboarding_screen.windows_install_selected(),
+        litellm_credentials_updated: onboarding_screen.litellm_credentials_updated(),
+        litellm_model_selected: onboarding_screen.litellm_model_selected(),
     })
 }
diff --git a/codex-rs/tui/src/onboarding/welcome.rs b/codex-rs/tui/src/onboarding/welcome.rs
index 645c86ba4..ea819fc3d 100644
--- a/codex-rs/tui/src/onboarding/welcome.rs
+++ b/codex-rs/tui/src/onboarding/welcome.rs
@@ -68,9 +68,12 @@ impl WidgetRef for &WelcomeWidget {
         lines.push(Line::from(vec![
             "  ".into(),
             "Welcome to ".into(),
-            "Codex".bold(),
-            ", OpenAI's command-line coding agent".into(),
+            "codex-litellm".bold(),
+            ", the LiteLLM-native build of Codex".into(),
         ]));
+        lines.push(
+            "  We'll walk through connecting to your LiteLLM endpoint on the next screen.".into(),
+        );
 
         Paragraph::new(lines)
             .wrap(Wrap { trim: false })
diff --git a/codex-rs/tui/src/slash_command.rs b/codex-rs/tui/src/slash_command.rs
index 969d279b0..6f7f87abb 100644
--- a/codex-rs/tui/src/slash_command.rs
+++ b/codex-rs/tui/src/slash_command.rs
@@ -12,6 +12,7 @@ use strum_macros::IntoStaticStr;
 pub enum SlashCommand {
     // DO NOT ALPHA-SORT! Enum order is presentation order in the popup, so
     // more frequently used commands should be listed first.
+    #[strum(serialize = "model")]
     Model,
     Approvals,
     Review,
diff --git a/codex-rs/tui/src/status/card.rs b/codex-rs/tui/src/status/card.rs
index 11b723fef..197b25c06 100644
--- a/codex-rs/tui/src/status/card.rs
+++ b/codex-rs/tui/src/status/card.rs
@@ -33,6 +33,7 @@ use super::rate_limits::format_status_limit_summary;
 use super::rate_limits::render_status_limit_progress_bar;
 use crate::wrapping::RtOptions;
 use crate::wrapping::word_wrap_lines;
+use codex_litellm_model_session_telemetry::SessionTelemetrySnapshot;
 
 #[derive(Debug, Clone)]
 struct StatusContextWindowData {
@@ -60,6 +61,7 @@ struct StatusHistoryCell {
     account: Option<StatusAccountDisplay>,
     session_id: Option<String>,
     token_usage: StatusTokenUsageData,
+    model_session: Option<SessionTelemetrySnapshot>,
     rate_limits: StatusRateLimitData,
 }
 
@@ -68,6 +70,7 @@ pub(crate) fn new_status_output(
     total_usage: &TokenUsage,
     context_usage: Option<&TokenUsage>,
     session_id: &Option<ConversationId>,
+    model_session: Option<SessionTelemetrySnapshot>,
     rate_limits: Option<&RateLimitSnapshotDisplay>,
     now: DateTime<Local>,
 ) -> CompositeHistoryCell {
@@ -77,6 +80,7 @@ pub(crate) fn new_status_output(
         total_usage,
         context_usage,
         session_id,
+        model_session,
         rate_limits,
         now,
     );
@@ -90,6 +94,7 @@ impl StatusHistoryCell {
         total_usage: &TokenUsage,
         context_usage: Option<&TokenUsage>,
         session_id: &Option<ConversationId>,
+        model_session: Option<SessionTelemetrySnapshot>,
         rate_limits: Option<&RateLimitSnapshotDisplay>,
         now: DateTime<Local>,
     ) -> Self {
@@ -134,6 +139,7 @@ impl StatusHistoryCell {
             account,
             session_id,
             token_usage,
+            model_session,
             rate_limits,
         }
     }
@@ -268,7 +274,11 @@ impl HistoryCell for StatusHistoryCell {
             Span::from(format!("{}>_ ", FieldFormatter::INDENT)).dim(),
             Span::from("OpenAI Codex").bold(),
             Span::from(" ").dim(),
-            Span::from(format!("(v{CODEX_CLI_VERSION})")).dim(),
+            Span::from(format!(
+                "(v{})",
+                codex_common::litellm::decorate_version(CODEX_CLI_VERSION)
+            ))
+            .dim(),
         ]));
         lines.push(Line::from(Vec::<Span<'static>>::new()));
 
@@ -306,20 +316,17 @@ impl HistoryCell for StatusHistoryCell {
         if self.token_usage.context_window.is_some() {
             push_label(&mut labels, &mut seen, "Context window");
         }
+        push_label(&mut labels, &mut seen, "LiteLLM usage");
         self.collect_rate_limit_labels(&mut seen, &mut labels);
 
         let formatter = FieldFormatter::from_labels(labels.iter().map(String::as_str));
         let value_width = formatter.value_width(available_inner_width);
 
         let note_first_line = Line::from(vec![
-            Span::from("Visit ").cyan(),
-            "https://chatgpt.com/codex/settings/usage"
-                .cyan()
-                .underlined(),
-            Span::from(" for up-to-date").cyan(),
+            Span::from("LiteLLM endpoint configured via CODEX_HOME/config.toml").cyan(),
         ]);
         let note_second_line = Line::from(vec![
-            Span::from("information on rate limits and credits").cyan(),
+            Span::from("Per-turn usage recorded under CODEX_HOME/logs for release audits").cyan(),
         ]);
         let note_lines = word_wrap_lines(
             [note_first_line, note_second_line],
@@ -357,6 +364,58 @@ impl HistoryCell for StatusHistoryCell {
             lines.push(formatter.line("Token usage", self.token_usage_spans()));
         }
 
+        if let Some(snapshot) = self.model_session.as_ref() {
+            let summary_spans = if snapshot.total_turns == 0 {
+                vec![Span::from("no LiteLLM turns recorded yet").dim()]
+            } else {
+                let tokens_fmt = format_tokens_compact(snapshot.total_tokens);
+                let turns = snapshot.total_turns;
+                let last_model = snapshot.last_model.as_deref().unwrap_or("-");
+                let effort_suffix = snapshot
+                    .last_reasoning_effort
+                    .as_ref()
+                    .map(|eff| format!(" • last effort {eff}"))
+                    .unwrap_or_default();
+                vec![Span::from(format!(
+                    "{tokens_fmt} across {turns} turn{plural} (last model {last_model}{effort_suffix})",
+                    plural = if turns == 1 { "" } else { "s" }
+                ))]
+            };
+            lines.push(formatter.line("LiteLLM usage", summary_spans));
+
+            for model in snapshot.models.iter().take(5) {
+                let tokens_fmt = format_tokens_compact(model.total_tokens);
+                let turns = model.turns;
+                let effort_suffix = model
+                    .last_reasoning_effort
+                    .as_ref()
+                    .map(|eff| format!(" • last effort {eff}"))
+                    .unwrap_or_default();
+                let mut spans = Vec::new();
+                spans.push(Span::from(format!("• {}", model.model)));
+                spans.push(Span::from(" ").dim());
+                spans.push(Span::from(tokens_fmt));
+                spans.push(Span::from(" in ").dim());
+                spans.push(Span::from(format!(
+                    "{turns} turn{}",
+                    if turns == 1 { "" } else { "s" }
+                )));
+                if !effort_suffix.is_empty() {
+                    spans.push(Span::from(effort_suffix).dim());
+                }
+                lines.push(formatter.continuation(spans));
+            }
+
+            let remaining = snapshot.models.len().saturating_sub(5);
+            if remaining > 0 {
+                lines.push(formatter.continuation(vec![Span::from(format!(
+                    "… {remaining} additional model{}",
+                    if remaining == 1 { "" } else { "s" }
+                ))
+                .dim()]));
+            }
+        }
+
         if let Some(spans) = self.context_window_spans() {
             lines.push(formatter.line("Context window", spans));
         }
diff --git a/codex-rs/tui/src/status/snapshots/codex_tui__status__tests__status_snapshot_includes_monthly_limit.snap b/codex-rs/tui/src/status/snapshots/codex_tui__status__tests__status_snapshot_includes_monthly_limit.snap
index 85e6356c3..7a810ad0d 100644
--- a/codex-rs/tui/src/status/snapshots/codex_tui__status__tests__status_snapshot_includes_monthly_limit.snap
+++ b/codex-rs/tui/src/status/snapshots/codex_tui__status__tests__status_snapshot_includes_monthly_limit.snap
@@ -1,11 +1,12 @@
 ---
 source: tui/src/status/tests.rs
+assertion_line: 179
 expression: sanitized
 ---
 /status
 
 ╭────────────────────────────────────────────────────────────────────────────╮
-│  >_ OpenAI Codex (v0.0.0)                                                  │
+│  >_ OpenAI Codex (v0.53.0+bcf30cef+litbcf30cef)                            │
 │                                                                            │
 │ Visit https://chatgpt.com/codex/settings/usage for up-to-date              │
 │ information on rate limits and credits                                     │
diff --git a/codex-rs/tui/src/status/snapshots/codex_tui__status__tests__status_snapshot_includes_reasoning_details.snap b/codex-rs/tui/src/status/snapshots/codex_tui__status__tests__status_snapshot_includes_reasoning_details.snap
index 123547cb5..33197537c 100644
--- a/codex-rs/tui/src/status/snapshots/codex_tui__status__tests__status_snapshot_includes_reasoning_details.snap
+++ b/codex-rs/tui/src/status/snapshots/codex_tui__status__tests__status_snapshot_includes_reasoning_details.snap
@@ -1,11 +1,12 @@
 ---
 source: tui/src/status/tests.rs
+assertion_line: 130
 expression: sanitized
 ---
 /status
 
 ╭─────────────────────────────────────────────────────────────────────╮
-│  >_ OpenAI Codex (v0.0.0)                                           │
+│  >_ OpenAI Codex (v0.53.0+bcf30cef+litbcf30cef)                     │
 │                                                                     │
 │ Visit https://chatgpt.com/codex/settings/usage for up-to-date       │
 │ information on rate limits and credits                              │
diff --git a/codex-rs/tui/src/status/snapshots/codex_tui__status__tests__status_snapshot_shows_empty_limits_message.snap b/codex-rs/tui/src/status/snapshots/codex_tui__status__tests__status_snapshot_shows_empty_limits_message.snap
index 17862db23..b26a789b9 100644
--- a/codex-rs/tui/src/status/snapshots/codex_tui__status__tests__status_snapshot_shows_empty_limits_message.snap
+++ b/codex-rs/tui/src/status/snapshots/codex_tui__status__tests__status_snapshot_shows_empty_limits_message.snap
@@ -1,11 +1,12 @@
 ---
 source: tui/src/status/tests.rs
+assertion_line: 335
 expression: sanitized
 ---
 /status
 
 ╭─────────────────────────────────────────────────────────────────╮
-│  >_ OpenAI Codex (v0.0.0)                                       │
+│  >_ OpenAI Codex (v0.53.0+bcf30cef+litbcf30cef)                 │
 │                                                                 │
 │ Visit https://chatgpt.com/codex/settings/usage for up-to-date   │
 │ information on rate limits and credits                          │
diff --git a/codex-rs/tui/src/status/snapshots/codex_tui__status__tests__status_snapshot_shows_missing_limits_message.snap b/codex-rs/tui/src/status/snapshots/codex_tui__status__tests__status_snapshot_shows_missing_limits_message.snap
index 17862db23..1a7a87466 100644
--- a/codex-rs/tui/src/status/snapshots/codex_tui__status__tests__status_snapshot_shows_missing_limits_message.snap
+++ b/codex-rs/tui/src/status/snapshots/codex_tui__status__tests__status_snapshot_shows_missing_limits_message.snap
@@ -1,11 +1,12 @@
 ---
 source: tui/src/status/tests.rs
+assertion_line: 291
 expression: sanitized
 ---
 /status
 
 ╭─────────────────────────────────────────────────────────────────╮
-│  >_ OpenAI Codex (v0.0.0)                                       │
+│  >_ OpenAI Codex (v0.53.0+bcf30cef+litbcf30cef)                 │
 │                                                                 │
 │ Visit https://chatgpt.com/codex/settings/usage for up-to-date   │
 │ information on rate limits and credits                          │
diff --git a/codex-rs/tui/src/status/snapshots/codex_tui__status__tests__status_snapshot_shows_stale_limits_message.snap b/codex-rs/tui/src/status/snapshots/codex_tui__status__tests__status_snapshot_shows_stale_limits_message.snap
index 3a98ce95c..452e762e7 100644
--- a/codex-rs/tui/src/status/snapshots/codex_tui__status__tests__status_snapshot_shows_stale_limits_message.snap
+++ b/codex-rs/tui/src/status/snapshots/codex_tui__status__tests__status_snapshot_shows_stale_limits_message.snap
@@ -1,11 +1,12 @@
 ---
 source: tui/src/status/tests.rs
+assertion_line: 388
 expression: sanitized
 ---
 /status
 
 ╭─────────────────────────────────────────────────────────────────────╮
-│  >_ OpenAI Codex (v0.0.0)                                           │
+│  >_ OpenAI Codex (v0.53.0+bcf30cef+litbcf30cef)                     │
 │                                                                     │
 │ Visit https://chatgpt.com/codex/settings/usage for up-to-date       │
 │ information on rate limits and credits                              │
diff --git a/codex-rs/tui/src/status/snapshots/codex_tui__status__tests__status_snapshot_truncates_in_narrow_terminal.snap b/codex-rs/tui/src/status/snapshots/codex_tui__status__tests__status_snapshot_truncates_in_narrow_terminal.snap
index 5bbaaef7c..eadce1cdc 100644
--- a/codex-rs/tui/src/status/snapshots/codex_tui__status__tests__status_snapshot_truncates_in_narrow_terminal.snap
+++ b/codex-rs/tui/src/status/snapshots/codex_tui__status__tests__status_snapshot_truncates_in_narrow_terminal.snap
@@ -1,12 +1,13 @@
 ---
 source: tui/src/status/tests.rs
 assertion_line: 257
+assertion_line: 260
 expression: sanitized
 ---
 /status
 
 ╭────────────────────────────────────────────╮
-│  >_ OpenAI Codex (v0.0.0)                  │
+│  >_ OpenAI Codex (v0.53.0+bcf30cef+litbcf3 │
 │                                            │
 │ Visit https://chatgpt.com/codex/settings/  │
 │ usage for up-to-date                       │
diff --git a/codex-rs/tui/src/status/tests.rs b/codex-rs/tui/src/status/tests.rs
index 4ab4a8ea6..5820e8ab2 100644
--- a/codex-rs/tui/src/status/tests.rs
+++ b/codex-rs/tui/src/status/tests.rs
@@ -7,6 +7,7 @@ use chrono::Utc;
 use codex_core::config::Config;
 use codex_core::config::ConfigOverrides;
 use codex_core::config::ConfigToml;
+use codex_core::model_family::find_family_for_model;
 use codex_core::protocol::RateLimitSnapshot;
 use codex_core::protocol::RateLimitWindow;
 use codex_core::protocol::SandboxPolicy;
@@ -27,6 +28,24 @@ fn test_config(temp_home: &TempDir) -> Config {
     .expect("load config")
 }
 
+fn configure_openai_gpt5(config: &mut Config) {
+    let model = "gpt-5-codex";
+    config.model = model.to_string();
+    if let Some(family) = find_family_for_model(model) {
+        config.model_family = family;
+    }
+
+    let provider_id = "openai";
+    config.model_provider_id = provider_id.to_string();
+    if let Some(info) = config.model_providers.get(provider_id) {
+        config.model_provider = info.clone();
+    }
+
+    if config.model_context_window.is_none() {
+        config.model_context_window = Some(272_000);
+    }
+}
+
 fn render_lines(lines: &[Line<'static>]) -> Vec<String> {
     lines
         .iter()
@@ -72,8 +91,7 @@ fn reset_at_from(captured_at: &chrono::DateTime<chrono::Local>, seconds: i64) ->
 fn status_snapshot_includes_reasoning_details() {
     let temp_home = TempDir::new().expect("temp home");
     let mut config = test_config(&temp_home);
-    config.model = "gpt-5-codex".to_string();
-    config.model_provider_id = "openai".to_string();
+    configure_openai_gpt5(&mut config);
     config.model_reasoning_effort = Some(ReasoningEffort::High);
     config.model_reasoning_summary = ReasoningSummary::Detailed;
     config.sandbox_policy = SandboxPolicy::WorkspaceWrite {
@@ -116,6 +134,7 @@ fn status_snapshot_includes_reasoning_details() {
         &usage,
         Some(&usage),
         &None,
+        None,
         Some(&rate_display),
         captured_at,
     );
@@ -133,8 +152,7 @@ fn status_snapshot_includes_reasoning_details() {
 fn status_snapshot_includes_monthly_limit() {
     let temp_home = TempDir::new().expect("temp home");
     let mut config = test_config(&temp_home);
-    config.model = "gpt-5-codex".to_string();
-    config.model_provider_id = "openai".to_string();
+    configure_openai_gpt5(&mut config);
     config.cwd = PathBuf::from("/workspace/tests");
 
     let usage = TokenUsage {
@@ -164,6 +182,7 @@ fn status_snapshot_includes_monthly_limit() {
         &usage,
         Some(&usage),
         &None,
+        None,
         Some(&rate_display),
         captured_at,
     );
@@ -181,7 +200,7 @@ fn status_snapshot_includes_monthly_limit() {
 fn status_card_token_usage_excludes_cached_tokens() {
     let temp_home = TempDir::new().expect("temp home");
     let mut config = test_config(&temp_home);
-    config.model = "gpt-5-codex".to_string();
+    configure_openai_gpt5(&mut config);
     config.cwd = PathBuf::from("/workspace/tests");
 
     let usage = TokenUsage {
@@ -197,7 +216,7 @@ fn status_card_token_usage_excludes_cached_tokens() {
         .single()
         .expect("timestamp");
 
-    let composite = new_status_output(&config, &usage, Some(&usage), &None, None, now);
+    let composite = new_status_output(&config, &usage, Some(&usage), &None, None, None, now);
     let rendered = render_lines(&composite.display_lines(120));
 
     assert!(
@@ -210,8 +229,7 @@ fn status_card_token_usage_excludes_cached_tokens() {
 fn status_snapshot_truncates_in_narrow_terminal() {
     let temp_home = TempDir::new().expect("temp home");
     let mut config = test_config(&temp_home);
-    config.model = "gpt-5-codex".to_string();
-    config.model_provider_id = "openai".to_string();
+    configure_openai_gpt5(&mut config);
     config.model_reasoning_effort = Some(ReasoningEffort::High);
     config.model_reasoning_summary = ReasoningSummary::Detailed;
     config.cwd = PathBuf::from("/workspace/tests");
@@ -243,6 +261,7 @@ fn status_snapshot_truncates_in_narrow_terminal() {
         &usage,
         Some(&usage),
         &None,
+        None,
         Some(&rate_display),
         captured_at,
     );
@@ -261,7 +280,7 @@ fn status_snapshot_truncates_in_narrow_terminal() {
 fn status_snapshot_shows_missing_limits_message() {
     let temp_home = TempDir::new().expect("temp home");
     let mut config = test_config(&temp_home);
-    config.model = "gpt-5-codex".to_string();
+    configure_openai_gpt5(&mut config);
     config.cwd = PathBuf::from("/workspace/tests");
 
     let usage = TokenUsage {
@@ -277,7 +296,7 @@ fn status_snapshot_shows_missing_limits_message() {
         .single()
         .expect("timestamp");
 
-    let composite = new_status_output(&config, &usage, Some(&usage), &None, None, now);
+    let composite = new_status_output(&config, &usage, Some(&usage), &None, None, None, now);
     let mut rendered_lines = render_lines(&composite.display_lines(80));
     if cfg!(windows) {
         for line in &mut rendered_lines {
@@ -292,7 +311,7 @@ fn status_snapshot_shows_missing_limits_message() {
 fn status_snapshot_shows_empty_limits_message() {
     let temp_home = TempDir::new().expect("temp home");
     let mut config = test_config(&temp_home);
-    config.model = "gpt-5-codex".to_string();
+    configure_openai_gpt5(&mut config);
     config.cwd = PathBuf::from("/workspace/tests");
 
     let usage = TokenUsage {
@@ -318,6 +337,7 @@ fn status_snapshot_shows_empty_limits_message() {
         &usage,
         Some(&usage),
         &None,
+        None,
         Some(&rate_display),
         captured_at,
     );
@@ -335,7 +355,7 @@ fn status_snapshot_shows_empty_limits_message() {
 fn status_snapshot_shows_stale_limits_message() {
     let temp_home = TempDir::new().expect("temp home");
     let mut config = test_config(&temp_home);
-    config.model = "gpt-5-codex".to_string();
+    configure_openai_gpt5(&mut config);
     config.cwd = PathBuf::from("/workspace/tests");
 
     let usage = TokenUsage {
@@ -370,6 +390,7 @@ fn status_snapshot_shows_stale_limits_message() {
         &usage,
         Some(&usage),
         &None,
+        None,
         Some(&rate_display),
         now,
     );
@@ -409,7 +430,15 @@ fn status_context_window_uses_last_usage() {
         .single()
         .expect("timestamp");
 
-    let composite = new_status_output(&config, &total_usage, Some(&last_usage), &None, None, now);
+    let composite = new_status_output(
+        &config,
+        &total_usage,
+        Some(&last_usage),
+        &None,
+        None,
+        None,
+        now,
+    );
     let rendered_lines = render_lines(&composite.display_lines(80));
     let context_line = rendered_lines
         .into_iter()

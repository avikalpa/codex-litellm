diff --git a/codex-rs/Cargo.lock b/codex-rs/Cargo.lock
index 64c83b0f..8ac9f794 100644
--- a/codex-rs/Cargo.lock
+++ b/codex-rs/Cargo.lock
@@ -27,17 +27,6 @@ version = "2.0.1"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "320119579fcad9c21884f5c4861d16174d0e06250625266f50fe6898340abefa"
 
-[[package]]
-name = "aes"
-version = "0.8.4"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "b169f7a6d4742236a0a00c541b845991d0ac43e546831af1249753ab4c3aa3a0"
-dependencies = [
- "cfg-if",
- "cipher",
- "cpufeatures",
-]
-
 [[package]]
 name = "ahash"
 version = "0.8.12"
@@ -178,7 +167,7 @@ checksum = "b0674a1ddeecb70197781e945de4b3b8ffb61fa939a5597bcf48503737663100"
 
 [[package]]
 name = "app_test_support"
-version = "0.0.0"
+version = "0.53.0"
 dependencies = [
  "anyhow",
  "assert_cmd",
@@ -309,18 +298,6 @@ version = "1.5.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "9b34d609dfbaf33d6889b2b7106d3ca345eacad44200913df5ba02bfd31d2ba9"
 
-[[package]]
-name = "async-broadcast"
-version = "0.7.2"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "435a87a52755b8f27fcf321ac4f04b2802e337c8c4872923137471ec39c37532"
-dependencies = [
- "event-listener",
- "event-listener-strategy",
- "futures-core",
- "pin-project-lite",
-]
-
 [[package]]
 name = "async-channel"
 version = "2.5.0"
@@ -333,107 +310,6 @@ dependencies = [
  "pin-project-lite",
 ]
 
-[[package]]
-name = "async-executor"
-version = "1.13.3"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "497c00e0fd83a72a79a39fcbd8e3e2f055d6f6c7e025f3b3d91f4f8e76527fb8"
-dependencies = [
- "async-task",
- "concurrent-queue",
- "fastrand",
- "futures-lite",
- "pin-project-lite",
- "slab",
-]
-
-[[package]]
-name = "async-fs"
-version = "2.2.0"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "8034a681df4aed8b8edbd7fbe472401ecf009251c8b40556b304567052e294c5"
-dependencies = [
- "async-lock",
- "blocking",
- "futures-lite",
-]
-
-[[package]]
-name = "async-io"
-version = "2.6.0"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "456b8a8feb6f42d237746d4b3e9a178494627745c3c56c6ea55d92ba50d026fc"
-dependencies = [
- "autocfg",
- "cfg-if",
- "concurrent-queue",
- "futures-io",
- "futures-lite",
- "parking",
- "polling",
- "rustix 1.0.8",
- "slab",
- "windows-sys 0.61.1",
-]
-
-[[package]]
-name = "async-lock"
-version = "3.4.1"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "5fd03604047cee9b6ce9de9f70c6cd540a0520c813cbd49bae61f33ab80ed1dc"
-dependencies = [
- "event-listener",
- "event-listener-strategy",
- "pin-project-lite",
-]
-
-[[package]]
-name = "async-process"
-version = "2.5.0"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "fc50921ec0055cdd8a16de48773bfeec5c972598674347252c0399676be7da75"
-dependencies = [
- "async-channel",
- "async-io",
- "async-lock",
- "async-signal",
- "async-task",
- "blocking",
- "cfg-if",
- "event-listener",
- "futures-lite",
- "rustix 1.0.8",
-]
-
-[[package]]
-name = "async-recursion"
-version = "1.1.1"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "3b43422f69d8ff38f95f1b2bb76517c91589a924d1559a0e935d7c8ce0274c11"
-dependencies = [
- "proc-macro2",
- "quote",
- "syn 2.0.104",
-]
-
-[[package]]
-name = "async-signal"
-version = "0.2.13"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "43c070bbf59cd3570b6b2dd54cd772527c7c3620fce8be898406dd3ed6adc64c"
-dependencies = [
- "async-io",
- "async-lock",
- "atomic-waker",
- "cfg-if",
- "futures-core",
- "futures-io",
- "rustix 1.0.8",
- "signal-hook-registry",
- "slab",
- "windows-sys 0.61.1",
-]
-
 [[package]]
 name = "async-stream"
 version = "0.3.6"
@@ -456,12 +332,6 @@ dependencies = [
  "syn 2.0.104",
 ]
 
-[[package]]
-name = "async-task"
-version = "4.7.1"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "8b75356056920673b02621b35afd0f7dda9306d03c79a30f5c56c44cf256e3de"
-
 [[package]]
 name = "async-trait"
 version = "0.1.89"
@@ -605,28 +475,6 @@ dependencies = [
  "generic-array",
 ]
 
-[[package]]
-name = "block-padding"
-version = "0.3.3"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "a8894febbff9f758034a5b8e12d87918f56dfc64a8e1fe757d65e29041538d93"
-dependencies = [
- "generic-array",
-]
-
-[[package]]
-name = "blocking"
-version = "1.6.2"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "e83f8d02be6967315521be875afa792a316e28d57b5a2d401897e2a7921b7f21"
-dependencies = [
- "async-channel",
- "async-task",
- "futures-io",
- "futures-lite",
- "piper",
-]
-
 [[package]]
 name = "bstr"
 version = "1.12.0"
@@ -683,15 +531,6 @@ dependencies = [
  "rustversion",
 ]
 
-[[package]]
-name = "cbc"
-version = "0.1.2"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "26b52a9543ae338f279b96b0b9fed9c8093744685043739079ce85cd58f289a6"
-dependencies = [
- "cipher",
-]
-
 [[package]]
 name = "cc"
 version = "1.2.30"
@@ -745,16 +584,6 @@ version = "1.5.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "6e4de3bc4ea267985becf712dc6d9eed8b04c953b3fcfb339ebc87acd9804901"
 
-[[package]]
-name = "cipher"
-version = "0.4.4"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "773f3b9af64447d2ce9850330c473515014aa235e6a783b02db81ff39e4a3dad"
-dependencies = [
- "crypto-common",
- "inout",
-]
-
 [[package]]
 name = "clap"
 version = "4.5.47"
@@ -822,7 +651,7 @@ checksum = "e9b18233253483ce2f65329a24072ec414db782531bdbb7d0bbc4bd2ce6b7e21"
 
 [[package]]
 name = "codex-ansi-escape"
-version = "0.0.0"
+version = "0.53.0"
 dependencies = [
  "ansi-to-tui",
  "ratatui",
@@ -831,7 +660,7 @@ dependencies = [
 
 [[package]]
 name = "codex-app-server"
-version = "0.0.0"
+version = "0.53.0"
 dependencies = [
  "anyhow",
  "app_test_support",
@@ -866,7 +695,7 @@ dependencies = [
 
 [[package]]
 name = "codex-app-server-protocol"
-version = "0.0.0"
+version = "0.53.0"
 dependencies = [
  "anyhow",
  "clap",
@@ -883,7 +712,7 @@ dependencies = [
 
 [[package]]
 name = "codex-apply-patch"
-version = "0.0.0"
+version = "0.53.0"
 dependencies = [
  "anyhow",
  "assert_cmd",
@@ -898,7 +727,7 @@ dependencies = [
 
 [[package]]
 name = "codex-arg0"
-version = "0.0.0"
+version = "0.53.0"
 dependencies = [
  "anyhow",
  "codex-apply-patch",
@@ -911,7 +740,7 @@ dependencies = [
 
 [[package]]
 name = "codex-async-utils"
-version = "0.0.0"
+version = "0.53.0"
 dependencies = [
  "async-trait",
  "pretty_assertions",
@@ -935,7 +764,7 @@ dependencies = [
 
 [[package]]
 name = "codex-backend-openapi-models"
-version = "0.0.0"
+version = "0.53.0"
 dependencies = [
  "serde",
  "serde_json",
@@ -943,8 +772,12 @@ dependencies = [
 ]
 
 [[package]]
-name = "codex-chatgpt"
+name = "codex-build-info"
 version = "0.0.0"
+
+[[package]]
+name = "codex-chatgpt"
+version = "0.53.0"
 dependencies = [
  "anyhow",
  "clap",
@@ -959,7 +792,7 @@ dependencies = [
 
 [[package]]
 name = "codex-cli"
-version = "0.0.0"
+version = "0.53.0"
 dependencies = [
  "anyhow",
  "assert_cmd",
@@ -996,7 +829,7 @@ dependencies = [
 
 [[package]]
 name = "codex-cloud-tasks"
-version = "0.0.0"
+version = "0.53.0"
 dependencies = [
  "anyhow",
  "async-trait",
@@ -1022,7 +855,7 @@ dependencies = [
 
 [[package]]
 name = "codex-cloud-tasks-client"
-version = "0.0.0"
+version = "0.53.0"
 dependencies = [
  "anyhow",
  "async-trait",
@@ -1037,19 +870,22 @@ dependencies = [
 
 [[package]]
 name = "codex-common"
-version = "0.0.0"
+version = "0.53.0"
 dependencies = [
  "clap",
  "codex-app-server-protocol",
+ "codex-build-info",
  "codex-core",
  "codex-protocol",
+ "reqwest",
  "serde",
+ "serde_json",
  "toml",
 ]
 
 [[package]]
 name = "codex-core"
-version = "0.0.0"
+version = "0.53.0"
 dependencies = [
  "anyhow",
  "askama",
@@ -1128,7 +964,7 @@ dependencies = [
 
 [[package]]
 name = "codex-exec"
-version = "0.0.0"
+version = "0.53.0"
 dependencies = [
  "anyhow",
  "assert_cmd",
@@ -1161,7 +997,7 @@ dependencies = [
 
 [[package]]
 name = "codex-execpolicy"
-version = "0.0.0"
+version = "0.53.0"
 dependencies = [
  "allocative",
  "anyhow",
@@ -1181,7 +1017,7 @@ dependencies = [
 
 [[package]]
 name = "codex-feedback"
-version = "0.0.0"
+version = "0.53.0"
 dependencies = [
  "anyhow",
  "codex-protocol",
@@ -1192,7 +1028,7 @@ dependencies = [
 
 [[package]]
 name = "codex-file-search"
-version = "0.0.0"
+version = "0.53.0"
 dependencies = [
  "anyhow",
  "clap",
@@ -1205,7 +1041,7 @@ dependencies = [
 
 [[package]]
 name = "codex-git"
-version = "0.0.0"
+version = "0.53.0"
 dependencies = [
  "assert_matches",
  "once_cell",
@@ -1221,7 +1057,7 @@ dependencies = [
 
 [[package]]
 name = "codex-keyring-store"
-version = "0.0.0"
+version = "0.53.0"
 dependencies = [
  "keyring",
  "tracing",
@@ -1229,7 +1065,7 @@ dependencies = [
 
 [[package]]
 name = "codex-linux-sandbox"
-version = "0.0.0"
+version = "0.53.0"
 dependencies = [
  "clap",
  "codex-core",
@@ -1240,9 +1076,20 @@ dependencies = [
  "tokio",
 ]
 
+[[package]]
+name = "codex-litellm-model-session-telemetry"
+version = "0.1.0"
+dependencies = [
+ "chrono",
+ "once_cell",
+ "parking_lot",
+ "serde",
+ "serde_json",
+]
+
 [[package]]
 name = "codex-login"
-version = "0.0.0"
+version = "0.53.0"
 dependencies = [
  "anyhow",
  "base64",
@@ -1266,7 +1113,7 @@ dependencies = [
 
 [[package]]
 name = "codex-mcp-server"
-version = "0.0.0"
+version = "0.53.0"
 dependencies = [
  "anyhow",
  "assert_cmd",
@@ -1293,7 +1140,7 @@ dependencies = [
 
 [[package]]
 name = "codex-ollama"
-version = "0.0.0"
+version = "0.53.0"
 dependencies = [
  "assert_matches",
  "async-stream",
@@ -1309,7 +1156,7 @@ dependencies = [
 
 [[package]]
 name = "codex-otel"
-version = "0.0.0"
+version = "0.53.0"
 dependencies = [
  "chrono",
  "codex-app-server-protocol",
@@ -1330,14 +1177,14 @@ dependencies = [
 
 [[package]]
 name = "codex-process-hardening"
-version = "0.0.0"
+version = "0.53.0"
 dependencies = [
  "libc",
 ]
 
 [[package]]
 name = "codex-protocol"
-version = "0.0.0"
+version = "0.53.0"
 dependencies = [
  "anyhow",
  "base64",
@@ -1363,7 +1210,7 @@ dependencies = [
 
 [[package]]
 name = "codex-protocol-ts"
-version = "0.0.0"
+version = "0.53.0"
 dependencies = [
  "anyhow",
  "clap",
@@ -1373,7 +1220,7 @@ dependencies = [
 
 [[package]]
 name = "codex-responses-api-proxy"
-version = "0.0.0"
+version = "0.53.0"
 dependencies = [
  "anyhow",
  "clap",
@@ -1389,7 +1236,7 @@ dependencies = [
 
 [[package]]
 name = "codex-rmcp-client"
-version = "0.0.0"
+version = "0.53.0"
 dependencies = [
  "anyhow",
  "axum",
@@ -1418,7 +1265,7 @@ dependencies = [
 
 [[package]]
 name = "codex-stdio-to-uds"
-version = "0.0.0"
+version = "0.53.0"
 dependencies = [
  "anyhow",
  "assert_cmd",
@@ -1429,7 +1276,7 @@ dependencies = [
 
 [[package]]
 name = "codex-tui"
-version = "0.0.0"
+version = "0.53.0"
 dependencies = [
  "anyhow",
  "arboard",
@@ -1445,6 +1292,7 @@ dependencies = [
  "codex-core",
  "codex-feedback",
  "codex-file-search",
+ "codex-litellm-model-session-telemetry",
  "codex-login",
  "codex-ollama",
  "codex-protocol",
@@ -1491,7 +1339,7 @@ dependencies = [
 
 [[package]]
 name = "codex-utils-cache"
-version = "0.0.0"
+version = "0.53.0"
 dependencies = [
  "lru",
  "sha1",
@@ -1500,7 +1348,7 @@ dependencies = [
 
 [[package]]
 name = "codex-utils-image"
-version = "0.0.0"
+version = "0.53.0"
 dependencies = [
  "base64",
  "codex-utils-cache",
@@ -1512,7 +1360,7 @@ dependencies = [
 
 [[package]]
 name = "codex-utils-json-to-toml"
-version = "0.0.0"
+version = "0.53.0"
 dependencies = [
  "pretty_assertions",
  "serde_json",
@@ -1521,7 +1369,7 @@ dependencies = [
 
 [[package]]
 name = "codex-utils-pty"
-version = "0.0.0"
+version = "0.53.0"
 dependencies = [
  "anyhow",
  "portable-pty",
@@ -1530,7 +1378,7 @@ dependencies = [
 
 [[package]]
 name = "codex-utils-readiness"
-version = "0.0.0"
+version = "0.53.0"
 dependencies = [
  "assert_matches",
  "async-trait",
@@ -1541,11 +1389,11 @@ dependencies = [
 
 [[package]]
 name = "codex-utils-string"
-version = "0.0.0"
+version = "0.53.0"
 
 [[package]]
 name = "codex-utils-tokenizer"
-version = "0.0.0"
+version = "0.53.0"
 dependencies = [
  "anyhow",
  "pretty_assertions",
@@ -1680,7 +1528,7 @@ checksum = "773648b94d0e5d620f64f280777445740e61fe701025087ec8b57f45c791888b"
 
 [[package]]
 name = "core_test_support"
-version = "0.0.0"
+version = "0.53.0"
 dependencies = [
  "anyhow",
  "assert_cmd",
@@ -1885,35 +1733,6 @@ dependencies = [
  "syn 2.0.104",
 ]
 
-[[package]]
-name = "dbus"
-version = "0.9.9"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "190b6255e8ab55a7b568df5a883e9497edc3e4821c06396612048b430e5ad1e9"
-dependencies = [
- "libc",
- "libdbus-sys",
- "windows-sys 0.59.0",
-]
-
-[[package]]
-name = "dbus-secret-service"
-version = "4.1.0"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "708b509edf7889e53d7efb0ffadd994cc6c2345ccb62f55cfd6b0682165e4fa6"
-dependencies = [
- "aes",
- "block-padding",
- "cbc",
- "dbus",
- "fastrand",
- "hkdf",
- "num",
- "once_cell",
- "sha2",
- "zeroize",
-]
-
 [[package]]
 name = "deadpool"
 version = "0.12.3"
@@ -2046,7 +1865,6 @@ checksum = "9ed9a281f7bc9b7576e61468ba615a66a5c8cfdff42420a70aa82701a3b1e292"
 dependencies = [
  "block-buffer",
  "crypto-common",
- "subtle",
 ]
 
 [[package]]
@@ -2208,21 +2026,6 @@ version = "1.0.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "34aa73646ffb006b8f5147f3dc182bd4bcb190227ce861fc4a4844bf8e3cb2c0"
 
-[[package]]
-name = "encoding_rs"
-version = "0.8.35"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "75030f3c4f45dafd7586dd6780965a8c7e8e285a5ecb86713e63a79c5b2766f3"
-dependencies = [
- "cfg-if",
-]
-
-[[package]]
-name = "endi"
-version = "1.1.0"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "a3d8a32ae18130a3c84dd492d4215c3d913c3b07c6b63c2eb3eb7ff1101ab7bf"
-
 [[package]]
 name = "endian-type"
 version = "0.1.2"
@@ -2236,7 +2039,6 @@ source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "1027f7680c853e056ebcec683615fb6fbbc07dbaa13b4d5d9442b146ded4ecef"
 dependencies = [
  "enumflags2_derive",
- "serde",
 ]
 
 [[package]]
@@ -2491,21 +2293,6 @@ version = "0.1.5"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "d9c4f5dac5e15c24eb999c26181a6ca40b39fe946cbe4c263c7209467bc83af2"
 
-[[package]]
-name = "foreign-types"
-version = "0.3.2"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "f6f339eb8adc052cd2ca78910fda869aefa38d22d5cb648e6485e4d3fc06f3b1"
-dependencies = [
- "foreign-types-shared",
-]
-
-[[package]]
-name = "foreign-types-shared"
-version = "0.1.1"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "00b0228411908ca8685dba7fc2cdd70ec9990a6e753e89b6ac91a84c40fbaf4b"
-
 [[package]]
 name = "form_urlencoded"
 version = "1.2.1"
@@ -2572,19 +2359,6 @@ version = "0.3.31"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "9e5c1b78ca4aae1ac06c48a526a655760685149f0d465d21f37abfe57ce075c6"
 
-[[package]]
-name = "futures-lite"
-version = "2.6.1"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "f78e10609fe0e0b3f4157ffab1876319b5b0db102a2c60dc4626306dc46b44ad"
-dependencies = [
- "fastrand",
- "futures-core",
- "futures-io",
- "parking",
- "pin-project-lite",
-]
-
 [[package]]
 name = "futures-macro"
 version = "0.3.31"
@@ -2784,24 +2558,6 @@ version = "0.4.3"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "7f24254aa9a54b5c858eaee2f5bccdb46aaf0e486a595ed5fd8f86ba55232a70"
 
-[[package]]
-name = "hkdf"
-version = "0.12.4"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "7b5f8eb2ad728638ea2c7d47a21db23b7b58a72ed6a38256b8a1849f15fbbdf7"
-dependencies = [
- "hmac",
-]
-
-[[package]]
-name = "hmac"
-version = "0.12.1"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "6c49c37c09c17a53d937dfbb742eb3a961d65a994e6bcdcf37e7399d0cc8ab5e"
-dependencies = [
- "digest",
-]
-
 [[package]]
 name = "home"
 version = "0.5.11"
@@ -2909,13 +2665,13 @@ dependencies = [
  "http",
  "hyper",
  "hyper-util",
- "rustls",
+ "rustls 0.23.29",
  "rustls-native-certs",
  "rustls-pki-types",
  "tokio",
  "tokio-rustls",
  "tower-service",
- "webpki-roots",
+ "webpki-roots 1.0.2",
 ]
 
 [[package]]
@@ -2931,22 +2687,6 @@ dependencies = [
  "tower-service",
 ]
 
-[[package]]
-name = "hyper-tls"
-version = "0.6.0"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "70206fc6890eaca9fde8a0bf71caa2ddfc9fe045ac9e5c70df101a7dbde866e0"
-dependencies = [
- "bytes",
- "http-body-util",
- "hyper",
- "hyper-util",
- "native-tls",
- "tokio",
- "tokio-native-tls",
- "tower-service",
-]
-
 [[package]]
 name = "hyper-util"
 version = "0.1.16"
@@ -2966,11 +2706,9 @@ dependencies = [
  "percent-encoding",
  "pin-project-lite",
  "socket2 0.6.0",
- "system-configuration",
  "tokio",
  "tower-service",
  "tracing",
- "windows-registry",
 ]
 
 [[package]]
@@ -3236,16 +2974,6 @@ dependencies = [
  "libc",
 ]
 
-[[package]]
-name = "inout"
-version = "0.1.4"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "879f10e63c20629ecabbb64a8010319738c66a5cd0c29b02d63d272b03751d01"
-dependencies = [
- "block-padding",
- "generic-array",
-]
-
 [[package]]
 name = "insta"
 version = "1.43.2"
@@ -3424,15 +3152,7 @@ version = "3.6.3"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "eebcc3aff044e5944a8fbaf69eb277d11986064cba30c468730e8b9909fb551c"
 dependencies = [
- "byteorder",
- "dbus-secret-service",
- "linux-keyutils",
  "log",
- "secret-service",
- "security-framework 2.11.1",
- "security-framework 3.5.1",
- "windows-sys 0.60.2",
- "zbus",
  "zeroize",
 ]
 
@@ -3510,15 +3230,6 @@ version = "0.2.175"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "6a82ae493e598baaea5209805c49bbf2ea7de956d50d7da0da1164f9c6d28543"
 
-[[package]]
-name = "libdbus-sys"
-version = "0.2.6"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "5cbe856efeb50e4681f010e9aaa2bf0a644e10139e54cde10fc83a307c23bd9f"
-dependencies = [
- "pkg-config",
-]
-
 [[package]]
 name = "libm"
 version = "0.2.15"
@@ -3535,16 +3246,6 @@ dependencies = [
  "libc",
 ]
 
-[[package]]
-name = "linux-keyutils"
-version = "0.2.4"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "761e49ec5fd8a5a463f9b84e877c373d888935b71c6be78f3767fe2ae6bed18e"
-dependencies = [
- "bitflags 2.10.0",
- "libc",
-]
-
 [[package]]
 name = "linux-raw-sys"
 version = "0.4.15"
@@ -3653,7 +3354,7 @@ checksum = "47e1ffaa40ddd1f3ed91f717a33c8c0ee23fff369e3aa8772b9605cc1d22f4c3"
 
 [[package]]
 name = "mcp-types"
-version = "0.0.0"
+version = "0.53.0"
 dependencies = [
  "schemars 0.8.22",
  "serde",
@@ -3663,7 +3364,7 @@ dependencies = [
 
 [[package]]
 name = "mcp_test_support"
-version = "0.0.0"
+version = "0.53.0"
 dependencies = [
  "anyhow",
  "assert_cmd",
@@ -3765,23 +3466,6 @@ dependencies = [
  "serde",
 ]
 
-[[package]]
-name = "native-tls"
-version = "0.2.14"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "87de3442987e9dbec73158d5c715e7ad9072fda936bb03d19d7fa10e00520f0e"
-dependencies = [
- "libc",
- "log",
- "openssl",
- "openssl-probe",
- "openssl-sys",
- "schannel",
- "security-framework 2.11.1",
- "security-framework-sys",
- "tempfile",
-]
-
 [[package]]
 name = "ndk-context"
 version = "0.1.1"
@@ -3798,34 +3482,21 @@ checksum = "650eef8c711430f1a879fdd01d4745a7deea475becfb90269c06775983bbf086"
 name = "nibble_vec"
 version = "0.1.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "77a5d83df9f36fe23f0c3648c6bbb8b0298bb5f1939c8f2704431371f4b84d43"
-dependencies = [
- "smallvec",
-]
-
-[[package]]
-name = "nix"
-version = "0.28.0"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "ab2156c4fce2f8df6c499cc1c763e4394b7482525bf2a9701c9d79d215f519e4"
-dependencies = [
- "bitflags 2.10.0",
- "cfg-if",
- "cfg_aliases 0.1.1",
- "libc",
+checksum = "77a5d83df9f36fe23f0c3648c6bbb8b0298bb5f1939c8f2704431371f4b84d43"
+dependencies = [
+ "smallvec",
 ]
 
 [[package]]
 name = "nix"
-version = "0.29.0"
+version = "0.28.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "71e2746dc3a24dd78b3cfcb7be93368c6de9963d30f43a6a73998a9cf4b17b46"
+checksum = "ab2156c4fce2f8df6c499cc1c763e4394b7482525bf2a9701c9d79d215f519e4"
 dependencies = [
  "bitflags 2.10.0",
  "cfg-if",
- "cfg_aliases 0.2.1",
+ "cfg_aliases 0.1.1",
  "libc",
- "memoffset 0.9.1",
 ]
 
 [[package]]
@@ -3899,20 +3570,6 @@ dependencies = [
  "unicode-segmentation",
 ]
 
-[[package]]
-name = "num"
-version = "0.4.3"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "35bd024e8b2ff75562e5f34e7f4905839deb4b22955ef5e73d2fea1b9813cb23"
-dependencies = [
- "num-bigint",
- "num-complex",
- "num-integer",
- "num-iter",
- "num-rational",
- "num-traits",
-]
-
 [[package]]
 name = "num-bigint"
 version = "0.4.6"
@@ -3923,15 +3580,6 @@ dependencies = [
  "num-traits",
 ]
 
-[[package]]
-name = "num-complex"
-version = "0.4.6"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "73f88a1307638156682bada9d7604135552957b7818057dcef22705b4d509495"
-dependencies = [
- "num-traits",
-]
-
 [[package]]
 name = "num-conv"
 version = "0.1.0"
@@ -3947,28 +3595,6 @@ dependencies = [
  "num-traits",
 ]
 
-[[package]]
-name = "num-iter"
-version = "0.1.45"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "1429034a0490724d0075ebb2bc9e875d6503c3cf69e235a8941aa757d83ef5bf"
-dependencies = [
- "autocfg",
- "num-integer",
- "num-traits",
-]
-
-[[package]]
-name = "num-rational"
-version = "0.4.2"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "f83d14da390562dca69fc84082e73e548e1ad308d24accdedd2720017cb37824"
-dependencies = [
- "num-bigint",
- "num-integer",
- "num-traits",
-]
-
 [[package]]
 name = "num-traits"
 version = "0.2.19"
@@ -4111,32 +3737,6 @@ version = "1.70.1"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "a4895175b425cb1f87721b59f0f286c2092bd4af812243672510e1ac53e2e0ad"
 
-[[package]]
-name = "openssl"
-version = "0.10.73"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "8505734d46c8ab1e19a1dce3aef597ad87dcb4c37e7188231769bd6bd51cebf8"
-dependencies = [
- "bitflags 2.10.0",
- "cfg-if",
- "foreign-types",
- "libc",
- "once_cell",
- "openssl-macros",
- "openssl-sys",
-]
-
-[[package]]
-name = "openssl-macros"
-version = "0.1.1"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "a948666b637a0f465e8564c73e89d4dde00d72d4d473cc972f390fc3dcee7d9c"
-dependencies = [
- "proc-macro2",
- "quote",
- "syn 2.0.104",
-]
-
 [[package]]
 name = "openssl-probe"
 version = "0.1.6"
@@ -4269,16 +3869,6 @@ version = "0.2.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "04744f49eae99ab78e0d5c0b603ab218f515ea8cfe5a456d7629ad883a3b6e7d"
 
-[[package]]
-name = "ordered-stream"
-version = "0.2.0"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "9aa2b01e1d916879f73a53d01d1d6cee68adbb31d6d9177a8cfce093cced1d50"
-dependencies = [
- "futures-core",
- "pin-project-lite",
-]
-
 [[package]]
 name = "os_info"
 version = "3.12.0"
@@ -4413,17 +4003,6 @@ version = "0.1.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "8b870d8c151b6f2fb93e84a13146138f05d02ed11c7e7c54f8826aaaf7c9f184"
 
-[[package]]
-name = "piper"
-version = "0.2.4"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "96c8c490f422ef9a4efd2cb5b42b76c8613d7e7dfc1caf667b8a3350a5acc066"
-dependencies = [
- "atomic-waker",
- "fastrand",
- "futures-io",
-]
-
 [[package]]
 name = "pkg-config"
 version = "0.3.32"
@@ -4456,20 +4035,6 @@ dependencies = [
  "miniz_oxide",
 ]
 
-[[package]]
-name = "polling"
-version = "3.11.0"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "5d0e4f59085d47d8241c88ead0f274e8a0cb551f3625263c05eb8dd897c34218"
-dependencies = [
- "cfg-if",
- "concurrent-queue",
- "hermit-abi",
- "pin-project-lite",
- "rustix 1.0.8",
- "windows-sys 0.61.1",
-]
-
 [[package]]
 name = "portable-atomic"
 version = "1.11.1"
@@ -4578,15 +4143,6 @@ dependencies = [
  "yansi",
 ]
 
-[[package]]
-name = "proc-macro-crate"
-version = "3.4.0"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "219cb19e96be00ab2e37d6e299658a0cfa83e52429179969b0f0121b4ac46983"
-dependencies = [
- "toml_edit",
-]
-
 [[package]]
 name = "proc-macro2"
 version = "1.0.95"
@@ -4688,7 +4244,7 @@ dependencies = [
  "quinn-proto",
  "quinn-udp",
  "rustc-hash 2.1.1",
- "rustls",
+ "rustls 0.23.29",
  "socket2 0.6.0",
  "thiserror 2.0.16",
  "tokio",
@@ -4708,7 +4264,7 @@ dependencies = [
  "rand 0.9.2",
  "ring",
  "rustc-hash 2.1.1",
- "rustls",
+ "rustls 0.23.29",
  "rustls-pki-types",
  "slab",
  "thiserror 2.0.16",
@@ -4944,26 +4500,21 @@ checksum = "d429f34c8092b2d42c7c93cec323bb4adeb7c67698f70839adec842ec10c7ceb"
 dependencies = [
  "base64",
  "bytes",
- "encoding_rs",
  "futures-channel",
  "futures-core",
  "futures-util",
- "h2",
  "http",
  "http-body",
  "http-body-util",
  "hyper",
  "hyper-rustls",
- "hyper-tls",
  "hyper-util",
  "js-sys",
  "log",
- "mime",
- "native-tls",
  "percent-encoding",
  "pin-project-lite",
  "quinn",
- "rustls",
+ "rustls 0.23.29",
  "rustls-native-certs",
  "rustls-pki-types",
  "serde",
@@ -4971,7 +4522,6 @@ dependencies = [
  "serde_urlencoded",
  "sync_wrapper",
  "tokio",
- "tokio-native-tls",
  "tokio-rustls",
  "tokio-util",
  "tower",
@@ -4982,7 +4532,7 @@ dependencies = [
  "wasm-bindgen-futures",
  "wasm-streams",
  "web-sys",
- "webpki-roots",
+ "webpki-roots 1.0.2",
 ]
 
 [[package]]
@@ -5099,16 +4649,31 @@ dependencies = [
  "windows-sys 0.60.2",
 ]
 
+[[package]]
+name = "rustls"
+version = "0.22.4"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "bf4ef73721ac7bcd79b2b315da7779d8fc09718c6b3d2d1b2d94850eb8c18432"
+dependencies = [
+ "log",
+ "ring",
+ "rustls-pki-types",
+ "rustls-webpki 0.102.8",
+ "subtle",
+ "zeroize",
+]
+
 [[package]]
 name = "rustls"
 version = "0.23.29"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "2491382039b29b9b11ff08b76ff6c97cf287671dbb74f0be44bda389fffe9bd1"
 dependencies = [
+ "log",
  "once_cell",
  "ring",
  "rustls-pki-types",
- "rustls-webpki",
+ "rustls-webpki 0.103.4",
  "subtle",
  "zeroize",
 ]
@@ -5122,7 +4687,7 @@ dependencies = [
  "openssl-probe",
  "rustls-pki-types",
  "schannel",
- "security-framework 3.5.1",
+ "security-framework",
 ]
 
 [[package]]
@@ -5135,6 +4700,17 @@ dependencies = [
  "zeroize",
 ]
 
+[[package]]
+name = "rustls-webpki"
+version = "0.102.8"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "64ca1bc8749bd4cf37b5ce386cc146580777b4e8572c7b97baf22c83f444bee9"
+dependencies = [
+ "ring",
+ "rustls-pki-types",
+ "untrusted",
+]
+
 [[package]]
 name = "rustls-webpki"
 version = "0.103.4"
@@ -5332,38 +4908,6 @@ dependencies = [
  "libc",
 ]
 
-[[package]]
-name = "secret-service"
-version = "4.0.0"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "e4d35ad99a181be0a60ffcbe85d680d98f87bdc4d7644ade319b87076b9dbfd4"
-dependencies = [
- "aes",
- "cbc",
- "futures-util",
- "generic-array",
- "hkdf",
- "num",
- "once_cell",
- "rand 0.8.5",
- "serde",
- "sha2",
- "zbus",
-]
-
-[[package]]
-name = "security-framework"
-version = "2.11.1"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "897b2245f0b511c87893af39b033e5ca9cce68824c4d7e7630b5a1d339658d02"
-dependencies = [
- "bitflags 2.10.0",
- "core-foundation 0.9.4",
- "core-foundation-sys",
- "libc",
- "security-framework-sys",
-]
-
 [[package]]
 name = "security-framework"
 version = "3.5.1"
@@ -5400,8 +4944,8 @@ source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "5484316556650182f03b43d4c746ce0e3e48074a21e2f51244b648b6542e1066"
 dependencies = [
  "httpdate",
- "native-tls",
  "reqwest",
+ "rustls 0.22.4",
  "sentry-backtrace",
  "sentry-contexts",
  "sentry-core",
@@ -5410,6 +4954,7 @@ dependencies = [
  "sentry-tracing",
  "tokio",
  "ureq",
+ "webpki-roots 0.26.11",
 ]
 
 [[package]]
@@ -6063,27 +5608,6 @@ dependencies = [
  "libc",
 ]
 
-[[package]]
-name = "system-configuration"
-version = "0.6.1"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "3c879d448e9d986b661742763247d3693ed13609438cf3d006f51f5368a5ba6b"
-dependencies = [
- "bitflags 2.10.0",
- "core-foundation 0.9.4",
- "system-configuration-sys",
-]
-
-[[package]]
-name = "system-configuration-sys"
-version = "0.6.0"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "8e1d1b10ced5ca923a1fcb8d03e96b8d3268065d724548c0211415ff6ac6bac4"
-dependencies = [
- "core-foundation-sys",
- "libc",
-]
-
 [[package]]
 name = "tempfile"
 version = "3.23.0"
@@ -6363,23 +5887,13 @@ dependencies = [
  "syn 2.0.104",
 ]
 
-[[package]]
-name = "tokio-native-tls"
-version = "0.3.1"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "bbae76ab933c85776efabc971569dd6119c580d8f5d448769dec1764bf796ef2"
-dependencies = [
- "native-tls",
- "tokio",
-]
-
 [[package]]
 name = "tokio-rustls"
 version = "0.26.2"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "8e727b36a1a0e8b74c376ac2211e40c2c8af09fb4013c60d910495810f008e9b"
 dependencies = [
- "rustls",
+ "rustls 0.23.29",
  "tokio",
 ]
 
@@ -6821,9 +6335,11 @@ checksum = "02d1a66277ed75f640d608235660df48c8e3c19f3b4edb6a263315626cc3c01d"
 dependencies = [
  "base64",
  "log",
- "native-tls",
  "once_cell",
+ "rustls 0.23.29",
+ "rustls-pki-types",
  "url",
+ "webpki-roots 0.26.11",
 ]
 
 [[package]]
@@ -7070,6 +6586,15 @@ dependencies = [
  "web-sys",
 ]
 
+[[package]]
+name = "webpki-roots"
+version = "0.26.11"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "521bc38abb08001b01866da9f51eb7c5d647a19260e00054a8c7fd5f9e57f7a9"
+dependencies = [
+ "webpki-roots 1.0.2",
+]
+
 [[package]]
 name = "webpki-roots"
 version = "1.0.2"
@@ -7224,17 +6749,6 @@ dependencies = [
  "windows-link 0.1.3",
 ]
 
-[[package]]
-name = "windows-registry"
-version = "0.5.3"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "5b8a9ed28765efc97bbc954883f4e6796c33a06546ebafacbabee9696967499e"
-dependencies = [
- "windows-link 0.1.3",
- "windows-result",
- "windows-strings",
-]
-
 [[package]]
 name = "windows-result"
 version = "0.3.4"
@@ -7628,16 +7142,6 @@ version = "0.13.1"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "ec107c4503ea0b4a98ef47356329af139c0a4f7750e621cf2973cd3385ebcb3d"
 
-[[package]]
-name = "xdg-home"
-version = "1.3.0"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "ec1cdab258fb55c0da61328dc52c8764709b249011b2cad0454c72f0bf10a1f6"
-dependencies = [
- "libc",
- "windows-sys 0.59.0",
-]
-
 [[package]]
 name = "yansi"
 version = "1.0.1"
@@ -7668,68 +7172,6 @@ dependencies = [
  "synstructure",
 ]
 
-[[package]]
-name = "zbus"
-version = "4.4.0"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "bb97012beadd29e654708a0fdb4c84bc046f537aecfde2c3ee0a9e4b4d48c725"
-dependencies = [
- "async-broadcast",
- "async-executor",
- "async-fs",
- "async-io",
- "async-lock",
- "async-process",
- "async-recursion",
- "async-task",
- "async-trait",
- "blocking",
- "enumflags2",
- "event-listener",
- "futures-core",
- "futures-sink",
- "futures-util",
- "hex",
- "nix 0.29.0",
- "ordered-stream",
- "rand 0.8.5",
- "serde",
- "serde_repr",
- "sha1",
- "static_assertions",
- "tracing",
- "uds_windows",
- "windows-sys 0.52.0",
- "xdg-home",
- "zbus_macros",
- "zbus_names",
- "zvariant",
-]
-
-[[package]]
-name = "zbus_macros"
-version = "4.4.0"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "267db9407081e90bbfa46d841d3cbc60f59c0351838c4bc65199ecd79ab1983e"
-dependencies = [
- "proc-macro-crate",
- "proc-macro2",
- "quote",
- "syn 2.0.104",
- "zvariant_utils",
-]
-
-[[package]]
-name = "zbus_names"
-version = "3.0.0"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "4b9b1fef7d021261cc16cba64c351d291b715febe0fa10dc3a443ac5a5022e6c"
-dependencies = [
- "serde",
- "static_assertions",
- "zvariant",
-]
-
 [[package]]
 name = "zerocopy"
 version = "0.8.26"
@@ -7776,20 +7218,6 @@ name = "zeroize"
 version = "1.8.1"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "ced3678a2879b30306d323f4542626697a464a97c0a07c9aebf7ebca65cd4dde"
-dependencies = [
- "zeroize_derive",
-]
-
-[[package]]
-name = "zeroize_derive"
-version = "1.4.2"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "ce36e65b0d2999d2aafac989fb249189a141aee1f53c612c1f37d72631959f69"
-dependencies = [
- "proc-macro2",
- "quote",
- "syn 2.0.104",
-]
 
 [[package]]
 name = "zerotrie"
@@ -7839,40 +7267,3 @@ checksum = "2c9e525af0a6a658e031e95f14b7f889976b74a11ba0eca5a5fc9ac8a1c43a6a"
 dependencies = [
  "zune-core",
 ]
-
-[[package]]
-name = "zvariant"
-version = "4.2.0"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "2084290ab9a1c471c38fc524945837734fbf124487e105daec2bb57fd48c81fe"
-dependencies = [
- "endi",
- "enumflags2",
- "serde",
- "static_assertions",
- "zvariant_derive",
-]
-
-[[package]]
-name = "zvariant_derive"
-version = "4.2.0"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "73e2ba546bda683a90652bac4a279bc146adad1386f25379cf73200d2002c449"
-dependencies = [
- "proc-macro-crate",
- "proc-macro2",
- "quote",
- "syn 2.0.104",
- "zvariant_utils",
-]
-
-[[package]]
-name = "zvariant_utils"
-version = "2.1.0"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "c51bcff7cc3dbb5055396bcf774748c3dab426b4b8659046963523cee4808340"
-dependencies = [
- "proc-macro2",
- "quote",
- "syn 2.0.104",
-]
diff --git a/codex-rs/Cargo.toml b/codex-rs/Cargo.toml
index 06fe7cba..cb013279 100644
--- a/codex-rs/Cargo.toml
+++ b/codex-rs/Cargo.toml
@@ -2,6 +2,7 @@
 members = [
     "backend-client",
     "ansi-escape",
+    "build-info",
     "async-utils",
     "app-server",
     "app-server-protocol",
@@ -13,6 +14,7 @@ members = [
     "cloud-tasks-client",
     "cli",
     "common",
+    "codex-litellm-model-session-telemetry",
     "core",
     "exec",
     "execpolicy",
@@ -60,6 +62,7 @@ codex-apply-patch = { path = "apply-patch" }
 codex-arg0 = { path = "arg0" }
 codex-async-utils = { path = "async-utils" }
 codex-backend-client = { path = "backend-client" }
+codex-build-info = { path = "build-info" }
 codex-chatgpt = { path = "chatgpt" }
 codex-common = { path = "common" }
 codex-core = { path = "core" }
@@ -68,6 +71,7 @@ codex-feedback = { path = "feedback" }
 codex-file-search = { path = "file-search" }
 codex-git = { path = "utils/git" }
 codex-keyring-store = { path = "keyring-store" }
+codex-litellm-model-session-telemetry = { path = "codex-litellm-model-session-telemetry" }
 codex-linux-sandbox = { path = "linux-sandbox" }
 codex-login = { path = "login" }
 codex-mcp-server = { path = "mcp-server" }
@@ -142,7 +146,8 @@ mime_guess = "2.0.5"
 multimap = "0.10.0"
 notify = "8.2.0"
 nucleo-matcher = "0.3.1"
-openssl-sys = "*"
+openssl = { version = "0.10", features = ["vendored"] }
+openssl-sys = { version = "0.9", features = ["vendored"] }
 opentelemetry = "0.30.0"
 opentelemetry-appender-tracing = "0.30.0"
 opentelemetry-otlp = "0.30.0"
@@ -161,11 +166,18 @@ rand = "0.9"
 ratatui = "0.29.0"
 ratatui-macros = "0.6.0"
 regex-lite = "0.1.7"
-reqwest = "0.12"
+reqwest = { version = "0.12", default-features = false, features = ["blocking", "json", "rustls-tls", "stream"] }
 rmcp = { version = "0.8.3", default-features = false }
 schemars = "0.8.22"
 seccompiler = "0.5.0"
-sentry = "0.34.0"
+sentry = { version = "0.34.0", default-features = false, features = [
+    "backtrace",
+    "contexts",
+    "debug-images",
+    "panic",
+    "reqwest",
+    "rustls",
+] }
 serde = "1"
 serde_json = "1"
 serde_with = "3.14"
@@ -194,7 +206,7 @@ toml_edit = "0.23.4"
 tonic = "0.13.1"
 tracing = "0.1.41"
 tracing-appender = "0.2.3"
-tracing-subscriber = "0.3.20"
+tracing-subscriber = { version = "0.3.20", features = ["env-filter", "fmt", "registry"] }
 tracing-test = "0.2.5"
 tree-sitter = "0.25.10"
 tree-sitter-bash = "0.25"
diff --git a/codex-rs/app-server/src/codex_message_processor.rs b/codex-rs/app-server/src/codex_message_processor.rs
index 99310e19..1e2aa8d2 100644
--- a/codex-rs/app-server/src/codex_message_processor.rs
+++ b/codex-rs/app-server/src/codex_message_processor.rs
@@ -1781,6 +1781,7 @@ async fn derive_config_from_params(
         tools_web_search_request: None,
         experimental_sandbox_command_assessment: None,
         additional_writable_roots: Vec::new(),
+        telemetry_enabled: None,
     };
 
     let cli_overrides = cli_overrides
diff --git a/codex-rs/app-server/src/models.rs b/codex-rs/app-server/src/models.rs
index be47cbc2..36d3f09f 100644
--- a/codex-rs/app-server/src/models.rs
+++ b/codex-rs/app-server/src/models.rs
@@ -16,7 +16,7 @@ fn model_from_preset(preset: ModelPreset) -> Model {
         id: preset.id.to_string(),
         model: preset.model.to_string(),
         display_name: preset.display_name.to_string(),
-        description: preset.description.to_string(),
+        description: preset.description.clone().unwrap_or_default(),
         supported_reasoning_efforts: reasoning_efforts_from_preset(
             preset.supported_reasoning_efforts,
         ),
diff --git a/codex-rs/cli/src/main.rs b/codex-rs/cli/src/main.rs
index 9b2d106c..32363ae2 100644
--- a/codex-rs/cli/src/main.rs
+++ b/codex-rs/cli/src/main.rs
@@ -519,6 +519,7 @@ async fn cli_main(codex_linux_sandbox_exe: Option<PathBuf>) -> anyhow::Result<()
                 let overrides = ConfigOverrides {
                     config_profile: interactive.config_profile.clone(),
                     tools_web_search_request: interactive.web_search.then_some(true),
+                    telemetry_enabled: None,
                     ..Default::default()
                 };
 
diff --git a/codex-rs/common/Cargo.toml b/codex-rs/common/Cargo.toml
index d8f30cc0..2c667b84 100644
--- a/codex-rs/common/Cargo.toml
+++ b/codex-rs/common/Cargo.toml
@@ -11,11 +11,14 @@ clap = { workspace = true, features = ["derive", "wrap_help"], optional = true }
 codex-core = { workspace = true }
 codex-protocol = { workspace = true }
 codex-app-server-protocol = { workspace = true }
+codex-build-info = { workspace = true }
 serde = { workspace = true, optional = true }
+serde_json = { workspace = true, optional = true }
+reqwest = { workspace = true, features = ["blocking", "json"], optional = true }
 toml = { workspace = true, optional = true }
 
 [features]
 # Separate feature so that `clap` is not a mandatory dependency.
-cli = ["clap", "serde", "toml"]
+cli = ["clap", "serde", "serde_json", "toml", "reqwest"]
 elapsed = []
 sandbox_summary = []
diff --git a/codex-rs/common/src/lib.rs b/codex-rs/common/src/lib.rs
index 276bfca0..c445cec5 100644
--- a/codex-rs/common/src/lib.rs
+++ b/codex-rs/common/src/lib.rs
@@ -30,6 +30,7 @@ pub use sandbox_summary::summarize_sandbox_policy;
 mod config_summary;
 
 pub use config_summary::create_config_summary_entries;
+pub mod litellm;
 // Shared fuzzy matcher (used by TUI selection popups and other UI filtering)
 pub mod fuzzy_match;
 // Shared model presets used by TUI and MCP server
diff --git a/codex-rs/common/src/model_presets.rs b/codex-rs/common/src/model_presets.rs
index b6d06438..3947ebb4 100644
--- a/codex-rs/common/src/model_presets.rs
+++ b/codex-rs/common/src/model_presets.rs
@@ -1,5 +1,9 @@
 use codex_app_server_protocol::AuthMode;
 use codex_core::protocol_config_types::ReasoningEffort;
+use reqwest::blocking::Client;
+use serde::Deserialize;
+use serde_json::Value;
+use std::collections::HashSet;
 
 /// A reasoning effort option that can be surfaced for a model.
 #[derive(Debug, Clone, Copy)]
@@ -11,16 +15,16 @@ pub struct ReasoningEffortPreset {
 }
 
 /// Metadata describing a Codex-supported model.
-#[derive(Debug, Clone, Copy)]
+#[derive(Debug, Clone)]
 pub struct ModelPreset {
     /// Stable identifier for the preset.
-    pub id: &'static str,
+    pub id: String,
     /// Model slug (e.g., "gpt-5").
-    pub model: &'static str,
+    pub model: String,
     /// Display name shown in UIs.
-    pub display_name: &'static str,
+    pub display_name: String,
     /// Short human description shown in UIs.
-    pub description: &'static str,
+    pub description: Option<String>,
     /// Reasoning effort applied when none is explicitly chosen.
     pub default_reasoning_effort: ReasoningEffort,
     /// Supported reasoning effort options.
@@ -29,68 +33,278 @@ pub struct ModelPreset {
     pub is_default: bool,
 }
 
-const PRESETS: &[ModelPreset] = &[
-    ModelPreset {
-        id: "gpt-5-codex",
-        model: "gpt-5-codex",
-        display_name: "gpt-5-codex",
-        description: "Optimized for coding tasks with many tools.",
-        default_reasoning_effort: ReasoningEffort::Medium,
-        supported_reasoning_efforts: &[
-            ReasoningEffortPreset {
-                effort: ReasoningEffort::Low,
-                description: "Fastest responses with limited reasoning",
-            },
-            ReasoningEffortPreset {
-                effort: ReasoningEffort::Medium,
-                description: "Dynamically adjusts reasoning based on the task",
-            },
-            ReasoningEffortPreset {
-                effort: ReasoningEffort::High,
-                description: "Maximizes reasoning depth for complex or ambiguous problems",
-            },
-        ],
-        is_default: true,
+const STANDARD_REASONING_PRESETS: &[ReasoningEffortPreset] = &[
+    ReasoningEffortPreset {
+        effort: ReasoningEffort::Low,
+        description: "Fastest responses with limited reasoning",
     },
-    ModelPreset {
-        id: "gpt-5",
-        model: "gpt-5",
-        display_name: "gpt-5",
-        description: "Broad world knowledge with strong general reasoning.",
-        default_reasoning_effort: ReasoningEffort::Medium,
-        supported_reasoning_efforts: &[
-            ReasoningEffortPreset {
-                effort: ReasoningEffort::Minimal,
-                description: "Fastest responses with little reasoning",
-            },
-            ReasoningEffortPreset {
-                effort: ReasoningEffort::Low,
-                description: "Balances speed with some reasoning; useful for straightforward queries and short explanations",
-            },
-            ReasoningEffortPreset {
-                effort: ReasoningEffort::Medium,
-                description: "Provides a solid balance of reasoning depth and latency for general-purpose tasks",
-            },
-            ReasoningEffortPreset {
-                effort: ReasoningEffort::High,
-                description: "Maximizes reasoning depth for complex or ambiguous problems",
-            },
-        ],
-        is_default: false,
+    ReasoningEffortPreset {
+        effort: ReasoningEffort::Medium,
+        description: "Dynamically adjusts reasoning based on the task",
+    },
+    ReasoningEffortPreset {
+        effort: ReasoningEffort::High,
+        description: "Maximizes reasoning depth for complex or ambiguous problems",
+    },
+];
+
+const GPT5_REASONING_PRESETS: &[ReasoningEffortPreset] = &[
+    ReasoningEffortPreset {
+        effort: ReasoningEffort::Minimal,
+        description: "Fastest responses with little reasoning",
+    },
+    ReasoningEffortPreset {
+        effort: ReasoningEffort::Low,
+        description: "Balances speed with some reasoning; useful for straightforward queries and short explanations",
+    },
+    ReasoningEffortPreset {
+        effort: ReasoningEffort::Medium,
+        description: "Provides a solid balance of reasoning depth and latency for general-purpose tasks",
+    },
+    ReasoningEffortPreset {
+        effort: ReasoningEffort::High,
+        description: "Maximizes reasoning depth for complex or ambiguous problems",
     },
 ];
 
 pub fn builtin_model_presets(_auth_mode: Option<AuthMode>) -> Vec<ModelPreset> {
-    PRESETS.to_vec()
+    vec![
+        ModelPreset {
+            id: "gpt-5-codex".to_string(),
+            model: "gpt-5-codex".to_string(),
+            display_name: "gpt-5-codex".to_string(),
+            description: Some("Optimized for coding tasks with many tools.".to_string()),
+            default_reasoning_effort: ReasoningEffort::Medium,
+            supported_reasoning_efforts: STANDARD_REASONING_PRESETS,
+            is_default: true,
+        },
+        ModelPreset {
+            id: "gpt-5".to_string(),
+            model: "gpt-5".to_string(),
+            display_name: "gpt-5".to_string(),
+            description: Some("Broad world knowledge with strong general reasoning.".to_string()),
+            default_reasoning_effort: ReasoningEffort::Medium,
+            supported_reasoning_efforts: GPT5_REASONING_PRESETS,
+            is_default: false,
+        },
+    ]
+}
+
+pub fn make_litellm_model_preset(model: &str, is_default: bool) -> ModelPreset {
+    ModelPreset {
+        id: model.to_string(),
+        model: model.to_string(),
+        display_name: model.to_string(),
+        description: None,
+        default_reasoning_effort: ReasoningEffort::Medium,
+        supported_reasoning_efforts: STANDARD_REASONING_PRESETS,
+        is_default,
+    }
+}
+
+pub const LITELLM_PROVIDER_ID: &str = "litellm";
+pub const LEGACY_LITELLM_PROVIDER_ID: &str = "litellm-direct";
+pub const LITELLM_DEFAULT_MODEL: &str = "";
+
+pub fn is_litellm_provider_id(provider_id: &str) -> bool {
+    matches!(
+        provider_id,
+        LITELLM_PROVIDER_ID | LEGACY_LITELLM_PROVIDER_ID
+    )
+}
+
+pub fn presets_for_provider(
+    provider_id: &str,
+    current_model: &str,
+    auth_mode: Option<AuthMode>,
+) -> Vec<ModelPreset> {
+    if matches!(
+        provider_id,
+        LITELLM_PROVIDER_ID | LEGACY_LITELLM_PROVIDER_ID
+    ) {
+        let mut presets = Vec::new();
+        presets.push(make_litellm_model_preset(current_model, true));
+        presets
+    } else {
+        builtin_model_presets(auth_mode)
+    }
+}
+
+#[derive(Debug, Deserialize)]
+struct LiteLlmModelEntry {
+    #[serde(default)]
+    id: Option<String>,
+    #[serde(default)]
+    model: Option<String>,
+    #[serde(default)]
+    description: Option<String>,
+    #[serde(default)]
+    display_name: Option<String>,
+}
+
+pub fn fetch_litellm_model_presets(
+    base_url: &str,
+    api_key: Option<&str>,
+    current_model: &str,
+) -> Result<Vec<ModelPreset>, String> {
+    let base = base_url.to_string();
+    let key = api_key.map(|k| k.to_string());
+    let current = current_model.to_string();
+
+    std::thread::spawn(move || {
+        fetch_litellm_model_presets_blocking(&base, key.as_deref(), &current)
+    })
+    .join()
+    .map_err(|_| "LiteLLM /models worker panicked".to_string())?
+}
+
+fn fetch_litellm_model_presets_blocking(
+    base_url: &str,
+    api_key: Option<&str>,
+    current_model: &str,
+) -> Result<Vec<ModelPreset>, String> {
+    let trimmed = base_url.trim_end_matches('/');
+    let url = format!("{trimmed}/models");
+
+    let client = Client::builder()
+        .user_agent("codex-litellm")
+        .build()
+        .map_err(|err| format!("failed to create HTTP client: {err}"))?;
+
+    let mut request = client.get(&url);
+    if let Some(token) = api_key {
+        request = request.bearer_auth(token);
+    }
+
+    let response = request
+        .send()
+        .map_err(|err| format!("LiteLLM /models request failed: {err}"))?;
+
+    if !response.status().is_success() {
+        return Err(format!(
+            "LiteLLM /models responded with status {}",
+            response.status()
+        ));
+    }
+
+    let payload: Value = response
+        .json()
+        .map_err(|err| format!("failed to decode LiteLLM /models response: {err}"))?;
+
+    let mut presets = extract_litellm_models(payload)?;
+    if presets.is_empty() {
+        return Err("LiteLLM /models returned an empty list".to_string());
+    }
+
+    sanitize_litellm_presets(&mut presets, current_model);
+
+    Ok(presets)
+}
+
+fn sanitize_litellm_presets(presets: &mut Vec<ModelPreset>, current_model: &str) {
+    let mut current_pos = None;
+    for (idx, preset) in presets.iter_mut().enumerate() {
+        if preset.model == current_model {
+            preset.is_default = true;
+            current_pos = Some(idx);
+        } else {
+            preset.is_default = false;
+        }
+    }
+
+    if let Some(pos) = current_pos {
+        if pos != 0 {
+            let preset = presets.remove(pos);
+            presets.insert(0, preset);
+        }
+    } else if !current_model.is_empty() {
+        // Ensure the current model is present as the default choice.
+        presets.insert(0, make_litellm_model_preset(current_model, true));
+    }
+}
+
+fn extract_litellm_models(payload: Value) -> Result<Vec<ModelPreset>, String> {
+    let array_value = payload
+        .get("data")
+        .or_else(|| payload.get("models"))
+        .or_else(|| payload.get("result"))
+        .cloned()
+        .unwrap_or(payload);
+
+    let items = match array_value {
+        Value::Array(values) => values,
+        other => return Err(format!("unexpected LiteLLM /models payload: {other:?}")),
+    };
+
+    let mut presets = Vec::new();
+    let mut seen: HashSet<String> = HashSet::new();
+    for item in items {
+        match item {
+            Value::String(slug) => {
+                if seen.insert(slug.clone()) {
+                    presets.push(make_litellm_model_preset(&slug, false));
+                }
+            }
+            Value::Object(map) => {
+                let entry: LiteLlmModelEntry = serde_json::from_value(Value::Object(map))
+                    .map_err(|err| format!("failed to parse LiteLLM model entry: {err}"))?;
+                if let Some(slug) = entry.id.or(entry.model) {
+                    if seen.insert(slug.clone()) {
+                        let mut preset = make_litellm_model_preset(&slug, false);
+                        if let Some(name) = entry.display_name.clone() {
+                            preset.display_name = name;
+                        }
+                        preset.description = entry.description;
+                        presets.push(preset);
+                    }
+                }
+            }
+            _ => {}
+        }
+    }
+    Ok(presets)
 }
 
 #[cfg(test)]
 mod tests {
     use super::*;
+    use serde_json::json;
 
     #[test]
     fn only_one_default_model_is_configured() {
-        let default_models = PRESETS.iter().filter(|preset| preset.is_default).count();
-        assert!(default_models == 1);
+        let default_models = builtin_model_presets(None)
+            .into_iter()
+            .filter(|preset| preset.is_default)
+            .count();
+        assert_eq!(default_models, 1);
+    }
+
+    #[test]
+    fn litellm_presets_include_current_model_first() {
+        let current = "or/minimax-m2:free";
+        let presets = presets_for_provider(LITELLM_PROVIDER_ID, current, None);
+        assert!(!presets.is_empty());
+        assert_eq!(presets[0].model, current);
+        assert!(presets[0].is_default);
+
+        let legacy = presets_for_provider(LEGACY_LITELLM_PROVIDER_ID, current, None);
+        assert!(!legacy.is_empty());
+        assert_eq!(legacy[0].model, current);
+        assert!(legacy[0].is_default);
+    }
+
+    #[test]
+    fn extract_models_handles_common_shapes() {
+        let payload = json!({
+            "data": [
+                { "id": "or/minimax-m2:free", "description": "MiniMax Lite" },
+                { "model": "vertex/gpt-oss-20b", "display_name": "GPT OSS 20B" },
+                "vertex/gpt-oss-120b"
+            ]
+        });
+        let presets = extract_litellm_models(payload).expect("parse models");
+        assert_eq!(presets.len(), 3);
+        assert_eq!(presets[0].model, "or/minimax-m2:free");
+        assert_eq!(presets[1].display_name, "GPT OSS 20B".to_string());
     }
 }
diff --git a/codex-rs/core/Cargo.toml b/codex-rs/core/Cargo.toml
index 2b48c088..e399201f 100644
--- a/codex-rs/core/Cargo.toml
+++ b/codex-rs/core/Cargo.toml
@@ -39,12 +39,7 @@ eventsource-stream = { workspace = true }
 futures = { workspace = true }
 http = { workspace = true }
 indexmap = { workspace = true }
-keyring = { workspace = true, features = [
-    "apple-native",
-    "crypto-rust",
-    "linux-native-async-persistent",
-    "windows-native",
-] }
+keyring = { workspace = true }
 libc = { workspace = true }
 mcp-types = { workspace = true }
 os_info = { workspace = true }
diff --git a/codex-rs/core/src/chat_completions.rs b/codex-rs/core/src/chat_completions.rs
index abb27d9b..02faab13 100644
--- a/codex-rs/core/src/chat_completions.rs
+++ b/codex-rs/core/src/chat_completions.rs
@@ -33,8 +33,7 @@ use std::task::Context;
 use std::task::Poll;
 use tokio::sync::mpsc;
 use tokio::time::timeout;
-use tracing::debug;
-use tracing::trace;
+use tracing::{debug, info, trace, warn};
 
 /// Implementation for the classic Chat Completions API.
 pub(crate) async fn stream_chat_completions(
@@ -328,6 +327,8 @@ pub(crate) async fn stream_chat_completions(
     }
 
     let tools_json = create_tools_json_for_chat_completions_api(&prompt.tools)?;
+    let tools_count = tools_json.len();
+    let message_count = messages.len();
     let payload = json!({
         "model": model_family.slug,
         "messages": messages,
@@ -361,6 +362,17 @@ pub(crate) async fn stream_chat_completions(
             req_builder = req_builder.header("x-openai-subagent", subagent);
         }
 
+        info!(
+            target: "codex_litellm_debug::model_request",
+            attempt,
+            max_retries,
+            model = %model_family.slug,
+            provider = %provider.name,
+            message_count,
+            tools_count,
+            "chat_completions.request_start"
+        );
+
         let res = otel_event_manager
             .log_request(attempt, || {
                 req_builder
@@ -372,6 +384,15 @@ pub(crate) async fn stream_chat_completions(
 
         match res {
             Ok(resp) if resp.status().is_success() => {
+                let status_code = resp.status();
+                info!(
+                    target: "codex_litellm_debug::model_response",
+                    attempt,
+                    model = %model_family.slug,
+                    provider = %provider.name,
+                    status = status_code.as_u16(),
+                    "chat_completions.response_stream_open"
+                );
                 let (tx_event, rx_event) = mpsc::channel::<Result<ResponseEvent>>(1600);
                 let stream = resp.bytes_stream().map_err(|e| {
                     CodexErr::ResponseStreamFailed(ResponseStreamFailed {
@@ -391,6 +412,16 @@ pub(crate) async fn stream_chat_completions(
                 let status = res.status();
                 if !(status == StatusCode::TOO_MANY_REQUESTS || status.is_server_error()) {
                     let body = (res.text().await).unwrap_or_default();
+                    let body_preview: String = body.chars().take(200).collect();
+                    info!(
+                        target: "codex_litellm_debug::model_response",
+                        attempt,
+                        model = %model_family.slug,
+                        provider = %provider.name,
+                        status = status.as_u16(),
+                        body_preview = body_preview,
+                        "chat_completions.response_unexpected_status"
+                    );
                     return Err(CodexErr::UnexpectedStatus(UnexpectedResponseError {
                         status,
                         body,
@@ -399,6 +430,14 @@ pub(crate) async fn stream_chat_completions(
                 }
 
                 if attempt > max_retries {
+                    info!(
+                        target: "codex_litellm_debug::model_response",
+                        attempt,
+                        model = %model_family.slug,
+                        provider = %provider.name,
+                        status = status.as_u16(),
+                        "chat_completions.retry_limit"
+                    );
                     return Err(CodexErr::RetryLimit(RetryLimitReachedError {
                         status,
                         request_id: None,
@@ -414,15 +453,42 @@ pub(crate) async fn stream_chat_completions(
                 let delay = retry_after_secs
                     .map(|s| Duration::from_millis(s * 1_000))
                     .unwrap_or_else(|| backoff(attempt));
+                info!(
+                    target: "codex_litellm_debug::model_response",
+                    attempt,
+                    model = %model_family.slug,
+                    provider = %provider.name,
+                    status = status.as_u16(),
+                    retry_after_secs,
+                    delay_ms = delay.as_millis(),
+                    "chat_completions.retry_scheduled"
+                );
                 tokio::time::sleep(delay).await;
             }
             Err(e) => {
                 if attempt > max_retries {
+                    warn!(
+                        target: "codex_litellm_debug::model_response",
+                        attempt,
+                        model = %model_family.slug,
+                        provider = %provider.name,
+                        error = %e,
+                        "chat_completions.request_failed"
+                    );
                     return Err(CodexErr::ConnectionFailed(ConnectionFailedError {
                         source: e,
                     }));
                 }
                 let delay = backoff(attempt);
+                warn!(
+                    target: "codex_litellm_debug::model_response",
+                    attempt,
+                    model = %model_family.slug,
+                    provider = %provider.name,
+                    error = %e,
+                    delay_ms = delay.as_millis(),
+                    "chat_completions.request_retry"
+                );
                 tokio::time::sleep(delay).await;
             }
         }
@@ -514,6 +580,61 @@ async fn process_chat_sse<S>(
     let mut assistant_item: Option<ResponseItem> = None;
     let mut reasoning_item: Option<ResponseItem> = None;
 
+    fn assistant_text_from_item(item: &ResponseItem) -> Option<String> {
+        if let ResponseItem::Message { role, content, .. } = item
+            && role == "assistant"
+        {
+            let mut text = String::new();
+            for piece in content {
+                if let ContentItem::OutputText { text: segment } = piece {
+                    text.push_str(segment);
+                }
+            }
+            if text.trim().is_empty() {
+                None
+            } else {
+                Some(text)
+            }
+        } else {
+            None
+        }
+    }
+
+    fn reasoning_text_from_item(item: &ResponseItem) -> Option<String> {
+        if let ResponseItem::Reasoning {
+            content: Some(items),
+            ..
+        } = item
+        {
+            let mut text = String::new();
+            for entry in items {
+                match entry {
+                    ReasoningItemContent::ReasoningText { text: segment }
+                    | ReasoningItemContent::Text { text: segment } => text.push_str(segment),
+                }
+            }
+            if text.trim().is_empty() {
+                None
+            } else {
+                Some(text)
+            }
+        } else {
+            None
+        }
+    }
+
+    fn truncate_preview(text: &str) -> String {
+        const MAX_LEN: usize = 200;
+        let flattened = text.replace('\n', " ");
+        if flattened.len() <= MAX_LEN {
+            flattened
+        } else {
+            let mut truncated = flattened[..MAX_LEN].to_string();
+            truncated.push_str("");
+            truncated
+        }
+    }
+
     loop {
         let start = std::time::Instant::now();
         let response = timeout(idle_timeout, stream.next()).await;
@@ -672,6 +793,20 @@ async fn process_chat_sse<S>(
             if let Some(finish_reason) = choice.get("finish_reason").and_then(|v| v.as_str()) {
                 match finish_reason {
                     "tool_calls" if fn_call_state.active => {
+                        let reasoning_text =
+                            reasoning_item.as_ref().and_then(reasoning_text_from_item);
+                        let reasoning_preview =
+                            reasoning_text.as_ref().map(|s| truncate_preview(s));
+                        info!(
+                            target: "codex_litellm_debug::model_response",
+                            finish_reason = finish_reason,
+                            tool_name = fn_call_state.name.as_deref(),
+                            call_id = fn_call_state.call_id.as_deref(),
+                            tool_arguments_len = fn_call_state.arguments.len(),
+                            reasoning_chars = reasoning_text.as_ref().map(|s| s.len()).unwrap_or(0),
+                            reasoning_preview = reasoning_preview.as_deref().unwrap_or(""),
+                            "chat_completions.finish_tool_call"
+                        );
                         // First, flush the terminal raw reasoning so UIs can finalize
                         // the reasoning stream before any exec/tool events begin.
                         if let Some(item) = reasoning_item.take() {
@@ -689,6 +824,25 @@ async fn process_chat_sse<S>(
                         let _ = tx_event.send(Ok(ResponseEvent::OutputItemDone(item))).await;
                     }
                     "stop" => {
+                        let assistant_text =
+                            assistant_item.as_ref().and_then(assistant_text_from_item);
+                        let assistant_chars = assistant_text.as_ref().map(|s| s.len()).unwrap_or(0);
+                        let assistant_preview =
+                            assistant_text.as_ref().map(|s| truncate_preview(s));
+                        let reasoning_text =
+                            reasoning_item.as_ref().and_then(reasoning_text_from_item);
+                        let reasoning_chars = reasoning_text.as_ref().map(|s| s.len()).unwrap_or(0);
+                        let reasoning_preview =
+                            reasoning_text.as_ref().map(|s| truncate_preview(s));
+                        info!(
+                            target: "codex_litellm_debug::model_response",
+                            finish_reason = finish_reason,
+                            assistant_chars,
+                            assistant_preview = assistant_preview.as_deref().unwrap_or(""),
+                            reasoning_chars,
+                            reasoning_preview = reasoning_preview.as_deref().unwrap_or(""),
+                            "chat_completions.finish_stop"
+                        );
                         // Regular turn without tool-call. Emit the final assistant message
                         // as a single OutputItemDone so non-delta consumers see the result.
                         if let Some(item) = assistant_item.take() {
@@ -776,6 +930,10 @@ where
                         &item,
                         codex_protocol::models::ResponseItem::Message { role, .. } if role == "assistant"
                     );
+                    let is_reasoning = matches!(
+                        &item,
+                        codex_protocol::models::ResponseItem::Reasoning { .. }
+                    );
 
                     if is_assistant_message {
                         match this.mode {
@@ -814,6 +972,9 @@ where
                             }
                         }
                     }
+                    if is_reasoning && matches!(this.mode, AggregateMode::AggregatedOnly) {
+                        this.cumulative_reasoning.clear();
+                    }
 
                     // Not an assistant message  forward immediately.
                     return Poll::Ready(Some(Ok(ResponseEvent::OutputItemDone(item))));
diff --git a/codex-rs/core/src/config/mod.rs b/codex-rs/core/src/config/mod.rs
index 1ee48d30..05147541 100644
--- a/codex-rs/core/src/config/mod.rs
+++ b/codex-rs/core/src/config/mod.rs
@@ -11,6 +11,8 @@ use crate::config::types::ReasoningSummaryFormat;
 use crate::config::types::SandboxWorkspaceWrite;
 use crate::config::types::ShellEnvironmentPolicy;
 use crate::config::types::ShellEnvironmentPolicyToml;
+use crate::config::types::TelemetryLogToml;
+use crate::config::types::TelemetryToml;
 use crate::config::types::Tui;
 use crate::config::types::UriBasedFileOpener;
 use crate::config_loader::LoadedConfigLayers;
@@ -32,6 +34,7 @@ use crate::project_doc::DEFAULT_PROJECT_DOC_FILENAME;
 use crate::project_doc::LOCAL_PROJECT_DOC_FILENAME;
 use crate::protocol::AskForApproval;
 use crate::protocol::SandboxPolicy;
+use chrono::Utc;
 use codex_app_server_protocol::Tools;
 use codex_app_server_protocol::UserSavedConfig;
 use codex_protocol::config_types::ForcedLoginMethod;
@@ -53,6 +56,9 @@ use std::path::PathBuf;
 use crate::config::profile::ConfigProfile;
 use toml::Value as TomlValue;
 use toml_edit::DocumentMut;
+use toml_edit::Item as TomlItem;
+use toml_edit::Table as TomlTable;
+use toml_edit::{Array as TomlArray, value};
 
 pub mod edit;
 pub mod profile;
@@ -71,6 +77,400 @@ pub const GPT_5_CODEX_MEDIUM_MODEL: &str = "gpt-5-codex";
 pub(crate) const PROJECT_DOC_MAX_BYTES: usize = 32 * 1024; // 32 KiB
 
 pub(crate) const CONFIG_TOML_FILE: &str = "config.toml";
+pub(crate) const LITELLM_PROVIDER_ID: &str = "litellm";
+const LEGACY_LITELLM_PROVIDER_ID: &str = "litellm-direct";
+pub(crate) const LITELLM_DEFAULT_MODEL: &str = "";
+const LITELLM_PROVIDER_NAME: &str = "LiteLLM Direct";
+const LITELLM_DEFAULT_MAX_TOKENS: i64 = 128_000;
+const LITELLM_DEFAULT_SANDBOX_MODE: &str = "workspace-write";
+const LITELLM_DEFAULT_APPROVAL_POLICY: &str = "never";
+const LITELLM_DEFAULT_REQUEST_MAX_RETRIES: i64 = 4;
+const LITELLM_DEFAULT_STREAM_MAX_RETRIES: i64 = 5;
+const LITELLM_DEFAULT_STREAM_IDLE_TIMEOUT_MS: i64 = 300_000;
+const LITELLM_DEFAULT_CONTEXT_WINDOW: i64 = 130_000;
+const LITELLM_DEFAULT_AUTO_COMPACT_TOKEN_LIMIT: i64 = (LITELLM_DEFAULT_CONTEXT_WINDOW * 9) / 10;
+const DEFAULT_TELEMETRY_LOG_DIR: &str = "logs";
+const DEFAULT_TUI_LOG: &str = "codex-tui.log";
+const DEFAULT_SESSION_LOG: &str = "codex-litellm-session.jsonl";
+const DEFAULT_TELEMETRY_MAX_TOTAL_BYTES: u64 = 100 * 1024 * 1024;
+
+#[derive(Debug, Clone, Default)]
+pub struct LiteLlmProviderUpdate {
+    pub base_url: Option<String>,
+    pub api_key: Option<String>,
+}
+
+impl LiteLlmProviderUpdate {
+    fn is_empty(&self) -> bool {
+        self.base_url.is_none() && self.api_key.is_none()
+    }
+}
+
+#[derive(Debug, Clone)]
+pub struct LiteLlmProviderState {
+    pub base_url: Option<String>,
+    pub api_key: Option<String>,
+}
+
+#[derive(Debug, Clone, PartialEq)]
+pub struct TelemetryLogConfig {
+    pub enabled: bool,
+    pub path: PathBuf,
+}
+
+#[derive(Debug, Clone, PartialEq)]
+pub struct TelemetryConfig {
+    pub enabled: bool,
+    pub dir: PathBuf,
+    pub logs: HashMap<String, TelemetryLogConfig>,
+    pub max_total_bytes: Option<u64>,
+}
+
+impl TelemetryConfig {
+    pub fn log_config(&self, name: &str) -> Option<&TelemetryLogConfig> {
+        self.logs.get(name)
+    }
+}
+
+impl Default for TelemetryConfig {
+    fn default() -> Self {
+        Self {
+            enabled: true,
+            dir: PathBuf::new(),
+            logs: HashMap::new(),
+            max_total_bytes: None,
+        }
+    }
+}
+
+pub fn ensure_litellm_baseline(codex_home: &Path) -> std::io::Result<()> {
+    std::fs::create_dir_all(codex_home)?;
+    let config_path = codex_home.join(CONFIG_TOML_FILE);
+    let mut doc = load_config_document(&config_path)?;
+    let mut changed = migrate_legacy_litellm_provider(&mut doc);
+
+    if !doc.as_table().contains_key("model_provider") {
+        doc["model_provider"] = value(LITELLM_PROVIDER_ID);
+        changed = true;
+    }
+
+    if !doc.as_table().contains_key("model") {
+        doc["model"] = value(LITELLM_DEFAULT_MODEL);
+        changed = true;
+    }
+
+    if !doc.as_table().contains_key("max_tokens") {
+        doc["max_tokens"] = value(LITELLM_DEFAULT_MAX_TOKENS);
+        changed = true;
+    }
+
+    if !doc.as_table().contains_key("model_context_window") {
+        doc["model_context_window"] = value(LITELLM_DEFAULT_CONTEXT_WINDOW);
+        changed = true;
+    }
+
+    if !doc.as_table().contains_key("context_length") {
+        doc["context_length"] = value(LITELLM_DEFAULT_CONTEXT_WINDOW);
+        changed = true;
+    }
+
+    if !doc
+        .as_table()
+        .contains_key("model_auto_compact_token_limit")
+    {
+        doc["model_auto_compact_token_limit"] = value(LITELLM_DEFAULT_AUTO_COMPACT_TOKEN_LIMIT);
+        changed = true;
+    }
+
+    if !doc.as_table().contains_key("sandbox_mode") {
+        doc["sandbox_mode"] = value(LITELLM_DEFAULT_SANDBOX_MODE);
+        changed = true;
+    }
+
+    if !doc.as_table().contains_key("approval_policy") {
+        doc["approval_policy"] = value(LITELLM_DEFAULT_APPROVAL_POLICY);
+        changed = true;
+    }
+
+    let provider_table = ensure_litellm_provider_table(&mut doc);
+    if !provider_table.contains_key("name") {
+        provider_table["name"] = value(LITELLM_PROVIDER_NAME);
+        changed = true;
+    }
+    if !provider_table.contains_key("wire_api") {
+        provider_table["wire_api"] = value("chat");
+        changed = true;
+    }
+    if !provider_table.contains_key("requires_openai_auth") {
+        provider_table["requires_openai_auth"] = value(false);
+        changed = true;
+    }
+    if !provider_table.contains_key("request_max_retries") {
+        provider_table["request_max_retries"] = value(LITELLM_DEFAULT_REQUEST_MAX_RETRIES);
+        changed = true;
+    }
+    if !provider_table.contains_key("stream_max_retries") {
+        provider_table["stream_max_retries"] = value(LITELLM_DEFAULT_STREAM_MAX_RETRIES);
+        changed = true;
+    }
+    if !provider_table.contains_key("stream_idle_timeout_ms") {
+        provider_table["stream_idle_timeout_ms"] = value(LITELLM_DEFAULT_STREAM_IDLE_TIMEOUT_MS);
+        changed = true;
+    }
+
+    let chat_params = provider_table
+        .entry("chat_completion_parameters")
+        .or_insert(TomlItem::Table(TomlTable::new()));
+    if let Some(params_table) = chat_params.as_table_mut() {
+        params_table.set_implicit(false);
+        if !params_table.contains_key("modalities") {
+            let mut arr = TomlArray::new();
+            arr.push("text");
+            params_table["modalities"] = TomlItem::Value(arr.into());
+            changed = true;
+        }
+        let response_format = params_table
+            .entry("response_format")
+            .or_insert(TomlItem::Table(TomlTable::new()));
+        if let Some(response_table) = response_format.as_table_mut() {
+            response_table.set_implicit(false);
+            if !response_table.contains_key("type") {
+                response_table["type"] = value("text");
+                changed = true;
+            }
+        }
+    }
+
+    let telemetry_entry = doc
+        .as_table_mut()
+        .entry("telemetry")
+        .or_insert(TomlItem::Table(TomlTable::new()));
+    if let Some(telemetry_table) = telemetry_entry.as_table_mut() {
+        telemetry_table.set_implicit(false);
+        if !telemetry_table.contains_key("dir") {
+            telemetry_table["dir"] = value(DEFAULT_TELEMETRY_LOG_DIR);
+            changed = true;
+        }
+        let logs_item = telemetry_table
+            .entry("logs")
+            .or_insert(TomlItem::Table(TomlTable::new()));
+        if let Some(logs_table) = logs_item.as_table_mut() {
+            logs_table.set_implicit(false);
+            let mut ensure_entry = |name: &str, default_file: Option<&str>| -> bool {
+                let mut modified = false;
+                if !logs_table.contains_key(name) {
+                    let mut entry_table = TomlTable::new();
+                    entry_table.set_implicit(false);
+                    if let Some(file) = default_file {
+                        entry_table["file"] = value(file);
+                    }
+                    logs_table[name] = TomlItem::Table(entry_table);
+                    return true;
+                }
+                if let Some(entry_table) = logs_table.get_mut(name).and_then(TomlItem::as_table_mut)
+                {
+                    entry_table.set_implicit(false);
+                    if let Some(file) = default_file {
+                        if !entry_table.contains_key("file") {
+                            entry_table["file"] = value(file);
+                            modified = true;
+                        }
+                    }
+                }
+                modified
+            };
+            if ensure_entry("tui", Some(DEFAULT_TUI_LOG)) {
+                changed = true;
+            }
+            if ensure_entry("session", Some(DEFAULT_SESSION_LOG)) {
+                changed = true;
+            }
+            if ensure_entry("debug", None) {
+                changed = true;
+            }
+        }
+    }
+
+    if changed {
+        save_config_document(&config_path, &doc)?;
+    }
+
+    Ok(())
+}
+
+pub fn read_litellm_provider_state(codex_home: &Path) -> std::io::Result<LiteLlmProviderState> {
+    ensure_litellm_baseline(codex_home)?;
+    let config_path = codex_home.join(CONFIG_TOML_FILE);
+    let doc = load_config_document(&config_path)?;
+    let provider_table = ensure_litellm_provider_table_ref(&doc);
+    let base_url = provider_table
+        .and_then(|table| table.get("base_url"))
+        .and_then(TomlItem::as_str)
+        .map(|value| value.trim().to_string())
+        .filter(|value| !value.is_empty());
+    let api_key = provider_table
+        .and_then(|table| table.get("experimental_bearer_token"))
+        .and_then(TomlItem::as_str)
+        .map(|value| value.trim().to_string())
+        .filter(|value| !value.is_empty());
+
+    Ok(LiteLlmProviderState { base_url, api_key })
+}
+
+pub fn write_litellm_provider_state(
+    codex_home: &Path,
+    update: LiteLlmProviderUpdate,
+) -> std::io::Result<()> {
+    if update.is_empty() {
+        return Ok(());
+    }
+
+    ensure_litellm_baseline(codex_home)?;
+    let config_path = codex_home.join(CONFIG_TOML_FILE);
+    let mut doc = load_config_document(&config_path)?;
+    let provider_table = ensure_litellm_provider_table(&mut doc);
+    let mut changed = false;
+
+    if let Some(base_url) = update.base_url {
+        if base_url.is_empty() {
+            provider_table.remove("base_url");
+        } else {
+            provider_table["base_url"] = value(base_url);
+        }
+        changed = true;
+    }
+
+    if let Some(api_key) = update.api_key {
+        if api_key.is_empty() {
+            provider_table.remove("experimental_bearer_token");
+        } else {
+            provider_table["experimental_bearer_token"] = value(api_key);
+        }
+        changed = true;
+    }
+
+    if changed {
+        save_config_document(&config_path, &doc)?;
+    }
+
+    Ok(())
+}
+
+pub fn set_default_model(codex_home: &Path, model: &str) -> std::io::Result<()> {
+    ensure_litellm_baseline(codex_home)?;
+    let config_path = codex_home.join(CONFIG_TOML_FILE);
+    let mut doc = load_config_document(&config_path)?;
+    doc["model"] = value(model);
+    save_config_document(&config_path, &doc)?;
+    Ok(())
+}
+
+pub fn clear_litellm_credentials(codex_home: &Path) -> std::io::Result<bool> {
+    let config_path = codex_home.join(CONFIG_TOML_FILE);
+    let mut doc = load_config_document(&config_path)?;
+    let provider_exists = match ensure_litellm_provider_table_mut(&mut doc) {
+        Some(table) => table,
+        None => return Ok(false),
+    };
+
+    let mut changed = false;
+    if provider_exists.remove("base_url").is_some() {
+        changed = true;
+    }
+    if provider_exists
+        .remove("experimental_bearer_token")
+        .is_some()
+    {
+        changed = true;
+    }
+
+    if changed {
+        save_config_document(&config_path, &doc)?;
+    }
+
+    Ok(changed)
+}
+
+fn migrate_legacy_litellm_provider(doc: &mut DocumentMut) -> bool {
+    let mut changed = false;
+
+    if doc
+        .as_table()
+        .get("model_provider")
+        .and_then(TomlItem::as_str)
+        == Some(LEGACY_LITELLM_PROVIDER_ID)
+    {
+        doc["model_provider"] = value(LITELLM_PROVIDER_ID);
+        changed = true;
+    }
+
+    if let Some(providers) = doc
+        .as_table_mut()
+        .get_mut("model_providers")
+        .and_then(TomlItem::as_table_mut)
+    {
+        if providers.contains_key(LEGACY_LITELLM_PROVIDER_ID) {
+            if !providers.contains_key(LITELLM_PROVIDER_ID) {
+                if let Some(existing) = providers.get(LEGACY_LITELLM_PROVIDER_ID).cloned() {
+                    providers.insert(LITELLM_PROVIDER_ID, existing);
+                }
+            }
+            providers.remove(LEGACY_LITELLM_PROVIDER_ID);
+            changed = true;
+        }
+    }
+
+    changed
+}
+
+fn ensure_litellm_provider_table(doc: &mut DocumentMut) -> &mut TomlTable {
+    let providers_item = doc
+        .as_table_mut()
+        .entry("model_providers")
+        .or_insert(TomlItem::Table(TomlTable::new()));
+    let providers_table = providers_item.as_table_mut().expect("table");
+    providers_table.set_implicit(false);
+    let provider_item = providers_table
+        .entry(LITELLM_PROVIDER_ID)
+        .or_insert(TomlItem::Table(TomlTable::new()));
+    let provider_table = provider_item.as_table_mut().expect("table");
+    provider_table.set_implicit(false);
+    provider_table
+}
+
+fn ensure_litellm_provider_table_ref(doc: &DocumentMut) -> Option<&TomlTable> {
+    doc.as_table()
+        .get("model_providers")
+        .and_then(TomlItem::as_table)
+        .and_then(|providers| providers.get(LITELLM_PROVIDER_ID))
+        .and_then(TomlItem::as_table)
+}
+
+fn ensure_litellm_provider_table_mut(doc: &mut DocumentMut) -> Option<&mut TomlTable> {
+    doc.as_table_mut()
+        .get_mut("model_providers")
+        .and_then(TomlItem::as_table_mut)
+        .and_then(|providers| providers.get_mut(LITELLM_PROVIDER_ID))
+        .and_then(TomlItem::as_table_mut)
+}
+
+fn load_config_document(path: &Path) -> std::io::Result<DocumentMut> {
+    match std::fs::read_to_string(path) {
+        Ok(contents) => match contents.parse::<DocumentMut>() {
+            Ok(doc) => Ok(doc),
+            Err(err) => {
+                tracing::warn!("Failed to parse {}: {err}", path.display());
+                Ok(DocumentMut::new())
+            }
+        },
+        Err(err) if err.kind() == ErrorKind::NotFound => Ok(DocumentMut::new()),
+        Err(err) => Err(err),
+    }
+}
+
+fn save_config_document(path: &Path, doc: &DocumentMut) -> std::io::Result<()> {
+    std::fs::write(path, doc.to_string())
+}
 
 /// Application configuration loaded from disk and merged with overrides.
 #[derive(Debug, Clone, PartialEq)]
@@ -98,6 +498,9 @@ pub struct Config {
     /// Info needed to make an API request to the model.
     pub model_provider: ModelProviderInfo,
 
+    /// Returns true when the bundled LiteLLM provider still needs a base URL.
+    pub litellm_setup_required: bool,
+
     /// Approval policy for executing commands.
     pub approval_policy: AskForApproval,
 
@@ -203,6 +606,9 @@ pub struct Config {
     /// output will be hyperlinked using the specified URI scheme.
     pub file_opener: UriBasedFileOpener,
 
+    /// Telemetry output configuration (log directories and file layout).
+    pub telemetry: TelemetryConfig,
+
     /// Path to the `codex-linux-sandbox` executable. This must be set if
     /// [`crate::exec::SandboxType::LinuxSeccomp`] is used. Note that this
     /// cannot be set in the config file: it must be set in code via
@@ -519,6 +925,9 @@ pub struct ConfigToml {
     /// Size of the context window for the model, in tokens.
     pub model_context_window: Option<i64>,
 
+    /// Alias for model_context_window to make LiteLLM configuration explicit.
+    pub context_length: Option<i64>,
+
     /// Maximum number of output tokens.
     pub model_max_output_tokens: Option<i64>,
 
@@ -630,6 +1039,9 @@ pub struct ConfigToml {
 
     pub projects: Option<HashMap<String, ProjectConfig>>,
 
+    #[serde(default)]
+    pub telemetry: TelemetryToml,
+
     /// Nested tools section for feature toggles
     pub tools: Option<ToolsToml>,
 
@@ -845,6 +1257,7 @@ pub struct ConfigOverrides {
     pub experimental_sandbox_command_assessment: Option<bool>,
     /// Additional directories that should be treated as writable roots for this session.
     pub additional_writable_roots: Vec<PathBuf>,
+    pub telemetry_enabled: Option<bool>,
 }
 
 impl Config {
@@ -875,6 +1288,7 @@ impl Config {
             tools_web_search_request: override_tools_web_search_request,
             experimental_sandbox_command_assessment: sandbox_command_assessment_override,
             additional_writable_roots,
+            telemetry_enabled,
         } = overrides;
 
         let active_profile_name = config_profile_key
@@ -976,13 +1390,21 @@ impl Config {
         let mut model_providers = built_in_model_providers();
         // Merge user-defined providers into the built-in list.
         for (key, provider) in cfg.model_providers.into_iter() {
-            model_providers.entry(key).or_insert(provider);
+            model_providers.insert(key, provider);
         }
 
-        let model_provider_id = model_provider
+        // Determine baseline provider
+        let mut resolved_provider_id = model_provider
             .or(config_profile.model_provider)
-            .or(cfg.model_provider)
-            .unwrap_or_else(|| "openai".to_string());
+            .or(cfg.model_provider);
+        if resolved_provider_id.is_none() {
+            if cfg.model.is_some() || config_profile.model.is_some() {
+                resolved_provider_id = Some("openai".to_string());
+            } else {
+                resolved_provider_id = Some(LITELLM_PROVIDER_ID.to_string());
+            }
+        }
+        let model_provider_id = resolved_provider_id.unwrap();
         let model_provider = model_providers
             .get(&model_provider_id)
             .ok_or_else(|| {
@@ -993,6 +1415,20 @@ impl Config {
             })?
             .clone();
 
+        let base_url_missing = model_provider
+            .base_url
+            .as_ref()
+            .map(|value| value.trim().is_empty())
+            .unwrap_or(true);
+        let token_missing = model_provider
+            .experimental_bearer_token
+            .as_ref()
+            .map(|value| value.trim().is_empty())
+            .unwrap_or(true);
+        let litellm_setup_required = (model_provider_id == "litellm"
+            || model_provider_id == LITELLM_PROVIDER_ID)
+            && (base_url_missing || token_missing);
+
         let shell_environment_policy = cfg.shell_environment_policy.into();
 
         let history = cfg.history.unwrap_or_default();
@@ -1020,7 +1456,16 @@ impl Config {
         let model = model
             .or(config_profile.model)
             .or(cfg.model)
-            .unwrap_or_else(default_model);
+            .unwrap_or_else(|| {
+                if matches!(
+                    model_provider_id.as_str(),
+                    LITELLM_PROVIDER_ID | LEGACY_LITELLM_PROVIDER_ID
+                ) {
+                    String::new()
+                } else {
+                    default_model()
+                }
+            });
 
         let mut model_family =
             find_family_for_model(&model).unwrap_or_else(|| derive_default_model_family(&model));
@@ -1033,8 +1478,8 @@ impl Config {
         }
 
         let openai_model_info = get_model_info(&model_family);
-        let model_context_window = cfg
-            .model_context_window
+        let configured_context_window = cfg.context_length.or(cfg.model_context_window);
+        let model_context_window = configured_context_window
             .or_else(|| openai_model_info.as_ref().map(|info| info.context_window));
         let model_max_output_tokens = cfg.model_max_output_tokens.or_else(|| {
             openai_model_info
@@ -1082,6 +1527,65 @@ impl Config {
         )?;
         let compact_prompt = compact_prompt.or(file_compact_prompt);
 
+        let telemetry_cfg = cfg.telemetry.clone();
+        let telemetry_enabled = telemetry_enabled.or(telemetry_cfg.enabled).unwrap_or(true);
+        let telemetry_dir = match telemetry_cfg.dir {
+            Some(path) if path.is_absolute() => path,
+            Some(path) => codex_home.join(path),
+            None => codex_home.join(DEFAULT_TELEMETRY_LOG_DIR),
+        };
+        let telemetry_max_total_bytes = telemetry_cfg
+            .max_total_bytes
+            .or(Some(DEFAULT_TELEMETRY_MAX_TOTAL_BYTES));
+        let mut telemetry_logs: HashMap<String, TelemetryLogConfig> = HashMap::new();
+        for (name, default_file, timestamped) in [
+            ("tui", Some(DEFAULT_TUI_LOG), false),
+            ("session", Some(DEFAULT_SESSION_LOG), false),
+            ("debug", None, true),
+        ] {
+            let entry: Option<&TelemetryLogToml> = telemetry_cfg.logs.get(name);
+            let enabled = telemetry_enabled && entry.and_then(|log| log.enabled).unwrap_or(true);
+            let path = if let Some(file) = entry.and_then(|log| log.file.clone()) {
+                if file.is_absolute() {
+                    file
+                } else {
+                    telemetry_dir.join(file)
+                }
+            } else if timestamped {
+                let filename = format!("{}.log", Utc::now().format("%Y%m%d-%H%M%S"));
+                telemetry_dir.join(filename)
+            } else if let Some(default) = default_file {
+                telemetry_dir.join(default)
+            } else {
+                telemetry_dir.clone()
+            };
+            telemetry_logs.insert(name.to_string(), TelemetryLogConfig { enabled, path });
+        }
+        for (name, details) in telemetry_cfg.logs {
+            if telemetry_logs.contains_key(&name) {
+                continue;
+            }
+            let enabled = telemetry_enabled && details.enabled.unwrap_or(true);
+            if let Some(file) = details.file {
+                let path = if file.is_absolute() {
+                    file
+                } else {
+                    telemetry_dir.join(file)
+                };
+                telemetry_logs.insert(name, TelemetryLogConfig { enabled, path });
+            }
+        }
+        let telemetry = TelemetryConfig {
+            enabled: telemetry_enabled,
+            dir: telemetry_dir.clone(),
+            logs: telemetry_logs,
+            max_total_bytes: telemetry_max_total_bytes,
+        };
+
+        if telemetry.enabled {
+            prune_telemetry_logs(&telemetry);
+        }
+
         // Default review model when not set in config; allow CLI override to take precedence.
         let review_model = override_review_model
             .or(cfg.review_model)
@@ -1096,6 +1600,7 @@ impl Config {
             model_auto_compact_token_limit,
             model_provider_id,
             model_provider,
+            litellm_setup_required,
             cwd: resolved_cwd,
             approval_policy,
             sandbox_policy,
@@ -1132,6 +1637,7 @@ impl Config {
             codex_home,
             history,
             file_opener: cfg.file_opener.unwrap_or(UriBasedFileOpener::VsCode),
+            telemetry,
             codex_linux_sandbox_exe,
 
             hide_agent_reasoning: cfg.hide_agent_reasoning.unwrap_or(false),
@@ -1274,9 +1780,83 @@ pub fn find_codex_home() -> std::io::Result<PathBuf> {
 /// Returns the path to the folder where Codex logs are stored. Does not verify
 /// that the directory exists.
 pub fn log_dir(cfg: &Config) -> std::io::Result<PathBuf> {
-    let mut p = cfg.codex_home.clone();
-    p.push("log");
-    Ok(p)
+    Ok(cfg.telemetry.dir.clone())
+}
+
+pub fn telemetry_log_config<'a>(cfg: &'a Config, name: &str) -> Option<&'a TelemetryLogConfig> {
+    cfg.telemetry.log_config(name)
+}
+
+fn prune_telemetry_logs(telemetry: &TelemetryConfig) {
+    if !telemetry.enabled {
+        return;
+    }
+    let Some(limit) = telemetry.max_total_bytes else {
+        return;
+    };
+    if limit == 0 {
+        return;
+    }
+
+    let dir_entries = match std::fs::read_dir(&telemetry.dir) {
+        Ok(entries) => entries,
+        Err(err) => {
+            tracing::debug!(
+                "telemetry: unable to read log directory {}: {err}",
+                telemetry.dir.display()
+            );
+            return;
+        }
+    };
+
+    let mut files: Vec<(PathBuf, u64, std::time::SystemTime)> = Vec::new();
+    for entry in dir_entries.flatten() {
+        let path = entry.path();
+        if let Ok(meta) = entry.metadata()
+            && meta.is_file()
+        {
+            let modified = meta.modified().unwrap_or(std::time::SystemTime::UNIX_EPOCH);
+            files.push((path, meta.len(), modified));
+        }
+    }
+
+    if files.is_empty() {
+        return;
+    }
+
+    let mut total_size: u64 = files.iter().map(|(_, size, _)| *size).sum();
+    if total_size <= limit {
+        return;
+    }
+
+    files.sort_by_key(|(_, _, modified)| *modified);
+
+    let active_paths: std::collections::HashSet<PathBuf> = telemetry
+        .logs
+        .values()
+        .map(|log| log.path.clone())
+        .collect();
+
+    for (path, size, _) in files {
+        if total_size <= limit {
+            break;
+        }
+        if active_paths.contains(&path) {
+            continue;
+        }
+        match std::fs::remove_file(&path) {
+            Ok(()) => {
+                total_size = total_size.saturating_sub(size);
+                tracing::debug!("telemetry: pruned {}", path.display());
+            }
+            Err(err) => {
+                tracing::warn!(
+                    "telemetry: failed to remove old log {}: {err}",
+                    path.display()
+                );
+            }
+        }
+    }
 }
 
 #[cfg(test)]
@@ -1292,6 +1872,7 @@ mod tests {
     use super::*;
     use pretty_assertions::assert_eq;
 
+    use std::collections::HashMap;
     use std::time::Duration;
     use tempfile::TempDir;
 
@@ -2870,6 +3451,7 @@ model_verbosity = "high"
                 model_auto_compact_token_limit: Some(180_000),
                 model_provider_id: "openai".to_string(),
                 model_provider: fixture.openai_provider.clone(),
+                litellm_setup_required: false,
                 approval_policy: AskForApproval::Never,
                 sandbox_policy: SandboxPolicy::new_read_only_policy(),
                 did_user_set_custom_approval_policy_or_sandbox_mode: true,
@@ -2898,6 +3480,7 @@ model_verbosity = "high"
                 developer_instructions: None,
                 compact_prompt: None,
                 forced_chatgpt_workspace_id: None,
+                telemetry: o3_profile_config.telemetry.clone(),
                 forced_login_method: None,
                 include_apply_patch_tool: false,
                 tools_web_search_request: false,
@@ -2942,6 +3525,7 @@ model_verbosity = "high"
             model_auto_compact_token_limit: Some(14_746),
             model_provider_id: "openai-chat-completions".to_string(),
             model_provider: fixture.openai_chat_completions_provider.clone(),
+            litellm_setup_required: false,
             approval_policy: AskForApproval::UnlessTrusted,
             sandbox_policy: SandboxPolicy::new_read_only_policy(),
             did_user_set_custom_approval_policy_or_sandbox_mode: true,
@@ -2970,6 +3554,7 @@ model_verbosity = "high"
             developer_instructions: None,
             compact_prompt: None,
             forced_chatgpt_workspace_id: None,
+            telemetry: gpt3_profile_config.telemetry.clone(),
             forced_login_method: None,
             include_apply_patch_tool: false,
             tools_web_search_request: false,
@@ -3029,6 +3614,7 @@ model_verbosity = "high"
             model_auto_compact_token_limit: Some(180_000),
             model_provider_id: "openai".to_string(),
             model_provider: fixture.openai_provider.clone(),
+            litellm_setup_required: false,
             approval_policy: AskForApproval::OnFailure,
             sandbox_policy: SandboxPolicy::new_read_only_policy(),
             did_user_set_custom_approval_policy_or_sandbox_mode: true,
@@ -3057,6 +3643,7 @@ model_verbosity = "high"
             developer_instructions: None,
             compact_prompt: None,
             forced_chatgpt_workspace_id: None,
+            telemetry: zdr_profile_config.telemetry.clone(),
             forced_login_method: None,
             include_apply_patch_tool: false,
             tools_web_search_request: false,
@@ -3102,6 +3689,7 @@ model_verbosity = "high"
             model_auto_compact_token_limit: Some(244_800),
             model_provider_id: "openai".to_string(),
             model_provider: fixture.openai_provider.clone(),
+            litellm_setup_required: false,
             approval_policy: AskForApproval::OnFailure,
             sandbox_policy: SandboxPolicy::new_read_only_policy(),
             did_user_set_custom_approval_policy_or_sandbox_mode: true,
@@ -3130,6 +3718,7 @@ model_verbosity = "high"
             developer_instructions: None,
             compact_prompt: None,
             forced_chatgpt_workspace_id: None,
+            telemetry: gpt5_profile_config.telemetry.clone(),
             forced_login_method: None,
             include_apply_patch_tool: false,
             tools_web_search_request: false,
@@ -3272,6 +3861,11 @@ mod notifications_tests {
     use crate::config::types::Notifications;
     use assert_matches::assert_matches;
     use serde::Deserialize;
+    use std::collections::HashMap;
+    use std::time::Duration;
+    use tempfile::TempDir;
+
+    use super::{TelemetryConfig, TelemetryLogConfig, prune_telemetry_logs};
 
     #[derive(Deserialize, Debug, PartialEq)]
     struct TuiTomlTest {
@@ -3306,4 +3900,57 @@ mod notifications_tests {
             Notifications::Custom(ref v) if v == &vec!["foo".to_string()]
         );
     }
+
+    #[test]
+    fn prune_telemetry_respects_size_limit() {
+        let tmp = TempDir::new().expect("tempdir");
+        let dir = tmp.path();
+
+        let old_a = dir.join("20240101.log");
+        let old_b = dir.join("20240201.log");
+        let active = dir.join("codex-tui.log");
+
+        std::fs::write(&old_a, vec![0u8; 30]).expect("write old_a");
+        std::thread::sleep(Duration::from_millis(10));
+        std::fs::write(&old_b, vec![0u8; 25]).expect("write old_b");
+        std::thread::sleep(Duration::from_millis(10));
+        std::fs::write(&active, vec![0u8; 40]).expect("write active");
+
+        let telemetry = TelemetryConfig {
+            enabled: true,
+            dir: dir.to_path_buf(),
+            logs: HashMap::from([(
+                "tui".to_string(),
+                TelemetryLogConfig {
+                    enabled: true,
+                    path: active.clone(),
+                },
+            )]),
+            max_total_bytes: Some(70),
+        };
+
+        prune_telemetry_logs(&telemetry);
+
+        assert!(
+            active.exists(),
+            "active log should never be removed by pruning"
+        );
+        let remaining_old = [old_a.clone(), old_b.clone()]
+            .into_iter()
+            .filter(|p| p.exists())
+            .count();
+        assert_eq!(
+            remaining_old, 1,
+            "only one of the old logs should remain after pruning"
+        );
+        let total: u64 = [old_a, old_b, active]
+            .iter()
+            .filter_map(|p| std::fs::metadata(p).ok())
+            .map(|m| m.len())
+            .sum();
+        assert!(
+            total <= 70,
+            "total telemetry size should respect the configured limit"
+        );
+    }
 }
diff --git a/codex-rs/core/src/config/types.rs b/codex-rs/core/src/config/types.rs
index fd93df8f..6eff1a78 100644
--- a/codex-rs/core/src/config/types.rs
+++ b/codex-rs/core/src/config/types.rs
@@ -161,6 +161,22 @@ const fn default_enabled() -> bool {
     true
 }
 
+#[derive(Serialize, Deserialize, Debug, Clone, PartialEq, Default)]
+#[serde(default, deny_unknown_fields)]
+pub struct TelemetryLogToml {
+    pub enabled: Option<bool>,
+    pub file: Option<PathBuf>,
+}
+
+#[derive(Serialize, Deserialize, Debug, Clone, PartialEq, Default)]
+#[serde(default, deny_unknown_fields)]
+pub struct TelemetryToml {
+    pub dir: Option<PathBuf>,
+    pub logs: HashMap<String, TelemetryLogToml>,
+    pub enabled: Option<bool>,
+    pub max_total_bytes: Option<u64>,
+}
+
 #[derive(Serialize, Deserialize, Debug, Clone, PartialEq)]
 #[serde(untagged, deny_unknown_fields, rename_all = "snake_case")]
 pub enum McpServerTransportConfig {
diff --git a/codex-rs/core/src/model_provider_info.rs b/codex-rs/core/src/model_provider_info.rs
index 8dc252aa..8ba901e7 100644
--- a/codex-rs/core/src/model_provider_info.rs
+++ b/codex-rs/core/src/model_provider_info.rs
@@ -261,15 +261,14 @@ impl ModelProviderInfo {
 const DEFAULT_OLLAMA_PORT: u32 = 11434;
 
 pub const BUILT_IN_OSS_MODEL_PROVIDER_ID: &str = "oss";
+pub const BUILT_IN_LITELLM_PROVIDER_ID: &str = "litellm";
 
 /// Built-in default provider list.
 pub fn built_in_model_providers() -> HashMap<String, ModelProviderInfo> {
     use ModelProviderInfo as P;
 
-    // We do not want to be in the business of adjucating which third-party
-    // providers are bundled with Codex CLI, so we only include the OpenAI and
-    // open source ("oss") providers by default. Users are encouraged to add to
-    // `model_providers` in config.toml to add their own providers.
+    // We include OpenAI, the upstream OSS provider, and a LiteLLM entry that
+    // our patches populate at runtime.
     [
         (
             "openai",
@@ -312,6 +311,7 @@ pub fn built_in_model_providers() -> HashMap<String, ModelProviderInfo> {
             },
         ),
         (BUILT_IN_OSS_MODEL_PROVIDER_ID, create_oss_provider()),
+        (BUILT_IN_LITELLM_PROVIDER_ID, create_litellm_provider()),
     ]
     .into_iter()
     .map(|(k, v)| (k.to_string(), v))
@@ -357,6 +357,24 @@ pub fn create_oss_provider_with_base_url(base_url: &str) -> ModelProviderInfo {
     }
 }
 
+pub fn create_litellm_provider() -> ModelProviderInfo {
+    ModelProviderInfo {
+        name: "LiteLLM Direct".into(),
+        base_url: None,
+        env_key: None,
+        env_key_instructions: None,
+        experimental_bearer_token: None,
+        wire_api: WireApi::Chat,
+        query_params: None,
+        http_headers: None,
+        env_http_headers: None,
+        request_max_retries: None,
+        stream_max_retries: None,
+        stream_idle_timeout_ms: None,
+        requires_openai_auth: false,
+    }
+}
+
 fn matches_azure_responses_base_url(base_url: &str) -> bool {
     let base = base_url.to_ascii_lowercase();
     const AZURE_MARKERS: [&str; 5] = [
diff --git a/codex-rs/core/tests/chat_completions_sse.rs b/codex-rs/core/tests/chat_completions_sse.rs
index 46378b08..05e86d9f 100644
--- a/codex-rs/core/tests/chat_completions_sse.rs
+++ b/codex-rs/core/tests/chat_completions_sse.rs
@@ -28,10 +28,18 @@ fn network_disabled() -> bool {
 }
 
 async fn run_stream(sse_body: &str) -> Vec<ResponseEvent> {
-    run_stream_with_bytes(sse_body.as_bytes()).await
+    run_stream_with_mode_bytes(sse_body.as_bytes(), true).await
+}
+
+async fn run_stream_aggregated(sse_body: &str) -> Vec<ResponseEvent> {
+    run_stream_with_mode_bytes(sse_body.as_bytes(), false).await
 }
 
 async fn run_stream_with_bytes(sse_body: &[u8]) -> Vec<ResponseEvent> {
+    run_stream_with_mode_bytes(sse_body, true).await
+}
+
+async fn run_stream_with_mode_bytes(sse_body: &[u8], raw_reasoning: bool) -> Vec<ResponseEvent> {
     let server = MockServer::start().await;
 
     let template = ResponseTemplate::new(200)
@@ -68,7 +76,7 @@ async fn run_stream_with_bytes(sse_body: &[u8]) -> Vec<ResponseEvent> {
     let mut config = load_default_config_for_test(&codex_home);
     config.model_provider_id = provider.name.clone();
     config.model_provider = provider.clone();
-    config.show_raw_agent_reasoning = true;
+    config.show_raw_agent_reasoning = raw_reasoning;
     let effort = config.model_reasoning_effort;
     let summary = config.model_reasoning_summary;
     let config = Arc::new(config);
@@ -191,6 +199,164 @@ async fn streams_text_without_reasoning() {
     assert_matches!(events[3], ResponseEvent::Completed { .. });
 }
 
+#[tokio::test(flavor = "multi_thread", worker_threads = 2)]
+async fn aggregates_text_when_raw_reasoning_disabled() {
+    if network_disabled() {
+        println!(
+            "Skipping test because it cannot execute when network is disabled in a Codex sandbox."
+        );
+        return;
+    }
+
+    let sse = concat!(
+        "data: {\"choices\":[{\"delta\":{\"content\":\"hi\"}}]}\n\n",
+        "data: {\"choices\":[{\"delta\":{}}]}\n\n",
+        "data: [DONE]\n\n",
+    );
+
+    let events = run_stream_aggregated(sse).await;
+    assert_eq!(events.len(), 3, "unexpected events: {events:?}");
+
+    match &events[0] {
+        ResponseEvent::OutputItemAdded(ResponseItem::Message { .. }) => {}
+        other => panic!("expected initial message, got {other:?}"),
+    }
+
+    match &events[1] {
+        ResponseEvent::OutputItemDone(item) => assert_message(item, "hi"),
+        other => panic!("expected aggregated message, got {other:?}"),
+    }
+
+    assert_matches!(events[2], ResponseEvent::Completed { .. });
+}
+
+#[tokio::test(flavor = "multi_thread", worker_threads = 2)]
+async fn aggregates_reasoning_before_message() {
+    if network_disabled() {
+        println!(
+            "Skipping test because it cannot execute when network is disabled in a Codex sandbox."
+        );
+        return;
+    }
+
+    let sse = concat!(
+        "data: {\"choices\":[{\"delta\":{\"reasoning\":\"think1\"}}]}\n\n",
+        "data: {\"choices\":[{\"delta\":{\"content\":\"ok\"}}]}\n\n",
+        "data: {\"choices\":[{\"delta\":{} ,\"finish_reason\":\"stop\"}]}\n\n",
+    );
+
+    let events = run_stream_aggregated(sse).await;
+    assert_eq!(events.len(), 5, "unexpected events: {events:?}");
+
+    match &events[0] {
+        ResponseEvent::OutputItemAdded(ResponseItem::Reasoning { .. }) => {}
+        other => panic!("expected initial reasoning item, got {other:?}"),
+    }
+
+    match &events[1] {
+        ResponseEvent::OutputItemAdded(ResponseItem::Message { .. }) => {}
+        other => panic!("expected initial message item, got {other:?}"),
+    }
+
+    match &events[2] {
+        ResponseEvent::OutputItemDone(item) => assert_reasoning(item, "think1"),
+        other => panic!("expected reasoning item, got {other:?}"),
+    }
+
+    match &events[3] {
+        ResponseEvent::OutputItemDone(item) => assert_message(item, "ok"),
+        other => panic!("expected aggregated message, got {other:?}"),
+    }
+
+    assert_matches!(events[4], ResponseEvent::Completed { .. });
+}
+
+#[tokio::test(flavor = "multi_thread", worker_threads = 2)]
+async fn aggregated_events_preserve_history_order() {
+    if network_disabled() {
+        println!(
+            "Skipping test because it cannot execute when network is disabled in a Codex sandbox."
+        );
+        return;
+    }
+
+    let sse = concat!(
+        "data: {\"choices\":[{\"delta\":{\"reasoning\":\"think1\"}}]}\n\n",
+        "data: {\"choices\":[{\"delta\":{\"content\":\"ok\"}}]}\n\n",
+        "data: {\"choices\":[{\"delta\":{} ,\"finish_reason\":\"stop\"}]}\n\n",
+    );
+
+    let events = run_stream_aggregated(sse).await;
+    let mut items: Vec<ResponseItem> = Vec::new();
+    for event in events {
+        if let ResponseEvent::OutputItemDone(item) = event {
+            items.push(item);
+        }
+    }
+    assert_eq!(
+        items.len(),
+        2,
+        "expected reasoning + message, got {:?}",
+        items
+    );
+
+    match &items[0] {
+        ResponseItem::Reasoning { .. } => assert_reasoning(&items[0], "think1"),
+        other => panic!("expected reasoning first, got {other:?}"),
+    }
+    match &items[1] {
+        ResponseItem::Message { .. } => assert_message(&items[1], "ok"),
+        other => panic!("expected message second, got {other:?}"),
+    }
+}
+
+#[tokio::test(flavor = "multi_thread", worker_threads = 2)]
+async fn aggregated_tool_call_keeps_context_pairing() {
+    if network_disabled() {
+        println!(
+            "Skipping test because it cannot execute when network is disabled in a Codex sandbox."
+        );
+        return;
+    }
+
+    let sse = concat!(
+        "data: {\"choices\":[{\"delta\":{\"reasoning\":\"pre-tool\"}}]}\n\n",
+        "data: {\"choices\":[{\"delta\":{\"tool_calls\":[{\"id\":\"call_1\",\"type\":\"function\",\"function\":{\"name\":\"run\",\"arguments\":\"{}\"}}]},\"finish_reason\":\"tool_calls\"}]}\n\n",
+    );
+
+    let events = run_stream_aggregated(sse).await;
+    let mut items: Vec<ResponseItem> = Vec::new();
+    for event in events {
+        if let ResponseEvent::OutputItemDone(item) = event {
+            items.push(item);
+        }
+    }
+    assert_eq!(
+        items.len(),
+        2,
+        "expected reasoning + function call, got {:?}",
+        items
+    );
+
+    match &items[0] {
+        ResponseItem::Reasoning { .. } => assert_reasoning(&items[0], "pre-tool"),
+        other => panic!("expected reasoning first, got {other:?}"),
+    }
+    match &items[1] {
+        ResponseItem::FunctionCall {
+            name,
+            arguments,
+            call_id,
+            ..
+        } => {
+            assert_eq!(name, "run");
+            assert_eq!(arguments, "{}");
+            assert_eq!(call_id, "call_1");
+        }
+        other => panic!("expected function call second, got {other:?}"),
+    }
+}
+
 #[tokio::test(flavor = "multi_thread", worker_threads = 2)]
 async fn streams_reasoning_from_string_delta() {
     if network_disabled() {
diff --git a/codex-rs/exec/src/cli.rs b/codex-rs/exec/src/cli.rs
index f56d07dc..b4b549b1 100644
--- a/codex-rs/exec/src/cli.rs
+++ b/codex-rs/exec/src/cli.rs
@@ -1,3 +1,4 @@
+use clap::ArgAction;
 use clap::Parser;
 use clap::ValueEnum;
 use codex_common::CliConfigOverrides;
@@ -21,6 +22,14 @@ pub struct Cli {
     #[arg(long = "oss", default_value_t = false)]
     pub oss: bool,
 
+    /// Force-enable telemetry writers for this run (overrides config).
+    #[arg(long = "telemetry", action = ArgAction::SetTrue, conflicts_with = "no_telemetry")]
+    pub telemetry: bool,
+
+    /// Disable all telemetry writers for this run (overrides config).
+    #[arg(long = "no-telemetry", action = ArgAction::SetTrue)]
+    pub no_telemetry: bool,
+
     /// Select the sandbox policy to use when executing model-generated shell
     /// commands.
     #[arg(long = "sandbox", short = 's', value_enum)]
diff --git a/codex-rs/exec/src/lib.rs b/codex-rs/exec/src/lib.rs
index 13d43f61..42021806 100644
--- a/codex-rs/exec/src/lib.rs
+++ b/codex-rs/exec/src/lib.rs
@@ -69,6 +69,7 @@ pub async fn run_main(cli: Cli, codex_linux_sandbox_exe: Option<PathBuf>) -> any
         prompt,
         output_schema: output_schema_path,
         config_overrides,
+        ..
     } = cli;
 
     // Determine the prompt source (parent or subcommand) and read from stdin if needed.
@@ -181,6 +182,13 @@ pub async fn run_main(cli: Cli, codex_linux_sandbox_exe: Option<PathBuf>) -> any
         tools_web_search_request: None,
         experimental_sandbox_command_assessment: None,
         additional_writable_roots: Vec::new(),
+        telemetry_enabled: if cli.telemetry {
+            Some(true)
+        } else if cli.no_telemetry {
+            Some(false)
+        } else {
+            None
+        },
     };
     // Parse `-c` overrides.
     let cli_kv_overrides = match config_overrides.parse_overrides() {
diff --git a/codex-rs/feedback/Cargo.toml b/codex-rs/feedback/Cargo.toml
index b104f512..a0f2f47d 100644
--- a/codex-rs/feedback/Cargo.toml
+++ b/codex-rs/feedback/Cargo.toml
@@ -6,7 +6,14 @@ version.workspace = true
 [dependencies]
 anyhow = { workspace = true }
 codex-protocol = { workspace = true }
-sentry = { version = "0.34" }
+sentry = { version = "0.34", default-features = false, features = [
+    "backtrace",
+    "contexts",
+    "debug-images",
+    "panic",
+    "reqwest",
+    "rustls",
+] }
 tracing-subscriber = { workspace = true }
 
 [dev-dependencies]
diff --git a/codex-rs/keyring-store/Cargo.toml b/codex-rs/keyring-store/Cargo.toml
index f662e5d4..94d3d544 100644
--- a/codex-rs/keyring-store/Cargo.toml
+++ b/codex-rs/keyring-store/Cargo.toml
@@ -7,10 +7,5 @@ version = { workspace = true }
 workspace = true
 
 [dependencies]
-keyring = { workspace = true, features = [
-    "apple-native",
-    "crypto-rust",
-    "linux-native-async-persistent",
-    "windows-native",
-] }
+keyring = { workspace = true }
 tracing = { workspace = true }
diff --git a/codex-rs/mcp-server/src/codex_tool_config.rs b/codex-rs/mcp-server/src/codex_tool_config.rs
index 4e61bde0..ceb89a34 100644
--- a/codex-rs/mcp-server/src/codex_tool_config.rs
+++ b/codex-rs/mcp-server/src/codex_tool_config.rs
@@ -171,6 +171,7 @@ impl CodexToolCallParam {
             tools_web_search_request: None,
             experimental_sandbox_command_assessment: None,
             additional_writable_roots: Vec::new(),
+            telemetry_enabled: None,
         };
 
         let cli_overrides = cli_overrides
diff --git a/codex-rs/process-hardening/Cargo.toml b/codex-rs/process-hardening/Cargo.toml
index 7294b6e2..549d980a 100644
--- a/codex-rs/process-hardening/Cargo.toml
+++ b/codex-rs/process-hardening/Cargo.toml
@@ -19,3 +19,12 @@ libc = { workspace = true }
 
 [target.'cfg(target_os = "macos")'.dependencies]
 libc = { workspace = true }
+
+[target.'cfg(target_os = "illumos")'.dependencies]
+libc = { workspace = true }
+
+[target.'cfg(target_os = "freebsd")'.dependencies]
+libc = { workspace = true }
+
+[target.'cfg(target_os = "openbsd")'.dependencies]
+libc = { workspace = true }
diff --git a/codex-rs/process-hardening/src/lib.rs b/codex-rs/process-hardening/src/lib.rs
index a787b409..5194db40 100644
--- a/codex-rs/process-hardening/src/lib.rs
+++ b/codex-rs/process-hardening/src/lib.rs
@@ -10,6 +10,9 @@ pub fn pre_main_hardening() {
     #[cfg(target_os = "macos")]
     pre_main_hardening_macos();
 
+    #[cfg(any(target_os = "freebsd", target_os = "openbsd"))]
+    pre_main_hardening_bsd();
+
     #[cfg(windows)]
     pre_main_hardening_windows();
 }
@@ -20,7 +23,14 @@ const PRCTL_FAILED_EXIT_CODE: i32 = 5;
 #[cfg(target_os = "macos")]
 const PTRACE_DENY_ATTACH_FAILED_EXIT_CODE: i32 = 6;
 
-#[cfg(any(target_os = "linux", target_os = "android", target_os = "macos"))]
+#[cfg(any(
+    target_os = "linux",
+    target_os = "android",
+    target_os = "macos",
+    target_os = "illumos",
+    target_os = "freebsd",
+    target_os = "openbsd",
+))]
 const SET_RLIMIT_CORE_FAILED_EXIT_CODE: i32 = 7;
 
 #[cfg(any(target_os = "linux", target_os = "android"))]
@@ -108,6 +118,13 @@ fn set_core_file_size_limit_to_zero() {
     }
 }
 
+#[cfg(any(target_os = "freebsd", target_os = "openbsd"))]
+fn pre_main_hardening_bsd() {
+    // FreeBSD/OpenBSD do not expose ptrace toggles like PR_SET_DUMPABLE,
+    // but we can still disable core dumps for consistency across targets.
+    set_core_file_size_limit_to_zero();
+}
+
 #[cfg(windows)]
 pub(crate) fn pre_main_hardening_windows() {
     // TODO(mbolin): Perform the appropriate configuration for Windows.
diff --git a/codex-rs/rmcp-client/Cargo.toml b/codex-rs/rmcp-client/Cargo.toml
index e9f832e6..996d7ecf 100644
--- a/codex-rs/rmcp-client/Cargo.toml
+++ b/codex-rs/rmcp-client/Cargo.toml
@@ -16,12 +16,7 @@ codex-keyring-store = { workspace = true }
 codex-protocol = { workspace = true }
 dirs = { workspace = true }
 futures = { workspace = true, default-features = false, features = ["std"] }
-keyring = { workspace = true, features = [
-    "apple-native",
-    "crypto-rust",
-    "linux-native-async-persistent",
-    "windows-native",
-] }
+keyring = { workspace = true }
 mcp-types = { path = "../mcp-types" }
 oauth2 = "5"
 reqwest = { version = "0.12", default-features = false, features = [
diff --git a/codex-rs/tui/Cargo.toml b/codex-rs/tui/Cargo.toml
index f087202c..22a13a0c 100644
--- a/codex-rs/tui/Cargo.toml
+++ b/codex-rs/tui/Cargo.toml
@@ -40,6 +40,7 @@ codex-ollama = { workspace = true }
 codex-protocol = { workspace = true }
 codex-app-server-protocol = { workspace = true }
 codex-feedback = { workspace = true }
+codex-litellm-model-session-telemetry = { workspace = true }
 color-eyre = { workspace = true }
 crossterm = { workspace = true, features = ["bracketed-paste", "event-stream"] }
 diffy = { workspace = true }
diff --git a/codex-rs/tui/src/app.rs b/codex-rs/tui/src/app.rs
index 3c06113c..1cdb9df4 100644
--- a/codex-rs/tui/src/app.rs
+++ b/codex-rs/tui/src/app.rs
@@ -14,6 +14,7 @@ use crate::tui;
 use crate::tui::TuiEvent;
 use crate::updates::UpdateAction;
 use codex_ansi_escape::ansi_escape_line;
+use codex_common::model_presets::ModelPreset;
 use codex_core::AuthManager;
 use codex_core::ConversationManager;
 use codex_core::config::Config;
@@ -78,6 +79,7 @@ pub(crate) struct App {
     pub(crate) feedback: codex_feedback::CodexFeedback,
     /// Set when the user confirms an update; propagated on exit.
     pub(crate) pending_update_action: Option<UpdateAction>,
+    model_presets: Vec<ModelPreset>,
 }
 
 impl App {
@@ -91,6 +93,9 @@ impl App {
         initial_images: Vec<PathBuf>,
         resume_selection: ResumeSelection,
         feedback: codex_feedback::CodexFeedback,
+        model_presets: Vec<ModelPreset>,
+        auto_open_model_selector: bool,
+        litellm_model_missing: bool,
     ) -> Result<AppExitInfo> {
         use tokio_stream::StreamExt;
         let (app_event_tx, mut app_event_rx) = unbounded_channel();
@@ -103,6 +108,12 @@ impl App {
 
         let enhanced_keys_supported = tui.enhanced_keys_supported();
 
+        let should_auto_open_selector = auto_open_model_selector
+            && matches!(
+                resume_selection,
+                ResumeSelection::StartFresh | ResumeSelection::Exit
+            );
+
         let chat_widget = match resume_selection {
             ResumeSelection::StartFresh | ResumeSelection::Exit => {
                 let init = crate::chatwidget::ChatWidgetInit {
@@ -114,6 +125,9 @@ impl App {
                     enhanced_keys_supported,
                     auth_manager: auth_manager.clone(),
                     feedback: feedback.clone(),
+                    model_presets: model_presets.clone(),
+                    auto_open_model_selector: should_auto_open_selector,
+                    litellm_model_missing,
                 };
                 ChatWidget::new(init, conversation_manager.clone())
             }
@@ -137,6 +151,9 @@ impl App {
                     enhanced_keys_supported,
                     auth_manager: auth_manager.clone(),
                     feedback: feedback.clone(),
+                    model_presets: model_presets.clone(),
+                    auto_open_model_selector: false,
+                    litellm_model_missing,
                 };
                 ChatWidget::new_from_existing(
                     init,
@@ -167,6 +184,7 @@ impl App {
             backtrack: BacktrackState::default(),
             feedback: feedback.clone(),
             pending_update_action: None,
+            model_presets,
         };
 
         #[cfg(not(debug_assertions))]
@@ -257,6 +275,9 @@ impl App {
                     enhanced_keys_supported: self.enhanced_keys_supported,
                     auth_manager: self.auth_manager.clone(),
                     feedback: self.feedback.clone(),
+                    model_presets: self.model_presets.clone(),
+                    auto_open_model_selector: false,
+                    litellm_model_missing: false,
                 };
                 self.chat_widget = ChatWidget::new(init, self.server.clone());
                 tui.frame_requester().schedule_frame();
@@ -380,6 +401,14 @@ impl App {
                     .await
                 {
                     Ok(()) => {
+                        let profile_label = profile.unwrap_or("default");
+                        tracing::info!(
+                            target: "codex_litellm_debug::model_selection",
+                            model = %model,
+                            effort = ?effort,
+                            profile = profile_label,
+                            "model_selection.persisted"
+                        );
                         let effort_label = effort
                             .map(|eff| format!(" with {eff} reasoning"))
                             .unwrap_or_else(|| " with default reasoning".to_string());
@@ -399,7 +428,11 @@ impl App {
                     }
                     Err(err) => {
                         tracing::error!(
+                            target: "codex_litellm_debug::model_selection",
                             error = %err,
+                            model = %model,
+                            effort = ?effort,
+                            profile = %profile.unwrap_or("default"),
                             "failed to persist model selection"
                         );
                         if let Some(profile) = profile {
@@ -591,6 +624,7 @@ mod tests {
             backtrack: BacktrackState::default(),
             feedback: codex_feedback::CodexFeedback::new(),
             pending_update_action: None,
+            model_presets: Vec::new(),
         }
     }
 
diff --git a/codex-rs/tui/src/app_backtrack.rs b/codex-rs/tui/src/app_backtrack.rs
index e2ded3d3..ee7218c9 100644
--- a/codex-rs/tui/src/app_backtrack.rs
+++ b/codex-rs/tui/src/app_backtrack.rs
@@ -347,6 +347,9 @@ impl App {
             enhanced_keys_supported: self.enhanced_keys_supported,
             auth_manager: self.auth_manager.clone(),
             feedback: self.feedback.clone(),
+            model_presets: Vec::new(),
+            auto_open_model_selector: false,
+            litellm_model_missing: false,
         };
         self.chat_widget =
             crate::chatwidget::ChatWidget::new_from_existing(init, conv, session_configured);
diff --git a/codex-rs/tui/src/bottom_pane/list_selection_view.rs b/codex-rs/tui/src/bottom_pane/list_selection_view.rs
index 44d7b264..a5988a8e 100644
--- a/codex-rs/tui/src/bottom_pane/list_selection_view.rs
+++ b/codex-rs/tui/src/bottom_pane/list_selection_view.rs
@@ -364,13 +364,32 @@ impl Renderable for ListSelectionView {
         let rows = self.build_rows();
         let rows_height =
             measure_rows_height(&rows, &self.state, MAX_POPUP_ROWS, content_area.width);
+
+        let inset_area = content_area.inset(Insets::vh(1, 2));
+        let mut remaining = inset_area.height;
+
+        let header_len = header_height.min(remaining);
+        remaining = remaining.saturating_sub(header_len);
+
+        let spacer_len = remaining.min(1);
+        remaining = remaining.saturating_sub(spacer_len);
+
+        let search_len = if self.is_searchable {
+            remaining.min(1)
+        } else {
+            0
+        };
+        remaining = remaining.saturating_sub(search_len);
+
+        let rows_len = rows_height.min(remaining);
+
         let [header_area, _, search_area, list_area] = Layout::vertical([
-            Constraint::Max(header_height),
-            Constraint::Max(1),
-            Constraint::Length(if self.is_searchable { 1 } else { 0 }),
-            Constraint::Length(rows_height),
+            Constraint::Length(header_len),
+            Constraint::Length(spacer_len),
+            Constraint::Length(search_len),
+            Constraint::Fill(1),
         ])
-        .areas(content_area.inset(Insets::vh(1, 2)));
+        .areas(inset_area);
 
         if header_area.height < header_height {
             let [header_area, elision_area] =
@@ -385,31 +404,35 @@ impl Renderable for ListSelectionView {
         }
 
         if self.is_searchable {
-            Line::from(self.search_query.clone()).render(search_area, buf);
-            let query_span: Span<'static> = if self.search_query.is_empty() {
-                self.search_placeholder
-                    .as_ref()
-                    .map(|placeholder| placeholder.clone().dim())
-                    .unwrap_or_else(|| "".into())
-            } else {
-                self.search_query.clone().into()
-            };
-            Line::from(query_span).render(search_area, buf);
+            if search_area.height > 0 {
+                Line::from(self.search_query.clone()).render(search_area, buf);
+                let query_span: Span<'static> = if self.search_query.is_empty() {
+                    self.search_placeholder
+                        .as_ref()
+                        .map(|placeholder| placeholder.clone().dim())
+                        .unwrap_or_else(|| "".into())
+                } else {
+                    self.search_query.clone().into()
+                };
+                Line::from(query_span).render(search_area, buf);
+            }
         }
 
-        if list_area.height > 0 {
-            let list_area = Rect {
-                x: list_area.x - 2,
+        let usable_rows = rows_len.min(list_area.height);
+
+        if usable_rows > 0 {
+            let render_area = Rect {
+                x: list_area.x,
                 y: list_area.y,
-                width: list_area.width + 2,
-                height: list_area.height,
+                width: list_area.width,
+                height: usable_rows,
             };
             render_rows(
-                list_area,
+                render_area,
                 buf,
                 &rows,
                 &self.state,
-                list_area.height as usize,
+                usable_rows as usize,
                 "no matches",
             );
         }
diff --git a/codex-rs/tui/src/bottom_pane/mod.rs b/codex-rs/tui/src/bottom_pane/mod.rs
index 38a5ecb6..346db101 100644
--- a/codex-rs/tui/src/bottom_pane/mod.rs
+++ b/codex-rs/tui/src/bottom_pane/mod.rs
@@ -407,6 +407,11 @@ impl BottomPane {
         self.request_redraw();
     }
 
+    #[cfg(test)]
+    pub(crate) fn context_window_percent_for_tests(&self) -> Option<i64> {
+        self.context_window_percent
+    }
+
     /// Show a generic list selection view with the provided items.
     pub(crate) fn show_selection_view(&mut self, params: list_selection_view::SelectionViewParams) {
         let view = list_selection_view::ListSelectionView::new(params, self.app_event_tx.clone());
diff --git a/codex-rs/tui/src/bottom_pane/snapshots/codex_tui__bottom_pane__list_selection_view__tests__list_selection_spacing_with_subtitle.snap b/codex-rs/tui/src/bottom_pane/snapshots/codex_tui__bottom_pane__list_selection_view__tests__list_selection_spacing_with_subtitle.snap
index 512f6bbc..5719caa9 100644
--- a/codex-rs/tui/src/bottom_pane/snapshots/codex_tui__bottom_pane__list_selection_view__tests__list_selection_spacing_with_subtitle.snap
+++ b/codex-rs/tui/src/bottom_pane/snapshots/codex_tui__bottom_pane__list_selection_view__tests__list_selection_spacing_with_subtitle.snap
@@ -1,12 +1,13 @@
 ---
 source: tui/src/bottom_pane/list_selection_view.rs
+assertion_line: 528
 expression: render_lines(&view)
 ---
                                                 
   Select Approval Mode                          
   Switch between Codex approval presets         
                                                 
- 1. Read Only (current)  Codex can read files  
-  2. Full Access          Codex can edit files  
+   1. Read Only (current)  Codex can read      
+                            files               
                                                 
   Press enter to confirm or esc to go back
diff --git a/codex-rs/tui/src/bottom_pane/snapshots/codex_tui__bottom_pane__list_selection_view__tests__list_selection_spacing_without_subtitle.snap b/codex-rs/tui/src/bottom_pane/snapshots/codex_tui__bottom_pane__list_selection_view__tests__list_selection_spacing_without_subtitle.snap
index ddd0f90c..3262b9f4 100644
--- a/codex-rs/tui/src/bottom_pane/snapshots/codex_tui__bottom_pane__list_selection_view__tests__list_selection_spacing_without_subtitle.snap
+++ b/codex-rs/tui/src/bottom_pane/snapshots/codex_tui__bottom_pane__list_selection_view__tests__list_selection_spacing_without_subtitle.snap
@@ -1,11 +1,12 @@
 ---
 source: tui/src/bottom_pane/list_selection_view.rs
+assertion_line: 519
 expression: render_lines(&view)
 ---
                                                 
   Select Approval Mode                          
                                                 
- 1. Read Only (current)  Codex can read files  
-  2. Full Access          Codex can edit files  
+   1. Read Only (current)  Codex can read      
+                            files               
                                                 
   Press enter to confirm or esc to go back
diff --git a/codex-rs/tui/src/chatwidget.rs b/codex-rs/tui/src/chatwidget.rs
index 3dfa1b36..53522379 100644
--- a/codex-rs/tui/src/chatwidget.rs
+++ b/codex-rs/tui/src/chatwidget.rs
@@ -102,22 +102,25 @@ use self::interrupts::InterruptManager;
 mod agent;
 use self::agent::spawn_agent;
 use self::agent::spawn_agent_from_existing;
-mod session_header;
-use self::session_header::SessionHeader;
 use crate::streaming::controller::StreamController;
 use std::path::Path;
 
+const DISPLAY_PREVIEW_LIMIT: usize = 160;
+
 use chrono::Local;
 use codex_common::approval_presets::ApprovalPreset;
 use codex_common::approval_presets::builtin_approval_presets;
-use codex_common::model_presets::ModelPreset;
-use codex_common::model_presets::builtin_model_presets;
+use codex_common::model_presets::{
+    ModelPreset, builtin_model_presets, fetch_litellm_model_presets, is_litellm_provider_id,
+};
 use codex_core::AuthManager;
 use codex_core::ConversationManager;
+use codex_core::config::read_litellm_provider_state;
 use codex_core::protocol::AskForApproval;
 use codex_core::protocol::SandboxPolicy;
 use codex_core::protocol_config_types::ReasoningEffort as ReasoningEffortConfig;
 use codex_file_search::FileMatch;
+use codex_litellm_model_session_telemetry as session_telemetry;
 use codex_protocol::plan_tool::UpdatePlanArgs;
 use strum::IntoEnumIterator;
 
@@ -227,6 +230,9 @@ pub(crate) struct ChatWidgetInit {
     pub(crate) enhanced_keys_supported: bool,
     pub(crate) auth_manager: Arc<AuthManager>,
     pub(crate) feedback: codex_feedback::CodexFeedback,
+    pub(crate) model_presets: Vec<ModelPreset>,
+    pub(crate) auto_open_model_selector: bool,
+    pub(crate) litellm_model_missing: bool,
 }
 
 pub(crate) struct ChatWidget {
@@ -236,7 +242,6 @@ pub(crate) struct ChatWidget {
     active_cell: Option<Box<dyn HistoryCell>>,
     config: Config,
     auth_manager: Arc<AuthManager>,
-    session_header: SessionHeader,
     initial_user_message: Option<UserMessage>,
     token_info: Option<TokenUsageInfo>,
     rate_limit_snapshot: Option<RateLimitSnapshotDisplay>,
@@ -276,6 +281,12 @@ pub(crate) struct ChatWidget {
     feedback: codex_feedback::CodexFeedback,
     // Current session rollout path (if known)
     current_rollout_path: Option<PathBuf>,
+    model_presets: Vec<ModelPreset>,
+    auto_open_model_selector: bool,
+    litellm_model_missing: bool,
+    litellm_model_missing_notified: bool,
+    resumed_session: bool,
+    resume_overflow_warned: bool,
 }
 
 struct UserMessage {
@@ -301,6 +312,40 @@ fn create_initial_user_message(text: String, image_paths: Vec<PathBuf>) -> Optio
 }
 
 impl ChatWidget {
+    fn display_preview(text: &str) -> String {
+        let mut preview = String::new();
+        let mut chars = text.chars();
+        for _ in 0..DISPLAY_PREVIEW_LIMIT {
+            match chars.next() {
+                Some(ch) => preview.push(ch),
+                None => return preview,
+            }
+        }
+        if chars.next().is_some() {
+            preview.push('');
+        }
+        preview
+    }
+
+    fn log_display_event(&self, element: &str, event: &str, detail: Option<&str>) {
+        if let Some(detail) = detail {
+            tracing::info!(
+                target: "codex_litellm_debug::display",
+                element,
+                event,
+                detail = %Self::display_preview(detail),
+                "display.event"
+            );
+        } else {
+            tracing::info!(
+                target: "codex_litellm_debug::display",
+                element,
+                event,
+                "display.event"
+            );
+        }
+    }
+
     fn flush_answer_stream_with_separator(&mut self) {
         if let Some(mut controller) = self.stream_controller.take()
             && let Some(cell) = controller.finalize()
@@ -310,19 +355,32 @@ impl ChatWidget {
     }
 
     fn set_status_header(&mut self, header: String) {
+        self.log_display_event("status", "header_update", Some(&header));
         self.current_status_header = header.clone();
         self.bottom_pane.update_status_header(header);
     }
 
+    fn mark_litellm_model_missing(&mut self) {
+        if self.litellm_model_missing_notified {
+            return;
+        }
+        self.add_error_message("Select a LiteLLM model (run /model)".to_string());
+        self.litellm_model_missing_notified = true;
+    }
+
     // --- Small event handlers ---
     fn on_session_configured(&mut self, event: codex_core::protocol::SessionConfiguredEvent) {
+        let session_id_str = event.session_id.to_string();
         self.bottom_pane
             .set_history_metadata(event.history_log_id, event.history_entry_count);
         self.conversation_id = Some(event.session_id);
         self.current_rollout_path = Some(event.rollout_path.clone());
         let initial_messages = event.initial_messages.clone();
-        let model_for_header = event.model.clone();
-        self.session_header.set_model(&model_for_header);
+        let resumed = initial_messages.is_some();
+        self.resumed_session = resumed;
+        self.resume_overflow_warned = false;
+        self.config.model = event.model.clone();
+        session_telemetry::clear_session(Some(&session_id_str));
         self.add_to_history(history_cell::new_session_info(
             &self.config,
             event,
@@ -333,6 +391,10 @@ impl ChatWidget {
         }
         // Ask codex-core to enumerate custom prompts for this session.
         self.submit_op(Op::ListCustomPrompts);
+        if self.auto_open_model_selector {
+            self.auto_open_model_selector = false;
+            self.open_model_popup();
+        }
         if let Some(user_message) = self.initial_user_message.take() {
             self.submit_user_message(user_message);
         }
@@ -375,6 +437,7 @@ impl ChatWidget {
     }
 
     fn on_agent_message(&mut self, message: String) {
+        self.log_display_event("history", "agent_message_final", Some(&message));
         // If we have a stream_controller, then the final agent message is redundant and will be a
         // duplicate of what has already been streamed.
         if self.stream_controller.is_none() {
@@ -393,6 +456,7 @@ impl ChatWidget {
         // For reasoning deltas, do not stream to history. Accumulate the
         // current reasoning block and extract the first bold element
         // (between **/**) as the chunk header. Show this header as status.
+        self.log_display_event("status", "reasoning_delta", Some(&delta));
         self.reasoning_buffer.push_str(&delta);
 
         if let Some(header) = extract_first_bold(&self.reasoning_buffer) {
@@ -406,6 +470,15 @@ impl ChatWidget {
 
     fn on_agent_reasoning_final(&mut self) {
         // At the end of a reasoning block, record transcript-only content.
+        if !self.reasoning_buffer.is_empty() {
+            self.log_display_event(
+                "history",
+                "reasoning_complete",
+                Some(&self.reasoning_buffer),
+            );
+        } else {
+            self.log_display_event("history", "reasoning_complete", None);
+        }
         self.full_reasoning_buffer.push_str(&self.reasoning_buffer);
         if !self.full_reasoning_buffer.is_empty() {
             let cell = history_cell::new_reasoning_summary_block(
@@ -429,6 +502,7 @@ impl ChatWidget {
     // Raw reasoning uses the same flow as summarized reasoning
 
     fn on_task_started(&mut self) {
+        self.log_display_event("status", "task_started", None);
         self.bottom_pane.clear_ctrl_c_quit_hint();
         self.bottom_pane.set_task_running(true);
         self.retry_status_header = None;
@@ -450,6 +524,10 @@ impl ChatWidget {
         // If there is a queued user message, send exactly one now to begin the next turn.
         self.maybe_send_next_queued_input();
         // Emit a notification when the turn completes (suppressed if focused).
+        if let Some(message) = last_agent_message.as_ref() {
+            self.log_display_event("history", "task_complete_message", Some(message));
+        }
+        self.log_display_event("status", "task_complete", None);
         self.notify(Notification::AgentTurnComplete {
             response: last_agent_message.unwrap_or_default(),
         });
@@ -460,12 +538,61 @@ impl ChatWidget {
             let context_window = info
                 .model_context_window
                 .or(self.config.model_context_window);
-            let percent = context_window.map(|window| {
-                info.last_token_usage
-                    .percent_of_context_window_remaining(window)
-            });
+            if self.resumed_session && !self.resume_overflow_warned {
+                if let Some(window) = context_window {
+                    let tokens_in_context = info.total_token_usage.tokens_in_context_window();
+                    if tokens_in_context >= window {
+                        self.add_to_history(history_cell::new_warning_event(
+                            "Saved history exceeds the configured context window. Run /compact before continuing or start a new session with /new."
+                                .to_string(),
+                        ));
+                        self.set_status_header(
+                            "History exceeds context  run /compact".to_string(),
+                        );
+                        self.resume_overflow_warned = true;
+                    }
+                }
+            }
+            let last_usage = info.last_token_usage.clone();
+            let total_usage = info.total_token_usage.clone();
+            let percent = context_window
+                .map(|window| total_usage.percent_of_context_window_remaining(window));
+            let prompt_tokens = last_usage.input_tokens + last_usage.cached_input_tokens;
+            let detail_string = if let Some(p) = percent {
+                format!(
+                    "prompt={prompt_tokens}, completion={}, reasoning={}, total={}, context_remaining={}%",
+                    last_usage.output_tokens,
+                    last_usage.reasoning_output_tokens,
+                    last_usage.total_tokens,
+                    p
+                )
+            } else {
+                format!(
+                    "prompt={prompt_tokens}, completion={}, reasoning={}, total={}",
+                    last_usage.output_tokens,
+                    last_usage.reasoning_output_tokens,
+                    last_usage.total_tokens
+                )
+            };
+            self.log_display_event("status", "token_usage_updated", Some(&detail_string));
             self.bottom_pane.set_context_window_percent(percent);
             self.token_info = Some(info);
+
+            let session_id = self.conversation_id.as_ref().map(|id| id.to_string());
+            let reasoning_effort = self
+                .config
+                .model_reasoning_effort
+                .as_ref()
+                .map(ToString::to_string);
+            session_telemetry::record_turn(
+                session_id.as_deref(),
+                self.config.model.as_str(),
+                reasoning_effort.as_deref(),
+                prompt_tokens,
+                last_usage.output_tokens,
+                last_usage.reasoning_output_tokens,
+                last_usage.total_tokens,
+            );
         }
     }
 
@@ -488,10 +615,27 @@ impl ChatWidget {
             );
 
             let display = crate::status::rate_limit_snapshot_display(&snapshot, Local::now());
+            let fmt_percent = |value: Option<f64>| -> String {
+                value
+                    .map(|v| format!("{v:.1}"))
+                    .unwrap_or_else(|| "n/a".to_string())
+            };
+            let detail = format!(
+                "primary={}%, secondary={}%",
+                fmt_percent(snapshot.primary.as_ref().map(|window| window.used_percent)),
+                fmt_percent(
+                    snapshot
+                        .secondary
+                        .as_ref()
+                        .map(|window| window.used_percent)
+                )
+            );
+            self.log_display_event("status", "rate_limit_update", Some(&detail));
             self.rate_limit_snapshot = Some(display);
 
             if !warnings.is_empty() {
                 for warning in warnings {
+                    self.log_display_event("history", "rate_limit_warning", Some(&warning));
                     self.add_to_history(history_cell::new_warning_event(warning));
                 }
                 self.request_redraw();
@@ -511,6 +655,7 @@ impl ChatWidget {
     }
 
     fn on_error(&mut self, message: String) {
+        self.log_display_event("history", "error_event", Some(&message));
         self.finalize_turn();
         self.add_to_history(history_cell::new_error_event(message));
         self.request_redraw();
@@ -524,6 +669,8 @@ impl ChatWidget {
     /// separated by newlines rather than autosubmitting the next one.
     fn on_interrupted_turn(&mut self, reason: TurnAbortReason) {
         // Finalize, log a gentle prompt, and clear running state.
+        let reason_detail = format!("{reason:?}");
+        self.log_display_event("history", "turn_interrupted", Some(&reason_detail));
         self.finalize_turn();
 
         if reason != TurnAbortReason::ReviewEnded {
@@ -558,6 +705,7 @@ impl ChatWidget {
     }
 
     fn on_plan_update(&mut self, update: UpdatePlanArgs) {
+        self.log_display_event("history", "plan_update", None);
         self.add_to_history(history_cell::new_plan_update(update));
     }
 
@@ -758,6 +906,12 @@ impl ChatWidget {
         // Before streaming agent content, flush any active exec cell group.
         self.flush_active_cell();
 
+        if self.stream_controller.is_none() {
+            self.log_display_event("history", "agent_stream_start", Some(&delta));
+        } else {
+            self.log_display_event("history", "agent_stream_delta", Some(&delta));
+        }
+
         if self.stream_controller.is_none() {
             if self.needs_final_message_separator {
                 let elapsed_seconds = self
@@ -979,12 +1133,15 @@ impl ChatWidget {
             enhanced_keys_supported,
             auth_manager,
             feedback,
+            model_presets,
+            auto_open_model_selector,
+            litellm_model_missing,
         } = common;
         let mut rng = rand::rng();
         let placeholder = EXAMPLE_PROMPTS[rng.random_range(0..EXAMPLE_PROMPTS.len())].to_string();
         let codex_op_tx = spawn_agent(config.clone(), app_event_tx.clone(), conversation_manager);
 
-        Self {
+        let mut this = Self {
             app_event_tx: app_event_tx.clone(),
             frame_requester: frame_requester.clone(),
             codex_op_tx,
@@ -999,7 +1156,6 @@ impl ChatWidget {
             active_cell: None,
             config: config.clone(),
             auth_manager,
-            session_header: SessionHeader::new(config.model),
             initial_user_message: create_initial_user_message(
                 initial_prompt.unwrap_or_default(),
                 initial_images,
@@ -1025,7 +1181,19 @@ impl ChatWidget {
             last_rendered_width: std::cell::Cell::new(None),
             feedback,
             current_rollout_path: None,
+            model_presets,
+            auto_open_model_selector,
+            litellm_model_missing,
+            litellm_model_missing_notified: false,
+            resumed_session: false,
+            resume_overflow_warned: false,
+        };
+
+        if this.litellm_model_missing {
+            this.mark_litellm_model_missing();
         }
+
+        this
     }
 
     /// Create a ChatWidget attached to an existing conversation (e.g., a fork).
@@ -1043,6 +1211,9 @@ impl ChatWidget {
             enhanced_keys_supported,
             auth_manager,
             feedback,
+            model_presets,
+            auto_open_model_selector,
+            litellm_model_missing,
         } = common;
         let mut rng = rand::rng();
         let placeholder = EXAMPLE_PROMPTS[rng.random_range(0..EXAMPLE_PROMPTS.len())].to_string();
@@ -1050,7 +1221,7 @@ impl ChatWidget {
         let codex_op_tx =
             spawn_agent_from_existing(conversation, session_configured, app_event_tx.clone());
 
-        Self {
+        let mut this = Self {
             app_event_tx: app_event_tx.clone(),
             frame_requester: frame_requester.clone(),
             codex_op_tx,
@@ -1065,7 +1236,6 @@ impl ChatWidget {
             active_cell: None,
             config: config.clone(),
             auth_manager,
-            session_header: SessionHeader::new(config.model),
             initial_user_message: create_initial_user_message(
                 initial_prompt.unwrap_or_default(),
                 initial_images,
@@ -1091,7 +1261,19 @@ impl ChatWidget {
             last_rendered_width: std::cell::Cell::new(None),
             feedback,
             current_rollout_path: None,
+            model_presets,
+            auto_open_model_selector,
+            litellm_model_missing,
+            litellm_model_missing_notified: false,
+            resumed_session: false,
+            resume_overflow_warned: false,
+        };
+
+        if this.litellm_model_missing {
+            this.mark_litellm_model_missing();
         }
+
+        this
     }
 
     pub fn desired_height(&self, width: u16) -> u16 {
@@ -1369,6 +1551,7 @@ impl ChatWidget {
         }
 
         let mut items: Vec<UserInput> = Vec::new();
+        let image_count = image_paths.len();
 
         // Special-case: "!cmd" executes a local shell command instead of sending to the model.
         if let Some(stripped) = text.strip_prefix('!') {
@@ -1389,9 +1572,19 @@ impl ChatWidget {
         }
 
         if !text.is_empty() {
+            self.log_display_event("history", "user_prompt_submitted", Some(&text));
             items.push(UserInput::Text { text: text.clone() });
         }
 
+        if image_count > 0 {
+            let image_detail = if image_count == 1 {
+                "1 image".to_string()
+            } else {
+                format!("{image_count} images")
+            };
+            self.log_display_event("history", "user_images_attached", Some(&image_detail));
+        }
+
         for path in image_paths {
             items.push(UserInput::LocalImage { path });
         }
@@ -1593,7 +1786,12 @@ impl ChatWidget {
         }
     }
 
-    fn request_exit(&self) {
+    fn request_exit(&mut self) {
+        if let Some(conversation_id) = self.conversation_id.as_ref() {
+            let resume_cell = history_cell::new_resume_hint(conversation_id.to_string());
+            self.add_to_history(resume_cell);
+            self.request_redraw();
+        }
         self.app_event_tx.send(AppEvent::ExitRequest);
     }
 
@@ -1665,11 +1863,14 @@ impl ChatWidget {
         } else {
             (&default_usage, Some(&default_usage))
         };
+        let session_id = self.conversation_id.as_ref().map(|id| id.to_string());
+        let session_snapshot = session_telemetry::snapshot(session_id.as_deref());
         self.add_to_history(crate::status::new_status_output(
             &self.config,
             total_usage,
             context_usage,
             &self.conversation_id,
+            session_snapshot,
             self.rate_limit_snapshot.as_ref(),
             Local::now(),
         ));
@@ -1680,20 +1881,87 @@ impl ChatWidget {
     pub(crate) fn open_model_popup(&mut self) {
         let current_model = self.config.model.clone();
         let auth_mode = self.auth_manager.auth().map(|auth| auth.mode);
-        let presets: Vec<ModelPreset> = builtin_model_presets(auth_mode);
-
-        let mut items: Vec<SelectionItem> = Vec::new();
-        for preset in presets.into_iter() {
-            let description = if preset.description.is_empty() {
-                None
+        let (mut presets, preset_source): (Vec<ModelPreset>, &'static str) =
+            if is_litellm_provider_id(&self.config.model_provider_id) {
+                match read_litellm_provider_state(&self.config.codex_home)
+                    .map_err(|err| err.to_string())
+                    .and_then(|state| {
+                        let base_url = state
+                            .base_url
+                            .ok_or_else(|| "LiteLLM endpoint URL missing in config".to_string())?;
+                        fetch_litellm_model_presets(
+                            &base_url,
+                            state.api_key.as_deref(),
+                            &current_model,
+                        )
+                    }) {
+                    Ok(list) => {
+                        tracing::info!(
+                            target: "codex_litellm_debug::model_selection",
+                            provider = %self.config.model_provider_id,
+                            count = list.len(),
+                            "model_popup.presets_loaded"
+                        );
+                        (list, "litellm")
+                    }
+                    Err(err) => {
+                        tracing::warn!(
+                            target: "codex_litellm_debug::model_selection",
+                            provider = %self.config.model_provider_id,
+                            error = %err,
+                            "model_popup.presets_failed"
+                        );
+                        self.add_error_message(format!(
+                            "Unable to fetch LiteLLM model list ({err}). Falling back to defaults."
+                        ));
+                        (builtin_model_presets(auth_mode), "builtin")
+                    }
+                }
+            } else if self.model_presets.is_empty() {
+                let builtin = builtin_model_presets(auth_mode);
+                tracing::info!(
+                    target: "codex_litellm_debug::model_selection",
+                    provider = %self.config.model_provider_id,
+                    count = builtin.len(),
+                    "model_popup.presets_loaded"
+                );
+                (builtin, "builtin")
             } else {
-                Some(preset.description.to_string())
+                (self.model_presets.clone(), "cached")
             };
+
+        if let Some(default_index) = presets
+            .iter()
+            .position(|preset| preset.model == current_model)
+        {
+            for (idx, preset) in presets.iter_mut().enumerate() {
+                preset.is_default = idx == default_index;
+            }
+        } else if !presets.is_empty() {
+            presets[0].is_default = true;
+        }
+
+        self.model_presets = presets.clone();
+        tracing::info!(
+            target: "codex_litellm_debug::model_selection",
+            provider = %self.config.model_provider_id,
+            current = %current_model,
+            source = preset_source,
+            count = self.model_presets.len(),
+            "model_popup.opened"
+        );
+
+        let mut items: Vec<SelectionItem> = Vec::new();
+        for preset in presets.iter() {
+            let description = preset
+                .description
+                .as_ref()
+                .and_then(|text| (!text.is_empty()).then(|| text.to_string()));
             let is_current = preset.model == current_model;
-            let preset_for_action = preset;
+            let preset_for_action = preset.clone();
             let actions: Vec<SelectionAction> = vec![Box::new(move |tx| {
                 tx.send(AppEvent::OpenReasoningPopup {
-                    model: preset_for_action,
+                    model: preset_for_action.clone(),
                 });
             })];
             items.push(SelectionItem {
@@ -1747,6 +2015,13 @@ impl ChatWidget {
             .flatten()
             .or_else(|| choices.iter().find_map(|choice| choice.stored))
             .or(Some(default_effort));
+        tracing::info!(
+            target: "codex_litellm_debug::model_selection",
+            model = %preset.model,
+            total_options = choices.len(),
+            default = ?default_choice,
+            "reasoning_popup.opened"
+        );
 
         let model_slug = preset.model.to_string();
         let is_current_model = self.config.model == preset.model;
@@ -1803,11 +2078,10 @@ impl ChatWidget {
                     effort: effort_for_action,
                 });
                 tracing::info!(
-                    "Selected model: {}, Selected effort: {}",
-                    model_for_action,
-                    effort_for_action
-                        .map(|e| e.to_string())
-                        .unwrap_or_else(|| "default".to_string())
+                    target: "codex_litellm_debug::model_selection",
+                    model = %model_for_action,
+                    effort = ?effort_for_action,
+                    "model_selection.choice"
                 );
             })];
 
@@ -2059,16 +2333,24 @@ impl ChatWidget {
 
     /// Set the model in the widget's config copy.
     pub(crate) fn set_model(&mut self, model: &str) {
-        self.session_header.set_model(model);
         self.config.model = model.to_string();
+        for preset in self.model_presets.iter_mut() {
+            preset.is_default = preset.model == model;
+        }
+        self.request_redraw();
     }
 
     pub(crate) fn add_info_message(&mut self, message: String, hint: Option<String>) {
+        self.log_display_event("history", "info_message", Some(&message));
+        if let Some(hint) = hint.as_ref() {
+            self.log_display_event("history", "info_hint", Some(hint));
+        }
         self.add_to_history(history_cell::new_info_event(message, hint));
         self.request_redraw();
     }
 
     pub(crate) fn add_error_message(&mut self, message: String) {
+        self.log_display_event("history", "error_message", Some(&message));
         self.add_to_history(history_cell::new_error_event(message));
         self.request_redraw();
     }
@@ -2358,7 +2640,7 @@ impl ChatWidget {
 
 impl WidgetRef for &ChatWidget {
     fn render_ref(&self, area: Rect, buf: &mut Buffer) {
-        let [_, active_cell_area, bottom_pane_area] = self.layout_areas(area);
+        let [_header_area, active_cell_area, bottom_pane_area] = self.layout_areas(area);
         (&self.bottom_pane).render(bottom_pane_area, buf);
         if !active_cell_area.is_empty()
             && let Some(cell) = &self.active_cell
diff --git a/codex-rs/tui/src/chatwidget/session_header.rs b/codex-rs/tui/src/chatwidget/session_header.rs
deleted file mode 100644
index 32e31b66..00000000
--- a/codex-rs/tui/src/chatwidget/session_header.rs
+++ /dev/null
@@ -1,16 +0,0 @@
-pub(crate) struct SessionHeader {
-    model: String,
-}
-
-impl SessionHeader {
-    pub(crate) fn new(model: String) -> Self {
-        Self { model }
-    }
-
-    /// Updates the header's model text.
-    pub(crate) fn set_model(&mut self, model: &str) {
-        if self.model != model {
-            self.model = model.to_string();
-        }
-    }
-}
diff --git a/codex-rs/tui/src/chatwidget/snapshots/codex_tui__chatwidget__tests__approval_modal_exec.snap b/codex-rs/tui/src/chatwidget/snapshots/codex_tui__chatwidget__tests__approval_modal_exec.snap
index d0990fa9..6d873dcd 100644
--- a/codex-rs/tui/src/chatwidget/snapshots/codex_tui__chatwidget__tests__approval_modal_exec.snap
+++ b/codex-rs/tui/src/chatwidget/snapshots/codex_tui__chatwidget__tests__approval_modal_exec.snap
@@ -1,5 +1,6 @@
 ---
 source: tui/src/chatwidget/tests.rs
+assertion_line: 1716
 expression: terminal.backend().vt100().screen().contents()
 ---
   Would you like to run the following command?
@@ -9,8 +10,8 @@ expression: terminal.backend().vt100().screen().contents()
 
   $ echo hello world
 
- 1. Yes, proceed
-  2. Yes, and don't ask again for this command
-  3. No, and tell Codex what to do differently esc
+   1. Yes, proceed
+    2. Yes, and don't ask again for this command
+    3. No, and tell Codex what to do differently esc
 
   Press enter to confirm or esc to cancel
diff --git a/codex-rs/tui/src/chatwidget/snapshots/codex_tui__chatwidget__tests__approval_modal_exec_no_reason.snap b/codex-rs/tui/src/chatwidget/snapshots/codex_tui__chatwidget__tests__approval_modal_exec_no_reason.snap
index 3a557bf6..96f46b98 100644
--- a/codex-rs/tui/src/chatwidget/snapshots/codex_tui__chatwidget__tests__approval_modal_exec_no_reason.snap
+++ b/codex-rs/tui/src/chatwidget/snapshots/codex_tui__chatwidget__tests__approval_modal_exec_no_reason.snap
@@ -1,13 +1,14 @@
 ---
 source: tui/src/chatwidget/tests.rs
+assertion_line: 1749
 expression: terminal.backend().vt100().screen().contents()
 ---
   Would you like to run the following command?
 
   $ echo hello world
 
- 1. Yes, proceed
-  2. Yes, and don't ask again for this command
-  3. No, and tell Codex what to do differently esc
+   1. Yes, proceed
+    2. Yes, and don't ask again for this command
+    3. No, and tell Codex what to do differently esc
 
   Press enter to confirm or esc to cancel
diff --git a/codex-rs/tui/src/chatwidget/snapshots/codex_tui__chatwidget__tests__approval_modal_patch.snap b/codex-rs/tui/src/chatwidget/snapshots/codex_tui__chatwidget__tests__approval_modal_patch.snap
index 96dde8fb..d796fb16 100644
--- a/codex-rs/tui/src/chatwidget/snapshots/codex_tui__chatwidget__tests__approval_modal_patch.snap
+++ b/codex-rs/tui/src/chatwidget/snapshots/codex_tui__chatwidget__tests__approval_modal_patch.snap
@@ -1,5 +1,6 @@
 ---
 source: tui/src/chatwidget/tests.rs
+assertion_line: 1788
 expression: terminal.backend().vt100().screen().contents()
 ---
   Would you like to make the following edits?
@@ -11,7 +12,7 @@ expression: terminal.backend().vt100().screen().contents()
     1 +hello
     2 +world
 
- 1. Yes, proceed
-  2. No, and tell Codex what to do differently esc
+   1. Yes, proceed
+    2. No, and tell Codex what to do differently esc
 
   Press enter to confirm or esc to cancel
diff --git a/codex-rs/tui/src/chatwidget/snapshots/codex_tui__chatwidget__tests__approvals_selection_popup.snap b/codex-rs/tui/src/chatwidget/snapshots/codex_tui__chatwidget__tests__approvals_selection_popup.snap
index 190594b1..76fa30f7 100644
--- a/codex-rs/tui/src/chatwidget/snapshots/codex_tui__chatwidget__tests__approvals_selection_popup.snap
+++ b/codex-rs/tui/src/chatwidget/snapshots/codex_tui__chatwidget__tests__approvals_selection_popup.snap
@@ -1,17 +1,18 @@
 ---
 source: tui/src/chatwidget/tests.rs
+assertion_line: 1347
 expression: popup
 ---
   Select Approval Mode
 
- 1. Read Only (current)  Codex can read files and answer questions. Codex
-                          requires approval to make edits, run commands, or
-                          access network.
-  2. Auto                 Codex can read files, make edits, and run commands
-                          in the workspace. Codex requires approval to work
-                          outside the workspace or access network.
-  3. Full Access          Codex can read files, make edits, and run commands
-                          with network access, without approval. Exercise
-                          caution.
+   1. Read Only (current)  Codex can read files and answer questions. Codex
+                            requires approval to make edits, run commands, or
+                            access network.
+    2. Auto                 Codex can read files, make edits, and run commands
+                            in the workspace. Codex requires approval to work
+                            outside the workspace or access network.
+    3. Full Access          Codex can read files, make edits, and run commands
+                            with network access, without approval. Exercise
+                            caution.
 
   Press enter to confirm or esc to go back
diff --git a/codex-rs/tui/src/chatwidget/snapshots/codex_tui__chatwidget__tests__binary_size_ideal_response.snap b/codex-rs/tui/src/chatwidget/snapshots/codex_tui__chatwidget__tests__binary_size_ideal_response.snap
index 77738439..8f0da610 100644
--- a/codex-rs/tui/src/chatwidget/snapshots/codex_tui__chatwidget__tests__binary_size_ideal_response.snap
+++ b/codex-rs/tui/src/chatwidget/snapshots/codex_tui__chatwidget__tests__binary_size_ideal_response.snap
@@ -1,16 +1,8 @@
 ---
 source: tui/src/chatwidget/tests.rs
+assertion_line: 1738
 expression: "lines[start_idx..].join(\"\\n\")"
 ---
- I need to check the codex-rs repository to explain why the project's binaries
-  are large. The user is likely seeking specifics about the setup: are Rust
-  builds static, what features are enabled, and is debug information included?
-  It could be due to static linking, included OpenSSL, or how panic handling
-  is set up. I should look into the Cargo.toml file to confirm features and
-  profiles without needing to edit any code. Let's get started on this!
-
- Worked for 0s 
-
  Im going to scan the workspace and Cargo manifests to see build profiles and
   dependencies that impact binary size. Then Ill summarize the main causes.
 
@@ -18,14 +10,6 @@ expression: "lines[start_idx..].join(\"\\n\")"
    List ls -la
     Read Cargo.toml
 
- I'm reviewing the workspace's release profile, which has settings like
-  lto=fat, strip=symbols, and codegen-units=1 to reduce binary size. However, it
-  seems that "strip = symbols" only removes symbols, not debug information. In
-  Rust 1.73+, both "symbols" and "debuginfo" stripping options are available. I
-  should check whether using these results in a smaller binary. Ill also skim
-  through crates for heavy dependencies in Cargo.toml, including cli, core,
-  exec, linux-sandbox, tui, login, ollama, and mcp.
-
  Ran for d in ansi-escape apply-patch arg0 cli common core exec execpolicy
    file-search linux-sandbox login mcp-client mcp-server mcp-types ollama
    tui; do echo "--- $d/Cargo.toml"; sed -n '1,200p' $d/Cargo.toml; echo;
@@ -39,79 +23,6 @@ expression: "lines[start_idx..].join(\"\\n\")"
  Explored
    Read Cargo.toml
 
- Im evaluating how binary size is influenced by various factors. There are
-  many binaries, such as codex and codex-exec, which include dependencies by
-  default, pulling in features like Tokio and hyper. For instance, reqwest
-  includes default features like TLS through rustls. If default features remain
-  enabled, it inflates the size with many additional components. I notice
-  that for musl targets, using vendored builds of OpenSSL could significantly
-  increase binary size. This is quite the intricate web of dependencies!
-
-  Analyzing musl and binary size
-
-  Im considering the implications of building for musl with static linking.
-  In this case, OpenSSL might come into play through some crates, but reqwest
-  defaults to using rustls, although openssl-sys vendored could increase
-  binary size. Its interesting to think about whether theyre targeting musl,
-  especially since it seems they might often be building for Mac and Linux with
-  dynamic linking instead.
-
-  I also realize theres a difference between debug and release builds  release
-  should be smaller due to optimizations, but if theyre defaulting to debug
-  builds, that could lead to much larger binaries.
-
-  Assessing crate impacts on size
-
-  I need to be precise when analyzing binary sizes from various crates. Some
-  crates, like ratatui-image, include heavy features such as libjpeg decoders,
-  which can really bulk things up. Then there are the tree-sitter crates that
-  contain large static grammar tables, adding significant size. The starlark
-  crate and tracing-subscriber also contribute to size increases.
-
-  With static linking per binary being standard in Rust, it leads to multiple
-  copies of the same dependencies unless cdylibs are used. That makes the
-  compile times longer, but using LTO helps reduce bloat somewhat through
-  deduplication.
-
-  Evaluating binary size factors
-
-  Having multiple binaries indeed increases the overall size significantly.
-  I'm considering options like upgrading to "opt-level = 'z'" or using
-  "strip=debuginfo" for reducing size. It's interesting that while the profile
-  only sets symbols for stripping, not debuginfo, this could inflate sizes if
-  retained in release builds.
-
-  The distinction between "symbols" and "debuginfo" is critical; "symbols"
-  keeps debug information which may not align with the goal of minimizing
-  size. Id like to verify with the Rust documentation, but my network access
-  is restricted.
-
-  Explaining large binary sizes
-
-  Im analyzing why binaries in this project are so large. The primary reasons
-  include monolithic static linking, where each binary includes the full
-  dependency graph without shared libraries. Heavy dependencies such as reqwest,
-  hyper, and TLS components also contribute significantly to size, along with
-  image decoders and other libraries.
-
-  Using MUSL targets means vendored OpenSSL can drastically increase the
-  size due to static linking. Furthermore, if the release profile keeps debug
-  information due to using "strip = symbols," that can lead to larger binaries
-  as well. Its a mix of several factors, including panic strategies and
-  platform-specific linking behavior.
-
-  Identifying binary size contributors
-
-  Im considering why binaries are still large even without static linking. The
-  tracing-subscriber crate potentially adds weight because it includes regex,
-  though it can sometimes use a lighter version. References from files like
-  "core/Cargo.toml" show dependencies like reqwest and tokio, while the tui uses
-  the image crate.
-
-  To present this clearly, Ill format the final answer with two main sections:
-  "Main Causes" and "Build-Mode Notes." I can also include brief suggestions for
-  reducing size, but I want to stay focused on answering the user's question.
-
  Worked for 0s 
 
  Heres whats driving size in this workspaces binaries.
@@ -151,3 +62,6 @@ expression: "lines[start_idx..].join(\"\\n\")"
   If you want, I can outline targeted trims (e.g., strip = "debuginfo", opt-
   level = "z", panic abort, tighter tokio/reqwest features) and estimate impact
   per binary.
+
+Session paused.
+To continue this session, run codex resume c7df96da-daec-4fe9-aed9-3cd19b7a6192
diff --git a/codex-rs/tui/src/chatwidget/snapshots/codex_tui__chatwidget__tests__exec_approval_modal_exec.snap b/codex-rs/tui/src/chatwidget/snapshots/codex_tui__chatwidget__tests__exec_approval_modal_exec.snap
index f0cf3a2d..18088c97 100644
--- a/codex-rs/tui/src/chatwidget/snapshots/codex_tui__chatwidget__tests__exec_approval_modal_exec.snap
+++ b/codex-rs/tui/src/chatwidget/snapshots/codex_tui__chatwidget__tests__exec_approval_modal_exec.snap
@@ -1,6 +1,6 @@
 ---
 source: tui/src/chatwidget/tests.rs
-assertion_line: 409
+assertion_line: 432
 expression: "format!(\"{buf:?}\")"
 ---
 Buffer {
@@ -15,9 +15,9 @@ Buffer {
         "                                                                                ",
         "  $ echo hello world                                                            ",
         "                                                                                ",
-        " 1. Yes, proceed                                                               ",
-        "  2. Yes, and don't ask again for this command                                  ",
-        "  3. No, and tell Codex what to do differently esc                              ",
+        "   1. Yes, proceed                                                             ",
+        "    2. Yes, and don't ask again for this command                                ",
+        "    3. No, and tell Codex what to do differently esc                            ",
         "                                                                                ",
         "  Press enter to confirm or esc to cancel                                       ",
     ],
@@ -29,10 +29,10 @@ Buffer {
         x: 73, y: 4, fg: Reset, bg: Reset, underline: Reset, modifier: NONE,
         x: 2, y: 5, fg: Reset, bg: Reset, underline: Reset, modifier: ITALIC,
         x: 7, y: 5, fg: Reset, bg: Reset, underline: Reset, modifier: NONE,
-        x: 0, y: 9, fg: Cyan, bg: Reset, underline: Reset, modifier: BOLD,
-        x: 17, y: 9, fg: Reset, bg: Reset, underline: Reset, modifier: NONE,
-        x: 47, y: 11, fg: Reset, bg: Reset, underline: Reset, modifier: DIM,
-        x: 50, y: 11, fg: Reset, bg: Reset, underline: Reset, modifier: NONE,
+        x: 2, y: 9, fg: Cyan, bg: Reset, underline: Reset, modifier: BOLD,
+        x: 19, y: 9, fg: Reset, bg: Reset, underline: Reset, modifier: NONE,
+        x: 49, y: 11, fg: Reset, bg: Reset, underline: Reset, modifier: DIM,
+        x: 52, y: 11, fg: Reset, bg: Reset, underline: Reset, modifier: NONE,
         x: 2, y: 13, fg: Reset, bg: Reset, underline: Reset, modifier: DIM,
     ]
 }
diff --git a/codex-rs/tui/src/chatwidget/snapshots/codex_tui__chatwidget__tests__feedback_selection_popup.snap b/codex-rs/tui/src/chatwidget/snapshots/codex_tui__chatwidget__tests__feedback_selection_popup.snap
index 4a982420..39bfe721 100644
--- a/codex-rs/tui/src/chatwidget/snapshots/codex_tui__chatwidget__tests__feedback_selection_popup.snap
+++ b/codex-rs/tui/src/chatwidget/snapshots/codex_tui__chatwidget__tests__feedback_selection_popup.snap
@@ -1,11 +1,12 @@
 ---
 source: tui/src/chatwidget/tests.rs
+assertion_line: 1425
 expression: popup
 ---
   How was this?
 
- 1. bug          Crash, error message, hang, or broken UI/behavior.
-  2. bad result   Output was off-target, incorrect, incomplete, or unhelpful.
-  3. good result  Helpful, correct, highquality, or delightful result worth
-                  celebrating.
-  4. other        Slowness, feature suggestion, UX feedback, or anything else.
+   1. bug          Crash, error message, hang, or broken UI/behavior.
+    2. bad result   Output was off-target, incorrect, incomplete, or
+                    unhelpful.
+    3. good result  Helpful, correct, highquality, or delightful result worth
+                    celebrating.
diff --git a/codex-rs/tui/src/chatwidget/snapshots/codex_tui__chatwidget__tests__feedback_upload_consent_popup.snap b/codex-rs/tui/src/chatwidget/snapshots/codex_tui__chatwidget__tests__feedback_upload_consent_popup.snap
index cc3d8e37..69d586d4 100644
--- a/codex-rs/tui/src/chatwidget/snapshots/codex_tui__chatwidget__tests__feedback_upload_consent_popup.snap
+++ b/codex-rs/tui/src/chatwidget/snapshots/codex_tui__chatwidget__tests__feedback_upload_consent_popup.snap
@@ -1,5 +1,6 @@
 ---
 source: tui/src/chatwidget/tests.rs
+assertion_line: 1436
 expression: popup
 ---
   Upload logs?
@@ -7,8 +8,8 @@ expression: popup
   The following files will be sent:
      codex-logs.log
 
- 1. Yes  Share the current Codex session logs with the team for
-          troubleshooting.
-  2. No
+   1. Yes  Share the current Codex session logs with the team for
+            troubleshooting.
+    2. No
 
   Press enter to confirm or esc to go back
diff --git a/codex-rs/tui/src/chatwidget/snapshots/codex_tui__chatwidget__tests__full_access_confirmation_popup.snap b/codex-rs/tui/src/chatwidget/snapshots/codex_tui__chatwidget__tests__full_access_confirmation_popup.snap
index 71dac5f5..0f6eed5d 100644
--- a/codex-rs/tui/src/chatwidget/snapshots/codex_tui__chatwidget__tests__full_access_confirmation_popup.snap
+++ b/codex-rs/tui/src/chatwidget/snapshots/codex_tui__chatwidget__tests__full_access_confirmation_popup.snap
@@ -1,5 +1,6 @@
 ---
 source: tui/src/chatwidget/tests.rs
+assertion_line: 1383
 expression: popup
 ---
   Enable full access?
@@ -8,8 +9,8 @@ expression: popup
   enabling full access. This significantly increases the risk of data loss,
   leaks, or unexpected behavior.
 
- 1. Yes, continue anyway      Apply full access for this session
-  2. Yes, and don't ask again  Enable full access and remember this choice
-  3. Cancel                    Go back without enabling full access
+   1. Yes, continue anyway      Apply full access for this session
+    2. Yes, and don't ask again  Enable full access and remember this choice
+    3. Cancel                    Go back without enabling full access
 
   Press enter to confirm or esc to go back
diff --git a/codex-rs/tui/src/chatwidget/snapshots/codex_tui__chatwidget__tests__model_reasoning_selection_popup.snap b/codex-rs/tui/src/chatwidget/snapshots/codex_tui__chatwidget__tests__model_reasoning_selection_popup.snap
index d2ef858a..045ebbdf 100644
--- a/codex-rs/tui/src/chatwidget/snapshots/codex_tui__chatwidget__tests__model_reasoning_selection_popup.snap
+++ b/codex-rs/tui/src/chatwidget/snapshots/codex_tui__chatwidget__tests__model_reasoning_selection_popup.snap
@@ -1,14 +1,15 @@
 ---
 source: tui/src/chatwidget/tests.rs
+assertion_line: 1414
 expression: popup
 ---
   Select Reasoning Level for gpt-5-codex
 
-  1. Low               Fastest responses with limited reasoning
-  2. Medium (default)  Dynamically adjusts reasoning based on the task
- 3. High (current)    Maximizes reasoning depth for complex or ambiguous
-                       problems
-                        High reasoning effort can quickly consume Plus plan
-                       rate limits.
+    1. Low               Fastest responses with limited reasoning
+    2. Medium (default)  Dynamically adjusts reasoning based on the task
+   3. High (current)    Maximizes reasoning depth for complex or ambiguous
+                         problems
+                          High reasoning effort can quickly consume Plus plan
+                         rate limits.
 
   Press enter to confirm or esc to go back
diff --git a/codex-rs/tui/src/chatwidget/snapshots/codex_tui__chatwidget__tests__model_selection_popup.snap b/codex-rs/tui/src/chatwidget/snapshots/codex_tui__chatwidget__tests__model_selection_popup.snap
index d4ca0491..0727ec90 100644
--- a/codex-rs/tui/src/chatwidget/snapshots/codex_tui__chatwidget__tests__model_selection_popup.snap
+++ b/codex-rs/tui/src/chatwidget/snapshots/codex_tui__chatwidget__tests__model_selection_popup.snap
@@ -1,12 +1,13 @@
 ---
 source: tui/src/chatwidget/tests.rs
+assertion_line: 1331
 expression: popup
 ---
   Select Model and Effort
   Switch the model for this and future Codex CLI sessions
 
- 1. gpt-5-codex (current)  Optimized for coding tasks with many tools.
-  2. gpt-5                  Broad world knowledge with strong general
-                            reasoning.
+   1. gpt-5-codex (current)  Optimized for coding tasks with many tools.
+    2. gpt-5                  Broad world knowledge with strong general
+                              reasoning.
 
   Press enter to select reasoning effort, or esc to dismiss.
diff --git a/codex-rs/tui/src/chatwidget/snapshots/codex_tui__chatwidget__tests__status_widget_and_approval_modal.snap b/codex-rs/tui/src/chatwidget/snapshots/codex_tui__chatwidget__tests__status_widget_and_approval_modal.snap
index 086a3a8c..4be372e7 100644
--- a/codex-rs/tui/src/chatwidget/snapshots/codex_tui__chatwidget__tests__status_widget_and_approval_modal.snap
+++ b/codex-rs/tui/src/chatwidget/snapshots/codex_tui__chatwidget__tests__status_widget_and_approval_modal.snap
@@ -1,6 +1,6 @@
 ---
 source: tui/src/chatwidget/tests.rs
-assertion_line: 1548
+assertion_line: 1960
 expression: terminal.backend()
 ---
 "                                                                                "
@@ -12,8 +12,8 @@ expression: terminal.backend()
 "                                                                                "
 "  $ echo 'hello world'                                                          "
 "                                                                                "
-" 1. Yes, proceed                                                               "
-"  2. Yes, and don't ask again for this command                                  "
-"  3. No, and tell Codex what to do differently esc                              "
+"   1. Yes, proceed                                                             "
+"    2. Yes, and don't ask again for this command                                "
+"    3. No, and tell Codex what to do differently esc                            "
 "                                                                                "
 "  Press enter to confirm or esc to cancel                                       "
diff --git a/codex-rs/tui/src/chatwidget/tests.rs b/codex-rs/tui/src/chatwidget/tests.rs
index 6c9b78e3..7693e550 100644
--- a/codex-rs/tui/src/chatwidget/tests.rs
+++ b/codex-rs/tui/src/chatwidget/tests.rs
@@ -5,6 +5,7 @@ use crate::test_backend::VT100Backend;
 use crate::tui::FrameRequester;
 use assert_matches::assert_matches;
 use codex_common::approval_presets::builtin_approval_presets;
+use codex_common::model_presets::builtin_model_presets;
 use codex_core::AuthManager;
 use codex_core::CodexAuth;
 use codex_core::config::Config;
@@ -34,6 +35,8 @@ use codex_core::protocol::ReviewRequest;
 use codex_core::protocol::StreamErrorEvent;
 use codex_core::protocol::TaskCompleteEvent;
 use codex_core::protocol::TaskStartedEvent;
+use codex_core::protocol::TokenUsage;
+use codex_core::protocol::TokenUsageInfo;
 use codex_core::protocol::UndoCompletedEvent;
 use codex_core::protocol::UndoStartedEvent;
 use codex_core::protocol::ViewImageToolCallEvent;
@@ -245,6 +248,9 @@ async fn helpers_are_available_and_do_not_panic() {
         enhanced_keys_supported: false,
         auth_manager,
         feedback: codex_feedback::CodexFeedback::new(),
+        model_presets: builtin_model_presets(None),
+        auto_open_model_selector: false,
+        litellm_model_missing: false,
     };
     let mut w = ChatWidget::new(init, conversation_manager);
     // Basic construction sanity.
@@ -277,7 +283,6 @@ fn make_chatwidget_manual() -> (
         active_cell: None,
         config: cfg.clone(),
         auth_manager,
-        session_header: SessionHeader::new(cfg.model),
         initial_user_message: None,
         token_info: None,
         rate_limit_snapshot: None,
@@ -301,6 +306,12 @@ fn make_chatwidget_manual() -> (
         last_rendered_width: std::cell::Cell::new(None),
         feedback: codex_feedback::CodexFeedback::new(),
         current_rollout_path: None,
+        model_presets: builtin_model_presets(None),
+        auto_open_model_selector: false,
+        litellm_model_missing: false,
+        litellm_model_missing_notified: false,
+        resumed_session: false,
+        resume_overflow_warned: false,
     };
     (widget, rx, op_rx)
 }
@@ -860,6 +871,73 @@ fn slash_quit_requests_exit() {
     assert_matches!(rx.try_recv(), Ok(AppEvent::ExitRequest));
 }
 
+#[test]
+fn slash_quit_inserts_resume_hint_when_session_active() {
+    let (mut chat, mut rx, _op_rx) = make_chatwidget_manual();
+    chat.conversation_id = Some(codex_protocol::ConversationId::new());
+
+    chat.dispatch_command(SlashCommand::Quit);
+
+    match rx.try_recv() {
+        Ok(AppEvent::InsertHistoryCell(cell)) => {
+            let lines = cell.display_lines(80);
+            let rendered = lines_to_single_string(&lines);
+            assert!(
+                rendered.contains("codex resume"),
+                "resume hint should mention resume command: {rendered}"
+            );
+        }
+        other => panic!("expected resume hint event before exit, got {other:?}"),
+    }
+    assert_matches!(rx.try_recv(), Ok(AppEvent::ExitRequest));
+}
+
+#[test]
+fn resume_history_overflow_prompts_compact_once() {
+    let (mut chat, mut rx, _op_rx) = make_chatwidget_manual();
+    chat.conversation_id = Some(ConversationId::new());
+    chat.resumed_session = true;
+
+    let info = TokenUsageInfo {
+        total_token_usage: TokenUsage {
+            total_tokens: 140_000,
+            ..TokenUsage::default()
+        },
+        last_token_usage: TokenUsage {
+            total_tokens: 140_000,
+            ..TokenUsage::default()
+        },
+        model_context_window: Some(130_000),
+    };
+
+    chat.set_token_info(Some(info.clone()));
+
+    let cells = drain_insert_history(&mut rx);
+    assert!(
+        cells
+            .iter()
+            .any(|lines| lines_to_single_string(lines).contains("/compact")),
+        "expected resume warning urging /compact"
+    );
+    assert!(chat.resume_overflow_warned);
+    assert_eq!(
+        chat.bottom_pane.context_window_percent_for_tests(),
+        Some(0),
+        "context indicator should show 0% remaining after overflow warning"
+    );
+
+    chat.set_token_info(Some(info));
+    assert!(
+        rx.try_recv().is_err(),
+        "second overflow update should not emit duplicate warning"
+    );
+    assert_eq!(
+        chat.bottom_pane.context_window_percent_for_tests(),
+        Some(0),
+        "context indicator should stay at 0% after subsequent updates"
+    );
+}
+
 #[test]
 fn slash_exit_requests_exit() {
     let (mut chat, mut rx, _op_rx) = make_chatwidget_manual();
diff --git a/codex-rs/tui/src/cli.rs b/codex-rs/tui/src/cli.rs
index d86040b5..0367bfc7 100644
--- a/codex-rs/tui/src/cli.rs
+++ b/codex-rs/tui/src/cli.rs
@@ -1,3 +1,4 @@
+use clap::ArgAction;
 use clap::Parser;
 use clap::ValueHint;
 use codex_common::ApprovalModeCliArg;
@@ -38,6 +39,14 @@ pub struct Cli {
     #[arg(long = "oss", default_value_t = false)]
     pub oss: bool,
 
+    /// Force-enable telemetry writers for this run (overrides config).
+    #[arg(long = "telemetry", action = ArgAction::SetTrue, conflicts_with = "no_telemetry")]
+    pub telemetry: bool,
+
+    /// Disable all telemetry writers for this run (overrides config).
+    #[arg(long = "no-telemetry", action = ArgAction::SetTrue)]
+    pub no_telemetry: bool,
+
     /// Configuration profile from config.toml to specify default options.
     #[arg(long = "profile", short = 'p')]
     pub config_profile: Option<String>,
diff --git a/codex-rs/tui/src/history_cell.rs b/codex-rs/tui/src/history_cell.rs
index 73d61dc4..f1c70b29 100644
--- a/codex-rs/tui/src/history_cell.rs
+++ b/codex-rs/tui/src/history_cell.rs
@@ -556,14 +556,6 @@ pub(crate) fn new_session_info(
         rollout_path: _,
     } = event;
     if is_first_event {
-        // Header box rendered as history (so it appears at the very top)
-        let header = SessionHeaderHistoryCell::new(
-            model,
-            reasoning_effort,
-            config.cwd.clone(),
-            crate::version::CODEX_CLI_VERSION,
-        );
-
         // Help lines below the header (new copy and list)
         let help_lines: Vec<Line<'static>> = vec![
             "  To get started, describe a task or try one of these commands:"
@@ -597,6 +589,13 @@ pub(crate) fn new_session_info(
             ]),
         ];
 
+        let header = SessionHeaderHistoryCell::new(
+            model,
+            reasoning_effort,
+            config.cwd.clone(),
+            codex_common::litellm::decorate_version(crate::version::CODEX_CLI_VERSION),
+        );
+
         CompositeHistoryCell {
             parts: vec![
                 Box::new(header),
@@ -623,7 +622,7 @@ pub(crate) fn new_user_prompt(message: String) -> UserHistoryCell {
 
 #[derive(Debug)]
 struct SessionHeaderHistoryCell {
-    version: &'static str,
+    version: String,
     model: String,
     reasoning_effort: Option<ReasoningEffortConfig>,
     directory: PathBuf,
@@ -634,7 +633,7 @@ impl SessionHeaderHistoryCell {
         model: String,
         reasoning_effort: Option<ReasoningEffortConfig>,
         directory: PathBuf,
-        version: &'static str,
+        version: String,
     ) -> Self {
         Self {
             version,
@@ -1229,6 +1228,19 @@ pub(crate) fn new_info_event(message: String, hint: Option<String>) -> PlainHist
     PlainHistoryCell { lines }
 }
 
+pub(crate) fn new_resume_hint(session_id: String) -> PlainHistoryCell {
+    let resume_cmd = format!("codex resume {session_id}");
+    let lines: Vec<Line<'static>> = vec![
+        vec!["Session paused.".bold().yellow()].into(),
+        vec![
+            "To continue this session, run ".dim(),
+            resume_cmd.clone().cyan(),
+        ]
+        .into(),
+    ];
+    PlainHistoryCell { lines }
+}
+
 pub(crate) fn new_error_event(message: String) -> PlainHistoryCell {
     // Use a hair space (U+200A) to create a subtle, near-invisible separation
     // before the text. VS16 is intentionally omitted to keep spacing tighter
@@ -1797,7 +1809,7 @@ mod tests {
             "gpt-4o".to_string(),
             Some(ReasoningEffortConfig::High),
             std::env::temp_dir(),
-            "test",
+            "test".to_string(),
         );
 
         let lines = render_lines(&cell.display_lines(80));
diff --git a/codex-rs/tui/src/lib.rs b/codex-rs/tui/src/lib.rs
index eb5d3ae9..ba1c9082 100644
--- a/codex-rs/tui/src/lib.rs
+++ b/codex-rs/tui/src/lib.rs
@@ -7,6 +7,7 @@ use additional_dirs::add_dir_warning_message;
 use app::App;
 pub use app::AppExitInfo;
 use codex_app_server_protocol::AuthMode;
+use codex_common::model_presets::{ModelPreset, is_litellm_provider_id};
 use codex_core::AuthManager;
 use codex_core::BUILT_IN_OSS_MODEL_PROVIDER_ID;
 use codex_core::CodexAuth;
@@ -17,6 +18,7 @@ use codex_core::config::Config;
 use codex_core::config::ConfigOverrides;
 use codex_core::find_conversation_path_by_id_str;
 use codex_core::protocol::AskForApproval;
+use codex_litellm_model_session_telemetry as session_telemetry;
 use codex_ollama::DEFAULT_OSS_MODEL;
 use codex_protocol::config_types::SandboxMode;
 use opentelemetry_appender_tracing::layer::OpenTelemetryTracingBridge;
@@ -24,9 +26,11 @@ use std::fs::OpenOptions;
 use std::path::PathBuf;
 use tracing::error;
 use tracing_appender::non_blocking;
+use tracing_appender::non_blocking::{NonBlocking, WorkerGuard};
 use tracing_subscriber::EnvFilter;
-use tracing_subscriber::filter::Targets;
+use tracing_subscriber::fmt;
 use tracing_subscriber::prelude::*;
+use tracing_subscriber::registry::LookupSpan;
 
 mod additional_dirs;
 mod app;
@@ -151,6 +155,13 @@ pub async fn run_main(
         tools_web_search_request: cli.web_search.then_some(true),
         experimental_sandbox_command_assessment: None,
         additional_writable_roots: additional_dirs,
+        telemetry_enabled: if cli.telemetry {
+            Some(true)
+        } else if cli.no_telemetry {
+            Some(false)
+        } else {
+            None
+        },
     };
     let raw_overrides = cli.config_overrides.raw_overrides.clone();
     let overrides_cli = codex_common::CliConfigOverrides { raw_overrides };
@@ -180,48 +191,145 @@ pub async fn run_main(
     }
 
     let active_profile = config.active_profile.clone();
-    let log_dir = codex_core::config::log_dir(&config)?;
-    std::fs::create_dir_all(&log_dir)?;
-    // Open (or create) your log file, appending to it.
-    let mut log_file_opts = OpenOptions::new();
-    log_file_opts.create(true).append(true);
-
-    // Ensure the file is only readable and writable by the current user.
-    // Doing the equivalent to `chmod 600` on Windows is quite a bit more code
-    // and requires the Windows API crates, so we can reconsider that when
-    // Codex CLI is officially supported on Windows.
-    #[cfg(unix)]
-    {
-        use std::os::unix::fs::OpenOptionsExt;
-        log_file_opts.mode(0o600);
-    }
+    let mut tracing_guards: Vec<WorkerGuard> = Vec::new();
+    let (file_writer, debug_writer) = if config.telemetry.enabled {
+        if let Err(err) = std::fs::create_dir_all(&config.telemetry.dir) {
+            #[allow(clippy::print_stderr)]
+            {
+                eprintln!(
+                    "telemetry: failed to create telemetry directory {}: {err}",
+                    config.telemetry.dir.display()
+                );
+            }
+        }
 
-    let log_file = log_file_opts.open(log_dir.join("codex-tui.log"))?;
+        let file_writer = codex_core::config::telemetry_log_config(&config, "tui")
+            .cloned()
+            .and_then(|log_cfg| {
+                if !log_cfg.enabled {
+                    return None;
+                }
+                if let Some(parent) = log_cfg.path.parent() {
+                    if let Err(err) = std::fs::create_dir_all(parent) {
+                        eprintln!(
+                            "telemetry: failed to create TUI log directory {}: {err}",
+                            parent.display()
+                        );
+                        return None;
+                    }
+                }
+                let mut log_file_opts = OpenOptions::new();
+                log_file_opts.create(true).append(true);
+                #[cfg(unix)]
+                {
+                    use std::os::unix::fs::OpenOptionsExt;
+                    log_file_opts.mode(0o600);
+                }
+                match log_file_opts.open(&log_cfg.path) {
+                    Ok(log_file) => {
+                        let (writer, guard) = non_blocking(log_file);
+                        tracing_guards.push(guard);
+                        eprintln!(
+                            "telemetry: writing Codex TUI log to {}",
+                            log_cfg.path.display()
+                        );
+                        Some(writer)
+                    }
+                    Err(err) => {
+                        eprintln!(
+                            "telemetry: failed to open TUI log {}: {err}",
+                            log_cfg.path.display()
+                        );
+                        None
+                    }
+                }
+            });
 
-    // Wrap file in nonblocking writer.
-    let (non_blocking, _guard) = non_blocking(log_file);
+        let debug_writer = codex_core::config::telemetry_log_config(&config, "debug")
+            .cloned()
+            .and_then(|log_cfg| {
+                if !log_cfg.enabled {
+                    return None;
+                }
+                if let Some(parent) = log_cfg.path.parent() {
+                    if let Err(err) = std::fs::create_dir_all(parent) {
+                        eprintln!(
+                            "telemetry: failed to create debug log directory {}: {err}",
+                            parent.display()
+                        );
+                        return None;
+                    }
+                }
+                let mut log_file_opts = OpenOptions::new();
+                log_file_opts.create(true).append(true);
+                #[cfg(unix)]
+                {
+                    use std::os::unix::fs::OpenOptionsExt;
+                    log_file_opts.mode(0o600);
+                }
+                match log_file_opts.open(&log_cfg.path) {
+                    Ok(log_file) => {
+                        let (writer, guard) = non_blocking(log_file);
+                        tracing_guards.push(guard);
+                        eprintln!(
+                            "telemetry: recording debug events to {}",
+                            log_cfg.path.display()
+                        );
+                        Some(writer)
+                    }
+                    Err(err) => {
+                        eprintln!(
+                            "telemetry: failed to open debug log {}: {err}",
+                            log_cfg.path.display()
+                        );
+                        None
+                    }
+                }
+            });
 
-    // use RUST_LOG env var, default to info for codex crates.
-    let env_filter = || {
-        EnvFilter::try_from_default_env().unwrap_or_else(|_| {
-            EnvFilter::new("codex_core=info,codex_tui=info,codex_rmcp_client=info")
-        })
+        if let Some(session_cfg) =
+            codex_core::config::telemetry_log_config(&config, "session").cloned()
+        {
+            if session_cfg.enabled {
+                if let Some(parent) = session_cfg.path.parent() {
+                    if let Err(err) = std::fs::create_dir_all(parent) {
+                        eprintln!(
+                            "telemetry: failed to create session log directory {}: {err}",
+                            parent.display()
+                        );
+                    }
+                }
+                match session_telemetry::configure_log_file(Some(session_cfg.path.clone())) {
+                    Ok(()) => eprintln!(
+                        "telemetry: recording LiteLLM session usage to {}",
+                        session_cfg.path.display()
+                    ),
+                    Err(err) => eprintln!(
+                        "telemetry: failed to open LiteLLM session log {}: {err}",
+                        session_cfg.path.display()
+                    ),
+                }
+            } else if let Err(err) = session_telemetry::configure_log_file(None) {
+                eprintln!("telemetry: failed to disable session telemetry logging: {err}");
+            }
+        } else if let Err(err) = session_telemetry::configure_log_file(None) {
+            eprintln!("telemetry: failed to disable session telemetry logging: {err}");
+        }
+
+        (file_writer, debug_writer)
+    } else {
+        if let Err(err) = session_telemetry::configure_log_file(None) {
+            eprintln!("telemetry: failed to disable session telemetry logging: {err}");
+        }
+        (None, None)
     };
 
-    let file_layer = tracing_subscriber::fmt::layer()
-        .with_writer(non_blocking)
-        .with_target(false)
-        .with_span_events(tracing_subscriber::fmt::format::FmtSpan::CLOSE)
-        .with_filter(env_filter());
+    // use RUST_LOG env var, default to info for codex crates.
+    let env_filter = EnvFilter::try_from_default_env().unwrap_or_else(|_| {
+        EnvFilter::new("codex_core=info,codex_tui=info,codex_rmcp_client=info")
+    });
 
     let feedback = codex_feedback::CodexFeedback::new();
-    let targets = Targets::new().with_default(tracing::Level::TRACE);
-
-    let feedback_layer = tracing_subscriber::fmt::layer()
-        .with_writer(feedback.make_writer())
-        .with_ansi(false)
-        .with_target(false)
-        .with_filter(targets);
 
     if cli.oss {
         codex_ollama::ensure_oss_ready(&config)
@@ -240,22 +348,72 @@ pub async fn run_main(
         }
     };
 
-    if let Some(provider) = otel.as_ref() {
-        let otel_layer = OpenTelemetryTracingBridge::new(&provider.logger).with_filter(
-            tracing_subscriber::filter::filter_fn(codex_core::otel_init::codex_export_filter),
-        );
-
-        let _ = tracing_subscriber::registry()
-            .with(file_layer)
-            .with(feedback_layer)
-            .with(otel_layer)
-            .try_init();
-    } else {
-        let _ = tracing_subscriber::registry()
-            .with(file_layer)
-            .with(feedback_layer)
-            .try_init();
-    };
+    match (file_writer, debug_writer) {
+        (Some(file_writer), Some(debug_writer)) => {
+            let base = tracing_subscriber::registry()
+                .with(env_filter.clone())
+                .with(tui_file_layer_for(file_writer))
+                .with(debug_layer_for(debug_writer))
+                .with(feedback_layer_for(&feedback));
+            if let Some(provider) = otel.as_ref() {
+                let otel_layer = OpenTelemetryTracingBridge::new(&provider.logger).with_filter(
+                    tracing_subscriber::filter::filter_fn(
+                        codex_core::otel_init::codex_export_filter,
+                    ),
+                );
+                let _ = base.with(otel_layer).try_init();
+            } else {
+                let _ = base.try_init();
+            }
+        }
+        (Some(file_writer), None) => {
+            let base = tracing_subscriber::registry()
+                .with(env_filter.clone())
+                .with(tui_file_layer_for(file_writer))
+                .with(feedback_layer_for(&feedback));
+            if let Some(provider) = otel.as_ref() {
+                let otel_layer = OpenTelemetryTracingBridge::new(&provider.logger).with_filter(
+                    tracing_subscriber::filter::filter_fn(
+                        codex_core::otel_init::codex_export_filter,
+                    ),
+                );
+                let _ = base.with(otel_layer).try_init();
+            } else {
+                let _ = base.try_init();
+            }
+        }
+        (None, Some(debug_writer)) => {
+            let base = tracing_subscriber::registry()
+                .with(env_filter.clone())
+                .with(debug_layer_for(debug_writer))
+                .with(feedback_layer_for(&feedback));
+            if let Some(provider) = otel.as_ref() {
+                let otel_layer = OpenTelemetryTracingBridge::new(&provider.logger).with_filter(
+                    tracing_subscriber::filter::filter_fn(
+                        codex_core::otel_init::codex_export_filter,
+                    ),
+                );
+                let _ = base.with(otel_layer).try_init();
+            } else {
+                let _ = base.try_init();
+            }
+        }
+        (None, None) => {
+            let base = tracing_subscriber::registry()
+                .with(env_filter.clone())
+                .with(feedback_layer_for(&feedback));
+            if let Some(provider) = otel.as_ref() {
+                let otel_layer = OpenTelemetryTracingBridge::new(&provider.logger).with_filter(
+                    tracing_subscriber::filter::filter_fn(
+                        codex_core::otel_init::codex_export_filter,
+                    ),
+                );
+                let _ = base.with(otel_layer).try_init();
+            } else {
+                let _ = base.try_init();
+            }
+        }
+    }
 
     run_ratatui_app(
         cli,
@@ -269,6 +427,47 @@ pub async fn run_main(
     .map_err(|err| std::io::Error::other(err.to_string()))
 }
 
+fn feedback_layer_for<S>(
+    feedback: &codex_feedback::CodexFeedback,
+) -> fmt::Layer<
+    S,
+    fmt::format::DefaultFields,
+    fmt::format::Format<fmt::format::Full>,
+    codex_feedback::FeedbackMakeWriter,
+>
+where
+    S: tracing::Subscriber + for<'a> LookupSpan<'a>,
+{
+    fmt::Layer::default()
+        .with_writer(feedback.make_writer())
+        .with_ansi(false)
+        .with_target(false)
+}
+
+fn tui_file_layer_for<S>(
+    writer: NonBlocking,
+) -> fmt::Layer<S, fmt::format::DefaultFields, fmt::format::Format<fmt::format::Full>, NonBlocking>
+where
+    S: tracing::Subscriber + for<'a> LookupSpan<'a>,
+{
+    fmt::Layer::default()
+        .with_writer(writer)
+        .with_target(false)
+        .with_span_events(tracing_subscriber::fmt::format::FmtSpan::CLOSE)
+}
+
+fn debug_layer_for<S>(
+    writer: NonBlocking,
+) -> fmt::Layer<S, fmt::format::DefaultFields, fmt::format::Format<fmt::format::Full>, NonBlocking>
+where
+    S: tracing::Subscriber + for<'a> LookupSpan<'a>,
+{
+    fmt::Layer::default()
+        .with_writer(writer)
+        .with_ansi(false)
+        .with_target(true)
+}
+
 async fn run_ratatui_app(
     cli: Cli,
     initial_config: Config,
@@ -331,6 +530,17 @@ async fn run_ratatui_app(
         should_show_trust_screen,
         should_show_windows_wsl_screen,
     );
+    tracing::info!(
+        target: "codex_litellm_debug::onboarding",
+        provider = %initial_config.model_provider_id,
+        model = %initial_config.model,
+        litellm_setup_required = initial_config.litellm_setup_required,
+        should_show = should_show_onboarding,
+        "onboarding.preflight"
+    );
+
+    let mut litellm_credentials_updated = false;
+    let mut litellm_model_selected = false;
 
     let config = if should_show_onboarding {
         let onboarding_result = run_onboarding_app(
@@ -358,12 +568,17 @@ async fn run_ratatui_app(
                 update_action: None,
             });
         }
+        litellm_credentials_updated = onboarding_result.litellm_credentials_updated;
+        litellm_model_selected = onboarding_result.litellm_model_selected;
         // if the user acknowledged windows or made an explicit decision ato trust the directory, reload the config accordingly
+        let reload_for_litellm = onboarding_result.litellm_credentials_updated
+            || onboarding_result.litellm_model_selected;
         if should_show_windows_wsl_screen
             || onboarding_result
                 .directory_trust_decision
                 .map(|d| d == TrustDirectorySelection::Trust)
                 .unwrap_or(false)
+            || reload_for_litellm
         {
             load_config_or_exit(cli_kv_overrides, overrides).await
         } else {
@@ -372,6 +587,33 @@ async fn run_ratatui_app(
     } else {
         initial_config
     };
+    tracing::info!(
+        target: "codex_litellm_debug::onboarding",
+        provider = %config.model_provider_id,
+        model = %config.model,
+        litellm_setup_required = config.litellm_setup_required,
+        "onboarding.post_config_load"
+    );
+
+    let auth_mode = auth_manager.auth().map(|auth| auth.mode);
+    let mut model_presets: Vec<ModelPreset> = codex_common::model_presets::presets_for_provider(
+        &config.model_provider_id,
+        &config.model,
+        auth_mode,
+    );
+
+    if is_litellm_provider_id(&config.model_provider_id)
+        && config.litellm_setup_required
+        && model_presets.is_empty()
+    {
+        model_presets.push(codex_common::model_presets::make_litellm_model_preset(
+            &config.model,
+            true,
+        ));
+    }
+    let auto_open_model_selector =
+        !litellm_model_selected && (litellm_credentials_updated || config.litellm_setup_required);
+    let litellm_model_missing = false;
 
     // Determine resume behavior: explicit id, then resume last, then picker.
     let resume_selection = if let Some(id_str) = cli.resume_session_id.as_deref() {
@@ -448,6 +690,9 @@ async fn run_ratatui_app(
         images,
         resume_selection,
         feedback,
+        model_presets,
+        auto_open_model_selector,
+        litellm_model_missing,
     )
     .await;
 
@@ -538,12 +783,21 @@ fn should_show_onboarding(
         return true;
     }
 
+    if is_litellm_provider_id(&config.model_provider_id)
+        && (config.litellm_setup_required || config.model.trim().is_empty())
+    {
+        return true;
+    }
+
     should_show_login_screen(login_status, config)
 }
 
 fn should_show_login_screen(login_status: LoginStatus, config: &Config) -> bool {
     // Only show the login screen for providers that actually require OpenAI auth
     // (OpenAI or equivalents). For OSS/other providers, skip login entirely.
+    if config.litellm_setup_required {
+        return true;
+    }
     if !config.model_provider.requires_openai_auth {
         return false;
     }
diff --git a/codex-rs/tui/src/onboarding/auth.rs b/codex-rs/tui/src/onboarding/auth.rs
index 56527ac8..ef5eb570 100644
--- a/codex-rs/tui/src/onboarding/auth.rs
+++ b/codex-rs/tui/src/onboarding/auth.rs
@@ -1,15 +1,18 @@
 #![allow(clippy::unwrap_used)]
 
+use codex_common::litellm::{LITELLM_API_KEY_ENV, LITELLM_BASE_URL_ENV};
 use codex_core::AuthManager;
 use codex_core::auth::AuthCredentialsStoreMode;
 use codex_core::auth::CLIENT_ID;
-use codex_core::auth::login_with_api_key;
-use codex_core::auth::read_openai_api_key_from_env;
+use codex_core::config::{
+    LiteLlmProviderUpdate, read_litellm_provider_state, write_litellm_provider_state,
+};
 use codex_login::ServerOptions;
 use codex_login::ShutdownHandle;
 use codex_login::run_login_server;
 use crossterm::event::KeyCode;
 use crossterm::event::KeyEvent;
+use crossterm::event::KeyEventKind;
 use crossterm::event::KeyModifiers;
 use ratatui::buffer::Buffer;
 use ratatui::layout::Constraint;
@@ -30,6 +33,7 @@ use ratatui::widgets::Wrap;
 
 use codex_app_server_protocol::AuthMode;
 use codex_protocol::config_types::ForcedLoginMethod;
+use std::env;
 use std::sync::RwLock;
 
 use crate::LoginStatus;
@@ -45,19 +49,32 @@ use super::onboarding_screen::StepState;
 #[derive(Clone)]
 pub(crate) enum SignInState {
     PickMode,
+    #[allow(dead_code)]
     ChatGptContinueInBrowser(ContinueInBrowserState),
+    #[allow(dead_code)]
     ChatGptSuccessMessage,
+    #[allow(dead_code)]
     ChatGptSuccess,
     ApiKeyEntry(ApiKeyInputState),
     ApiKeyConfigured,
 }
 
-const API_KEY_DISABLED_MESSAGE: &str = "API key login is disabled.";
+const API_KEY_DISABLED_MESSAGE: &str = "LiteLLM configuration is disabled.";
 
 #[derive(Clone, Default)]
 pub(crate) struct ApiKeyInputState {
-    value: String,
-    prepopulated_from_env: bool,
+    base_url: String,
+    api_key: String,
+    stage: ApiKeyEntryStage,
+    base_url_prepopulated: bool,
+    api_key_prepopulated: bool,
+}
+
+#[derive(Clone, Copy, Default, PartialEq, Eq)]
+enum ApiKeyEntryStage {
+    #[default]
+    BaseUrl,
+    ApiKey,
 }
 
 #[derive(Clone)]
@@ -77,54 +94,26 @@ impl Drop for ContinueInBrowserState {
 
 impl KeyboardHandler for AuthModeWidget {
     fn handle_key_event(&mut self, key_event: KeyEvent) {
+        if key_event.kind != KeyEventKind::Press && key_event.kind != KeyEventKind::Repeat {
+            return;
+        }
         if self.handle_api_key_entry_key_event(&key_event) {
             return;
         }
 
         match key_event.code {
-            KeyCode::Up | KeyCode::Char('k') => {
-                if self.is_chatgpt_login_allowed() {
-                    self.highlighted_mode = AuthMode::ChatGPT;
-                }
-            }
-            KeyCode::Down | KeyCode::Char('j') => {
+            KeyCode::Up | KeyCode::Char('k') | KeyCode::Down | KeyCode::Char('j') => {
                 if self.is_api_login_allowed() {
                     self.highlighted_mode = AuthMode::ApiKey;
                 }
             }
-            KeyCode::Char('1') => {
-                if self.is_chatgpt_login_allowed() {
-                    self.start_chatgpt_login();
-                }
-            }
-            KeyCode::Char('2') => {
+            KeyCode::Char('1') | KeyCode::Char('2') | KeyCode::Enter => {
                 if self.is_api_login_allowed() {
                     self.start_api_key_entry();
                 } else {
                     self.disallow_api_login();
                 }
             }
-            KeyCode::Enter => {
-                let sign_in_state = { (*self.sign_in_state.read().unwrap()).clone() };
-                match sign_in_state {
-                    SignInState::PickMode => match self.highlighted_mode {
-                        AuthMode::ChatGPT if self.is_chatgpt_login_allowed() => {
-                            self.start_chatgpt_login();
-                        }
-                        AuthMode::ApiKey if self.is_api_login_allowed() => {
-                            self.start_api_key_entry();
-                        }
-                        AuthMode::ChatGPT => {}
-                        AuthMode::ApiKey => {
-                            self.disallow_api_login();
-                        }
-                    },
-                    SignInState::ChatGptSuccessMessage => {
-                        *self.sign_in_state.write().unwrap() = SignInState::ChatGptSuccess;
-                    }
-                    _ => {}
-                }
-            }
             KeyCode::Esc => {
                 tracing::info!("Esc pressed");
                 let sign_in_state = { (*self.sign_in_state.read().unwrap()).clone() };
@@ -149,11 +138,13 @@ pub(crate) struct AuthModeWidget {
     pub error: Option<String>,
     pub sign_in_state: Arc<RwLock<SignInState>>,
     pub codex_home: PathBuf,
-    pub cli_auth_credentials_store_mode: AuthCredentialsStoreMode,
     pub login_status: LoginStatus,
+    #[allow(dead_code)]
     pub auth_manager: Arc<AuthManager>,
+    #[allow(dead_code)]
     pub forced_chatgpt_workspace_id: Option<String>,
     pub forced_login_method: Option<ForcedLoginMethod>,
+    pub cli_auth_credentials_store_mode: AuthCredentialsStoreMode,
 }
 
 impl AuthModeWidget {
@@ -161,12 +152,13 @@ impl AuthModeWidget {
         !matches!(self.forced_login_method, Some(ForcedLoginMethod::Chatgpt))
     }
 
+    #[allow(dead_code)]
     fn is_chatgpt_login_allowed(&self) -> bool {
         !matches!(self.forced_login_method, Some(ForcedLoginMethod::Api))
     }
 
     fn disallow_api_login(&mut self) {
-        self.highlighted_mode = AuthMode::ChatGPT;
+        self.highlighted_mode = AuthMode::ApiKey;
         self.error = Some(API_KEY_DISABLED_MESSAGE.to_string());
         *self.sign_in_state.write().unwrap() = SignInState::PickMode;
         self.request_frame.schedule_frame();
@@ -176,11 +168,11 @@ impl AuthModeWidget {
         let mut lines: Vec<Line> = vec![
             Line::from(vec![
                 "  ".into(),
-                "Sign in with ChatGPT to use Codex as part of your paid plan".into(),
+                "codex-litellm connects directly to your LiteLLM backend.".into(),
             ]),
             Line::from(vec![
                 "  ".into(),
-                "or connect an API key for usage-based billing".into(),
+                "Provide the endpoint URL and API key to configure this workspace.".into(),
             ]),
             "".into(),
         ];
@@ -214,39 +206,23 @@ impl AuthModeWidget {
             vec![line1, line2]
         };
 
-        let chatgpt_description = if self.is_chatgpt_login_allowed() {
-            "Usage included with Plus, Pro, and Team plans"
-        } else {
-            "ChatGPT login is disabled"
-        };
-        lines.extend(create_mode_item(
-            0,
-            AuthMode::ChatGPT,
-            "Sign in with ChatGPT",
-            chatgpt_description,
-        ));
-        lines.push("".into());
         if self.is_api_login_allowed() {
             lines.extend(create_mode_item(
-                1,
+                0,
                 AuthMode::ApiKey,
-                "Provide your own API key",
-                "Pay for what you use",
+                "Configure LiteLLM credentials",
+                "Paste the LiteLLM endpoint URL and API key (env: LITELLM_BASE_URL / LITELLM_API_KEY)",
             ));
             lines.push("".into());
         } else {
             lines.push(
-                "  API key login is disabled by this workspace. Sign in with ChatGPT to continue."
+                "  LiteLLM configuration is disabled for this workspace. Ask your admin to enable API key login."
                     .dim()
                     .into(),
             );
             lines.push("".into());
         }
-        lines.push(
-            // AE: Following styles.md, this should probably be Cyan because it's a user input tip.
-            //     But leaving this for a future cleanup.
-            "  Press Enter to continue".dim().into(),
-        );
+        lines.push("  Press Enter to continue".dim().into());
         if let Some(err) = &self.error {
             lines.push("".into());
             lines.push(err.as_str().red().into());
@@ -326,9 +302,9 @@ impl AuthModeWidget {
 
     fn render_api_key_configured(&self, area: Rect, buf: &mut Buffer) {
         let lines = vec![
-            " API key configured".fg(Color::Green).into(),
+            " LiteLLM credentials configured".fg(Color::Green).into(),
             "".into(),
-            "  Codex will use usage-based billing with your API key.".into(),
+            "  codex-litellm will reuse these settings for future sessions.".into(),
         ];
 
         Paragraph::new(lines)
@@ -344,48 +320,83 @@ impl AuthModeWidget {
         ])
         .areas(area);
 
-        let mut intro_lines: Vec<Line> = vec![
-            Line::from(vec![
-                "> ".into(),
-                "Use your own OpenAI API key for usage-based billing".bold(),
-            ]),
-            "".into(),
-            "  Paste or type your API key below. It will be stored locally in auth.json.".into(),
-            "".into(),
-        ];
-        if state.prepopulated_from_env {
-            intro_lines.push("  Detected OPENAI_API_KEY environment variable.".into());
-            intro_lines.push(
-                "  Paste a different key if you prefer to use another account."
-                    .dim()
-                    .into(),
-            );
-            intro_lines.push("".into());
-        }
+        let (title, placeholder, value, intro_lines) = match state.stage {
+            ApiKeyEntryStage::BaseUrl => {
+                let mut intro_lines: Vec<Line> = vec![
+                    Line::from(vec![
+                        "> ".into(),
+                        "Step 1 of 2: LiteLLM endpoint URL".bold(),
+                    ]),
+                    "".into(),
+                    "  Paste the base URL of your LiteLLM server (for example, https://litellm.example.com/v1)."
+                        .into(),
+                    "".into(),
+                ];
+                if state.base_url_prepopulated {
+                    intro_lines.push(format!("  Prefilled from ${LITELLM_BASE_URL_ENV}.").into());
+                    intro_lines.push("  Press Enter to accept or type to override.".dim().into());
+                    intro_lines.push("".into());
+                }
+                (
+                    "LiteLLM endpoint",
+                    "Paste or type the LiteLLM endpoint URL".to_string(),
+                    state.base_url.clone(),
+                    intro_lines,
+                )
+            }
+            ApiKeyEntryStage::ApiKey => {
+                let mut intro_lines: Vec<Line> = vec![
+                    Line::from(vec!["> ".into(), "Step 2 of 2: LiteLLM API key".bold()]),
+                    "".into(),
+                    "  Paste the API token used by your LiteLLM deployment.".into(),
+                    "  Stored locally in config.toml as a Bearer token."
+                        .dim()
+                        .into(),
+                    "".into(),
+                ];
+                if state.api_key_prepopulated {
+                    intro_lines.push(format!("  Prefilled from ${LITELLM_API_KEY_ENV}.").into());
+                    intro_lines.push("  Press Enter to accept or type to override.".dim().into());
+                    intro_lines.push("".into());
+                }
+                (
+                    "LiteLLM API key",
+                    "Paste or type your LiteLLM API key".to_string(),
+                    state.api_key.clone(),
+                    intro_lines,
+                )
+            }
+        };
         Paragraph::new(intro_lines)
             .wrap(Wrap { trim: false })
             .render(intro_area, buf);
 
-        let content_line: Line = if state.value.is_empty() {
-            vec!["Paste or type your API key".dim()].into()
+        let content_line: Line = if value.trim().is_empty() {
+            vec![placeholder.dim()].into()
         } else {
-            Line::from(state.value.clone())
+            Line::from(value)
         };
         Paragraph::new(content_line)
             .wrap(Wrap { trim: false })
             .block(
                 Block::default()
-                    .title("API key")
+                    .title(title)
                     .borders(Borders::ALL)
                     .border_type(BorderType::Rounded)
                     .border_style(Style::default().fg(Color::Cyan)),
             )
             .render(input_area, buf);
 
-        let mut footer_lines: Vec<Line> = vec![
-            "  Press Enter to save".dim().into(),
-            "  Press Esc to go back".dim().into(),
-        ];
+        let mut footer_lines: Vec<Line> = match state.stage {
+            ApiKeyEntryStage::BaseUrl => vec![
+                "  Press Enter to continue".dim().into(),
+                "  Press Esc to cancel".dim().into(),
+            ],
+            ApiKeyEntryStage::ApiKey => vec![
+                "  Press Enter to save".dim().into(),
+                "  Press Esc to go back".dim().into(),
+            ],
+        };
         if let Some(error) = &self.error {
             footer_lines.push("".into());
             footer_lines.push(error.as_str().red().into());
@@ -396,7 +407,10 @@ impl AuthModeWidget {
     }
 
     fn handle_api_key_entry_key_event(&mut self, key_event: &KeyEvent) -> bool {
-        let mut should_save: Option<String> = None;
+        if key_event.kind != KeyEventKind::Press && key_event.kind != KeyEventKind::Repeat {
+            return false;
+        }
+        let mut should_save: Option<(String, String)> = None;
         let mut should_request_frame = false;
 
         {
@@ -404,25 +418,59 @@ impl AuthModeWidget {
             if let SignInState::ApiKeyEntry(state) = &mut *guard {
                 match key_event.code {
                     KeyCode::Esc => {
-                        *guard = SignInState::PickMode;
+                        match state.stage {
+                            ApiKeyEntryStage::BaseUrl => {
+                                *guard = SignInState::PickMode;
+                            }
+                            ApiKeyEntryStage::ApiKey => {
+                                state.stage = ApiKeyEntryStage::BaseUrl;
+                            }
+                        }
                         self.error = None;
                         should_request_frame = true;
                     }
-                    KeyCode::Enter => {
-                        let trimmed = state.value.trim().to_string();
-                        if trimmed.is_empty() {
-                            self.error = Some("API key cannot be empty".to_string());
-                            should_request_frame = true;
-                        } else {
-                            should_save = Some(trimmed);
+                    KeyCode::Enter => match state.stage {
+                        ApiKeyEntryStage::BaseUrl => {
+                            let trimmed = state.base_url.trim().to_string();
+                            if trimmed.is_empty() {
+                                self.error =
+                                    Some("LiteLLM endpoint URL cannot be empty".to_string());
+                                should_request_frame = true;
+                            } else {
+                                state.base_url = trimmed;
+                                state.stage = ApiKeyEntryStage::ApiKey;
+                                self.error = None;
+                                should_request_frame = true;
+                            }
                         }
-                    }
+                        ApiKeyEntryStage::ApiKey => {
+                            let trimmed = state.api_key.trim().to_string();
+                            if trimmed.is_empty() {
+                                self.error = Some("LiteLLM API key cannot be empty".to_string());
+                                should_request_frame = true;
+                            } else {
+                                should_save = Some((state.base_url.clone(), trimmed));
+                            }
+                        }
+                    },
                     KeyCode::Backspace => {
-                        if state.prepopulated_from_env {
-                            state.value.clear();
-                            state.prepopulated_from_env = false;
-                        } else {
-                            state.value.pop();
+                        match state.stage {
+                            ApiKeyEntryStage::BaseUrl => {
+                                if state.base_url_prepopulated {
+                                    state.base_url.clear();
+                                    state.base_url_prepopulated = false;
+                                } else {
+                                    state.base_url.pop();
+                                }
+                            }
+                            ApiKeyEntryStage::ApiKey => {
+                                if state.api_key_prepopulated {
+                                    state.api_key.clear();
+                                    state.api_key_prepopulated = false;
+                                } else {
+                                    state.api_key.pop();
+                                }
+                            }
                         }
                         self.error = None;
                         should_request_frame = true;
@@ -431,24 +479,34 @@ impl AuthModeWidget {
                         if !key_event.modifiers.contains(KeyModifiers::CONTROL)
                             && !key_event.modifiers.contains(KeyModifiers::ALT) =>
                     {
-                        if state.prepopulated_from_env {
-                            state.value.clear();
-                            state.prepopulated_from_env = false;
+                        match state.stage {
+                            ApiKeyEntryStage::BaseUrl => {
+                                if state.base_url_prepopulated {
+                                    state.base_url.clear();
+                                    state.base_url_prepopulated = false;
+                                }
+                                state.base_url.push(c);
+                            }
+                            ApiKeyEntryStage::ApiKey => {
+                                if state.api_key_prepopulated {
+                                    state.api_key.clear();
+                                    state.api_key_prepopulated = false;
+                                }
+                                state.api_key.push(c);
+                            }
                         }
-                        state.value.push(c);
                         self.error = None;
                         should_request_frame = true;
                     }
                     _ => {}
                 }
-                // handled; let guard drop before potential save
             } else {
                 return false;
             }
         }
 
-        if let Some(api_key) = should_save {
-            self.save_api_key(api_key);
+        if let Some((base_url, api_key)) = should_save {
+            self.save_litellm_credentials(base_url, api_key);
         } else if should_request_frame {
             self.request_frame.schedule_frame();
         }
@@ -463,11 +521,22 @@ impl AuthModeWidget {
 
         let mut guard = self.sign_in_state.write().unwrap();
         if let SignInState::ApiKeyEntry(state) = &mut *guard {
-            if state.prepopulated_from_env {
-                state.value = trimmed.to_string();
-                state.prepopulated_from_env = false;
-            } else {
-                state.value.push_str(trimmed);
+            match state.stage {
+                ApiKeyEntryStage::BaseUrl => {
+                    if state.base_url_prepopulated {
+                        state.base_url.clear();
+                        state.base_url_prepopulated = false;
+                    }
+                    state.base_url.push_str(trimmed);
+                }
+                ApiKeyEntryStage::ApiKey => {
+                    if state.api_key_prepopulated {
+                        state.api_key = trimmed.to_string();
+                        state.api_key_prepopulated = false;
+                    } else {
+                        state.api_key.push_str(trimmed);
+                    }
+                }
             }
             self.error = None;
         } else {
@@ -479,72 +548,100 @@ impl AuthModeWidget {
         true
     }
 
-    fn start_api_key_entry(&mut self) {
+    pub(crate) fn start_api_key_entry(&mut self) {
         if !self.is_api_login_allowed() {
             self.disallow_api_login();
             return;
         }
         self.error = None;
-        let prefill_from_env = read_openai_api_key_from_env();
-        let mut guard = self.sign_in_state.write().unwrap();
-        match &mut *guard {
-            SignInState::ApiKeyEntry(state) => {
-                if state.value.is_empty() {
-                    if let Some(prefill) = prefill_from_env {
-                        state.value = prefill;
-                        state.prepopulated_from_env = true;
-                    } else {
-                        state.prepopulated_from_env = false;
-                    }
-                }
-            }
-            _ => {
-                *guard = SignInState::ApiKeyEntry(ApiKeyInputState {
-                    value: prefill_from_env.clone().unwrap_or_default(),
-                    prepopulated_from_env: prefill_from_env.is_some(),
-                });
+        let base_url_env = read_env_var(LITELLM_BASE_URL_ENV);
+        let api_key_env = read_env_var(LITELLM_API_KEY_ENV);
+
+        let provider_state = match read_litellm_provider_state(&self.codex_home) {
+            Ok(state) => state,
+            Err(err) => {
+                self.error = Some(format!("Failed to read LiteLLM configuration: {err}"));
+                self.request_frame.schedule_frame();
+                return;
             }
-        }
+        };
+
+        let (base_url, base_url_prepopulated) =
+            match provider_state.base_url.filter(|v| !v.trim().is_empty()) {
+                Some(existing) => (existing, false),
+                None => match base_url_env {
+                    Some(env_value) => (env_value, true),
+                    None => (String::new(), false),
+                },
+            };
+
+        let (api_key, api_key_prepopulated) =
+            match provider_state.api_key.filter(|v| !v.trim().is_empty()) {
+                Some(existing) => (strip_bearer(existing), false),
+                None => match api_key_env {
+                    Some(env_value) => (env_value, true),
+                    None => (String::new(), false),
+                },
+            };
+
+        let stage = if base_url.trim().is_empty() {
+            ApiKeyEntryStage::BaseUrl
+        } else {
+            ApiKeyEntryStage::ApiKey
+        };
+
+        let mut guard = self.sign_in_state.write().unwrap();
+        *guard = SignInState::ApiKeyEntry(ApiKeyInputState {
+            base_url,
+            api_key,
+            stage,
+            base_url_prepopulated,
+            api_key_prepopulated,
+        });
         drop(guard);
         self.request_frame.schedule_frame();
     }
 
-    fn save_api_key(&mut self, api_key: String) {
+    fn save_litellm_credentials(&mut self, base_url: String, api_key: String) {
         if !self.is_api_login_allowed() {
             self.disallow_api_login();
             return;
         }
-        match login_with_api_key(
-            &self.codex_home,
-            &api_key,
-            self.cli_auth_credentials_store_mode,
-        ) {
+        let trimmed_url = base_url.trim().to_string();
+        let trimmed_key = api_key.trim().to_string();
+        if trimmed_url.is_empty() {
+            self.error = Some("LiteLLM endpoint URL cannot be empty".to_string());
+            self.request_frame.schedule_frame();
+            return;
+        }
+        if trimmed_key.is_empty() {
+            self.error = Some("LiteLLM API key cannot be empty".to_string());
+            self.request_frame.schedule_frame();
+            return;
+        }
+
+        let update = LiteLlmProviderUpdate {
+            base_url: Some(trimmed_url),
+            api_key: Some(format_bearer(&trimmed_key)),
+        };
+
+        match write_litellm_provider_state(&self.codex_home, update) {
             Ok(()) => {
                 self.error = None;
-                self.login_status = LoginStatus::AuthMode(AuthMode::ApiKey);
+                self.login_status = LoginStatus::NotAuthenticated;
                 self.auth_manager.reload();
                 *self.sign_in_state.write().unwrap() = SignInState::ApiKeyConfigured;
             }
             Err(err) => {
-                self.error = Some(format!("Failed to save API key: {err}"));
-                let mut guard = self.sign_in_state.write().unwrap();
-                if let SignInState::ApiKeyEntry(existing) = &mut *guard {
-                    if existing.value.is_empty() {
-                        existing.value.push_str(&api_key);
-                    }
-                    existing.prepopulated_from_env = false;
-                } else {
-                    *guard = SignInState::ApiKeyEntry(ApiKeyInputState {
-                        value: api_key,
-                        prepopulated_from_env: false,
-                    });
-                }
+                self.error = Some(format!("Failed to write LiteLLM configuration: {err}"));
+                // fall through to reshow the form with existing input
             }
         }
 
         self.request_frame.schedule_frame();
     }
 
+    #[allow(dead_code)]
     fn start_chatgpt_login(&mut self) {
         // If we're already authenticated with ChatGPT, don't start a new login 
         // just proceed to the success message flow.
@@ -587,7 +684,6 @@ impl AuthModeWidget {
                         }
                         _ => {
                             *sign_in_state.write().unwrap() = SignInState::PickMode;
-                            // self.error = Some(e.to_string());
                             request_frame.schedule_frame();
                         }
                     }
@@ -641,14 +737,43 @@ impl WidgetRef for AuthModeWidget {
     }
 }
 
+fn read_env_var(name: &str) -> Option<String> {
+    env::var(name)
+        .ok()
+        .map(|value| value.trim().to_string())
+        .filter(|value| !value.is_empty())
+}
+
+fn strip_bearer(value: String) -> String {
+    let trimmed = value.trim();
+    if let Some(rest) = trimmed.strip_prefix("Bearer") {
+        rest.trim_start().to_string()
+    } else if let Some(rest) = trimmed.strip_prefix("bearer") {
+        rest.trim_start().to_string()
+    } else {
+        trimmed.to_string()
+    }
+}
+
+fn format_bearer(token: &str) -> String {
+    if token.len() >= 6 && token[..6].eq_ignore_ascii_case("bearer") {
+        let rest = token[6..].trim_start();
+        if rest.is_empty() {
+            "Bearer".to_string()
+        } else {
+            format!("Bearer {rest}")
+        }
+    } else {
+        format!("Bearer {token}")
+    }
+}
+
 #[cfg(test)]
 mod tests {
     use super::*;
     use pretty_assertions::assert_eq;
     use tempfile::TempDir;
 
-    use codex_core::auth::AuthCredentialsStoreMode;
-
     fn widget_forced_chatgpt() -> (AuthModeWidget, TempDir) {
         let codex_home = TempDir::new().unwrap();
         let codex_home_path = codex_home.path().to_path_buf();
@@ -658,15 +783,15 @@ mod tests {
             error: None,
             sign_in_state: Arc::new(RwLock::new(SignInState::PickMode)),
             codex_home: codex_home_path.clone(),
-            cli_auth_credentials_store_mode: AuthCredentialsStoreMode::File,
             login_status: LoginStatus::NotAuthenticated,
             auth_manager: AuthManager::shared(
                 codex_home_path,
                 false,
-                AuthCredentialsStoreMode::File,
+                AuthCredentialsStoreMode::default(),
             ),
             forced_chatgpt_workspace_id: None,
             forced_login_method: Some(ForcedLoginMethod::Chatgpt),
+            cli_auth_credentials_store_mode: AuthCredentialsStoreMode::default(),
         };
         (widget, codex_home)
     }
@@ -688,7 +813,8 @@ mod tests {
     fn saving_api_key_is_blocked_when_chatgpt_forced() {
         let (mut widget, _tmp) = widget_forced_chatgpt();
 
-        widget.save_api_key("sk-test".to_string());
+        widget
+            .save_litellm_credentials("https://example.com/v1".to_string(), "sk-test".to_string());
 
         assert_eq!(widget.error.as_deref(), Some(API_KEY_DISABLED_MESSAGE));
         assert!(matches!(
diff --git a/codex-rs/tui/src/onboarding/mod.rs b/codex-rs/tui/src/onboarding/mod.rs
index 6c420dae..23d79a96 100644
--- a/codex-rs/tui/src/onboarding/mod.rs
+++ b/codex-rs/tui/src/onboarding/mod.rs
@@ -1,4 +1,5 @@
 mod auth;
+pub mod model;
 pub mod onboarding_screen;
 mod trust_directory;
 pub use trust_directory::TrustDirectorySelection;
diff --git a/codex-rs/tui/src/onboarding/onboarding_screen.rs b/codex-rs/tui/src/onboarding/onboarding_screen.rs
index 709f158b..f5383af2 100644
--- a/codex-rs/tui/src/onboarding/onboarding_screen.rs
+++ b/codex-rs/tui/src/onboarding/onboarding_screen.rs
@@ -12,11 +12,13 @@ use ratatui::widgets::Clear;
 use ratatui::widgets::WidgetRef;
 
 use codex_app_server_protocol::AuthMode;
+use codex_common::model_presets::is_litellm_provider_id;
 use codex_protocol::config_types::ForcedLoginMethod;
 
 use crate::LoginStatus;
 use crate::onboarding::auth::AuthModeWidget;
 use crate::onboarding::auth::SignInState;
+use crate::onboarding::model::ModelSelectionWidget;
 use crate::onboarding::trust_directory::TrustDirectorySelection;
 use crate::onboarding::trust_directory::TrustDirectoryWidget;
 use crate::onboarding::welcome::WelcomeWidget;
@@ -33,6 +35,7 @@ enum Step {
     Windows(WindowsSetupWidget),
     Welcome(WelcomeWidget),
     Auth(AuthModeWidget),
+    Model(ModelSelectionWidget),
     TrustDirectory(TrustDirectoryWidget),
 }
 
@@ -57,6 +60,8 @@ pub(crate) struct OnboardingScreen {
     steps: Vec<Step>,
     is_done: bool,
     windows_install_selected: bool,
+    litellm_credentials_updated: bool,
+    litellm_model_selected: bool,
 }
 
 pub(crate) struct OnboardingScreenArgs {
@@ -71,6 +76,8 @@ pub(crate) struct OnboardingScreenArgs {
 pub(crate) struct OnboardingResult {
     pub directory_trust_decision: Option<TrustDirectorySelection>,
     pub windows_install_selected: bool,
+    pub litellm_credentials_updated: bool,
+    pub litellm_model_selected: bool,
 }
 
 impl OnboardingScreen {
@@ -86,8 +93,7 @@ impl OnboardingScreen {
         let cwd = config.cwd.clone();
         let forced_chatgpt_workspace_id = config.forced_chatgpt_workspace_id.clone();
         let forced_login_method = config.forced_login_method;
-        let codex_home = config.codex_home;
-        let cli_auth_credentials_store_mode = config.cli_auth_credentials_store_mode;
+        let codex_home = config.codex_home.clone();
         let mut steps: Vec<Step> = Vec::new();
         if show_windows_wsl_screen {
             steps.push(Step::Windows(WindowsSetupWidget::new(codex_home.clone())));
@@ -98,21 +104,29 @@ impl OnboardingScreen {
         )));
         if show_login_screen {
             let highlighted_mode = match forced_login_method {
-                Some(ForcedLoginMethod::Api) => AuthMode::ApiKey,
-                _ => AuthMode::ChatGPT,
+                Some(ForcedLoginMethod::Api) | None => AuthMode::ApiKey,
+                Some(ForcedLoginMethod::Chatgpt) => AuthMode::ChatGPT,
             };
-            steps.push(Step::Auth(AuthModeWidget {
+            let mut auth_widget = AuthModeWidget {
                 request_frame: tui.frame_requester(),
                 highlighted_mode,
                 error: None,
                 sign_in_state: Arc::new(RwLock::new(SignInState::PickMode)),
                 codex_home: codex_home.clone(),
-                cli_auth_credentials_store_mode,
                 login_status,
+                cli_auth_credentials_store_mode: config.cli_auth_credentials_store_mode,
                 auth_manager,
                 forced_chatgpt_workspace_id,
                 forced_login_method,
-            }))
+            };
+            if config.litellm_setup_required {
+                auth_widget.start_api_key_entry();
+            }
+            steps.push(Step::Auth(auth_widget))
+        }
+        if is_litellm_provider_id(&config.model_provider_id) {
+            let model_widget = ModelSelectionWidget::new(tui.frame_requester(), config.clone());
+            steps.push(Step::Model(model_widget));
         }
         let is_git_repo = get_git_repo_root(&cwd).is_some();
         let highlighted = if is_git_repo {
@@ -132,12 +146,30 @@ impl OnboardingScreen {
             }))
         }
         // TODO: add git warning.
-        Self {
+        let is_litellm = is_litellm_provider_id(&config.model_provider_id);
+        let credentials_ready = if is_litellm {
+            !config.litellm_setup_required
+        } else {
+            true
+        };
+
+        let mut screen = Self {
             request_frame: tui.frame_requester(),
             steps,
             is_done: false,
             windows_install_selected: false,
-        }
+            litellm_credentials_updated: credentials_ready,
+            litellm_model_selected: !is_litellm,
+        };
+        screen.refresh_litellm_state();
+        tracing::info!(
+            target: "codex_litellm_debug::onboarding",
+            provider = %config.model_provider_id,
+            credentials_ready = %credentials_ready,
+            model_selected = screen.litellm_model_selected,
+            "onboarding_screen.created"
+        );
+        screen
     }
 
     fn current_steps_mut(&mut self) -> Vec<&mut Step> {
@@ -194,6 +226,72 @@ impl OnboardingScreen {
     pub fn windows_install_selected(&self) -> bool {
         self.windows_install_selected
     }
+
+    pub fn litellm_credentials_updated(&self) -> bool {
+        self.litellm_credentials_updated
+    }
+
+    pub fn litellm_model_selected(&self) -> bool {
+        self.litellm_model_selected
+    }
+
+    fn refresh_litellm_state(&mut self) {
+        #[cfg(debug_assertions)]
+        tracing::debug!(
+            credentials_updated = self.litellm_credentials_updated,
+            model_selected = self.litellm_model_selected,
+            "onboarding.refresh_litellm_state:start"
+        );
+
+        let credentials_ready = {
+            if let Some(Step::Auth(widget)) =
+                self.steps.iter().find(|step| matches!(step, Step::Auth(_)))
+            {
+                widget
+                    .sign_in_state
+                    .read()
+                    .map(|guard| matches!(&*guard, super::auth::SignInState::ApiKeyConfigured))
+                    .unwrap_or(false)
+            } else {
+                false
+            }
+        };
+
+        if credentials_ready && !self.litellm_credentials_updated {
+            self.litellm_credentials_updated = true;
+            #[cfg(debug_assertions)]
+            tracing::debug!("onboarding: LiteLLM credentials captured");
+            tracing::info!(
+                target: "codex_litellm_debug::onboarding",
+                "onboarding.credentials_ready"
+            );
+        }
+
+        if let Some(Step::Model(widget)) = self
+            .steps
+            .iter_mut()
+            .find(|step| matches!(step, Step::Model(_)))
+        {
+            if self.litellm_credentials_updated {
+                #[cfg(debug_assertions)]
+                tracing::debug!("onboarding: triggering model preset load");
+                tracing::info!(
+                    target: "codex_litellm_debug::onboarding",
+                    "onboarding.model_widget.credentials_ready"
+                );
+                widget.credentials_ready();
+            }
+            if widget.selection().is_some() {
+                self.litellm_model_selected = true;
+                #[cfg(debug_assertions)]
+                tracing::debug!("onboarding: LiteLLM model selected");
+                tracing::info!(
+                    target: "codex_litellm_debug::onboarding",
+                    "onboarding.model_selected"
+                );
+            }
+        }
+    }
 }
 
 impl KeyboardHandler for OnboardingScreen {
@@ -240,6 +338,7 @@ impl KeyboardHandler for OnboardingScreen {
             self.is_done = true;
         }
         self.request_frame.schedule_frame();
+        self.refresh_litellm_state();
     }
 
     fn handle_paste(&mut self, pasted: String) {
@@ -251,6 +350,7 @@ impl KeyboardHandler for OnboardingScreen {
             active_step.handle_paste(pasted);
         }
         self.request_frame.schedule_frame();
+        self.refresh_litellm_state();
     }
 }
 
@@ -323,6 +423,7 @@ impl KeyboardHandler for Step {
             Step::Windows(widget) => widget.handle_key_event(key_event),
             Step::Welcome(widget) => widget.handle_key_event(key_event),
             Step::Auth(widget) => widget.handle_key_event(key_event),
+            Step::Model(widget) => widget.handle_key_event(key_event),
             Step::TrustDirectory(widget) => widget.handle_key_event(key_event),
         }
     }
@@ -332,6 +433,7 @@ impl KeyboardHandler for Step {
             Step::Windows(_) => {}
             Step::Welcome(_) => {}
             Step::Auth(widget) => widget.handle_paste(pasted),
+            Step::Model(_) => {}
             Step::TrustDirectory(widget) => widget.handle_paste(pasted),
         }
     }
@@ -343,6 +445,7 @@ impl StepStateProvider for Step {
             Step::Windows(w) => w.get_step_state(),
             Step::Welcome(w) => w.get_step_state(),
             Step::Auth(w) => w.get_step_state(),
+            Step::Model(w) => w.get_step_state(),
             Step::TrustDirectory(w) => w.get_step_state(),
         }
     }
@@ -360,6 +463,9 @@ impl WidgetRef for Step {
             Step::Auth(widget) => {
                 widget.render_ref(area, buf);
             }
+            Step::Model(widget) => {
+                widget.render_ref(area, buf);
+            }
             Step::TrustDirectory(widget) => {
                 widget.render_ref(area, buf);
             }
@@ -434,5 +540,7 @@ pub(crate) async fn run_onboarding_app(
     Ok(OnboardingResult {
         directory_trust_decision: onboarding_screen.directory_trust_decision(),
         windows_install_selected: onboarding_screen.windows_install_selected(),
+        litellm_credentials_updated: onboarding_screen.litellm_credentials_updated(),
+        litellm_model_selected: onboarding_screen.litellm_model_selected(),
     })
 }
diff --git a/codex-rs/tui/src/onboarding/welcome.rs b/codex-rs/tui/src/onboarding/welcome.rs
index 645c86ba..ea819fc3 100644
--- a/codex-rs/tui/src/onboarding/welcome.rs
+++ b/codex-rs/tui/src/onboarding/welcome.rs
@@ -68,9 +68,12 @@ impl WidgetRef for &WelcomeWidget {
         lines.push(Line::from(vec![
             "  ".into(),
             "Welcome to ".into(),
-            "Codex".bold(),
-            ", OpenAI's command-line coding agent".into(),
+            "codex-litellm".bold(),
+            ", the LiteLLM-native build of Codex".into(),
         ]));
+        lines.push(
+            "  We'll walk through connecting to your LiteLLM endpoint on the next screen.".into(),
+        );
 
         Paragraph::new(lines)
             .wrap(Wrap { trim: false })
diff --git a/codex-rs/tui/src/slash_command.rs b/codex-rs/tui/src/slash_command.rs
index 969d279b..6f7f87ab 100644
--- a/codex-rs/tui/src/slash_command.rs
+++ b/codex-rs/tui/src/slash_command.rs
@@ -12,6 +12,7 @@ use strum_macros::IntoStaticStr;
 pub enum SlashCommand {
     // DO NOT ALPHA-SORT! Enum order is presentation order in the popup, so
     // more frequently used commands should be listed first.
+    #[strum(serialize = "model")]
     Model,
     Approvals,
     Review,
diff --git a/codex-rs/tui/src/status/card.rs b/codex-rs/tui/src/status/card.rs
index fdf6629a..a70717f6 100644
--- a/codex-rs/tui/src/status/card.rs
+++ b/codex-rs/tui/src/status/card.rs
@@ -31,8 +31,7 @@ use super::rate_limits::StatusRateLimitRow;
 use super::rate_limits::compose_rate_limit_data;
 use super::rate_limits::format_status_limit_summary;
 use super::rate_limits::render_status_limit_progress_bar;
-use crate::wrapping::RtOptions;
-use crate::wrapping::word_wrap_lines;
+use codex_litellm_model_session_telemetry::SessionTelemetrySnapshot;
 
 #[derive(Debug, Clone)]
 struct StatusContextWindowData {
@@ -60,6 +59,7 @@ struct StatusHistoryCell {
     account: Option<StatusAccountDisplay>,
     session_id: Option<String>,
     token_usage: StatusTokenUsageData,
+    model_session: Option<SessionTelemetrySnapshot>,
     rate_limits: StatusRateLimitData,
 }
 
@@ -68,6 +68,7 @@ pub(crate) fn new_status_output(
     total_usage: &TokenUsage,
     context_usage: Option<&TokenUsage>,
     session_id: &Option<ConversationId>,
+    model_session: Option<SessionTelemetrySnapshot>,
     rate_limits: Option<&RateLimitSnapshotDisplay>,
     now: DateTime<Local>,
 ) -> CompositeHistoryCell {
@@ -77,6 +78,7 @@ pub(crate) fn new_status_output(
         total_usage,
         context_usage,
         session_id,
+        model_session,
         rate_limits,
         now,
     );
@@ -90,6 +92,7 @@ impl StatusHistoryCell {
         total_usage: &TokenUsage,
         context_usage: Option<&TokenUsage>,
         session_id: &Option<ConversationId>,
+        model_session: Option<SessionTelemetrySnapshot>,
         rate_limits: Option<&RateLimitSnapshotDisplay>,
         now: DateTime<Local>,
     ) -> Self {
@@ -134,6 +137,7 @@ impl StatusHistoryCell {
             account,
             session_id,
             token_usage,
+            model_session,
             rate_limits,
         }
     }
@@ -268,7 +272,11 @@ impl HistoryCell for StatusHistoryCell {
             Span::from(format!("{}>_ ", FieldFormatter::INDENT)).dim(),
             Span::from("OpenAI Codex").bold(),
             Span::from(" ").dim(),
-            Span::from(format!("(v{CODEX_CLI_VERSION})")).dim(),
+            Span::from(format!(
+                "(v{})",
+                codex_common::litellm::decorate_version(CODEX_CLI_VERSION)
+            ))
+            .dim(),
         ]));
         lines.push(Line::from(Vec::<Span<'static>>::new()));
 
@@ -306,24 +314,12 @@ impl HistoryCell for StatusHistoryCell {
         if self.token_usage.context_window.is_some() {
             push_label(&mut labels, &mut seen, "Context window");
         }
+        push_label(&mut labels, &mut seen, "LiteLLM usage");
         self.collect_rate_limit_labels(&mut seen, &mut labels);
 
         let formatter = FieldFormatter::from_labels(labels.iter().map(String::as_str));
         let value_width = formatter.value_width(available_inner_width);
 
-        let note_first_line = Line::from(vec![
-            Span::from("Visit ").cyan(),
-            "chatgpt.com/codex/settings/usage".cyan().underlined(),
-            Span::from(" for up-to-date").cyan(),
-        ]);
-        let note_second_line = Line::from(vec![
-            Span::from("information on rate limits and credits").cyan(),
-        ]);
-        let note_lines = word_wrap_lines(
-            [note_first_line, note_second_line],
-            RtOptions::new(available_inner_width),
-        );
-        lines.extend(note_lines);
         lines.push(Line::from(Vec::<Span<'static>>::new()));
 
         let mut model_spans = vec![Span::from(self.model_name.clone())];
@@ -355,6 +351,58 @@ impl HistoryCell for StatusHistoryCell {
             lines.push(formatter.line("Token usage", self.token_usage_spans()));
         }
 
+        if let Some(snapshot) = self.model_session.as_ref() {
+            let summary_spans = if snapshot.total_turns == 0 {
+                vec![Span::from("no LiteLLM turns recorded yet").dim()]
+            } else {
+                let tokens_fmt = format_tokens_compact(snapshot.total_tokens);
+                let turns = snapshot.total_turns;
+                let last_model = snapshot.last_model.as_deref().unwrap_or("-");
+                let effort_suffix = snapshot
+                    .last_reasoning_effort
+                    .as_ref()
+                    .map(|eff| format!("  last effort {eff}"))
+                    .unwrap_or_default();
+                vec![Span::from(format!(
+                    "{tokens_fmt} across {turns} turn{plural} (last model {last_model}{effort_suffix})",
+                    plural = if turns == 1 { "" } else { "s" }
+                ))]
+            };
+            lines.push(formatter.line("LiteLLM usage", summary_spans));
+
+            for model in snapshot.models.iter().take(5) {
+                let tokens_fmt = format_tokens_compact(model.total_tokens);
+                let turns = model.turns;
+                let effort_suffix = model
+                    .last_reasoning_effort
+                    .as_ref()
+                    .map(|eff| format!("  last effort {eff}"))
+                    .unwrap_or_default();
+                let mut spans = Vec::new();
+                spans.push(Span::from(format!(" {}", model.model)));
+                spans.push(Span::from(" ").dim());
+                spans.push(Span::from(tokens_fmt));
+                spans.push(Span::from(" in ").dim());
+                spans.push(Span::from(format!(
+                    "{turns} turn{}",
+                    if turns == 1 { "" } else { "s" }
+                )));
+                if !effort_suffix.is_empty() {
+                    spans.push(Span::from(effort_suffix).dim());
+                }
+                lines.push(formatter.continuation(spans));
+            }
+
+            let remaining = snapshot.models.len().saturating_sub(5);
+            if remaining > 0 {
+                lines.push(formatter.continuation(vec![Span::from(format!(
+                    " {remaining} additional model{}",
+                    if remaining == 1 { "" } else { "s" }
+                ))
+                .dim()]));
+            }
+        }
+
         if let Some(spans) = self.context_window_spans() {
             lines.push(formatter.line("Context window", spans));
         }
diff --git a/codex-rs/tui/src/status/snapshots/codex_tui__status__tests__status_snapshot_includes_monthly_limit.snap b/codex-rs/tui/src/status/snapshots/codex_tui__status__tests__status_snapshot_includes_monthly_limit.snap
index cf45f3f8..4415695a 100644
--- a/codex-rs/tui/src/status/snapshots/codex_tui__status__tests__status_snapshot_includes_monthly_limit.snap
+++ b/codex-rs/tui/src/status/snapshots/codex_tui__status__tests__status_snapshot_includes_monthly_limit.snap
@@ -1,14 +1,13 @@
 ---
 source: tui/src/status/tests.rs
+assertion_line: 179
 expression: sanitized
 ---
 /status
 
 
-  >_ OpenAI Codex (v0.0.0)                                                  
+  >_ OpenAI Codex (v0.53.0+bcf30cef+litbcf30cef)                            
                                                                             
- Visit chatgpt.com/codex/settings/usage for up-to-date                      
- information on rate limits and credits                                     
                                                                             
   Model:            gpt-5-codex (reasoning none, summaries auto)            
   Directory: [[workspace]]                                                  
diff --git a/codex-rs/tui/src/status/snapshots/codex_tui__status__tests__status_snapshot_includes_reasoning_details.snap b/codex-rs/tui/src/status/snapshots/codex_tui__status__tests__status_snapshot_includes_reasoning_details.snap
index e38a1c7c..bb72fe43 100644
--- a/codex-rs/tui/src/status/snapshots/codex_tui__status__tests__status_snapshot_includes_reasoning_details.snap
+++ b/codex-rs/tui/src/status/snapshots/codex_tui__status__tests__status_snapshot_includes_reasoning_details.snap
@@ -1,14 +1,13 @@
 ---
 source: tui/src/status/tests.rs
+assertion_line: 130
 expression: sanitized
 ---
 /status
 
 
-  >_ OpenAI Codex (v0.0.0)                                           
+  >_ OpenAI Codex (v0.53.0+bcf30cef+litbcf30cef)                     
                                                                      
- Visit chatgpt.com/codex/settings/usage for up-to-date               
- information on rate limits and credits                              
                                                                      
   Model:            gpt-5-codex (reasoning high, summaries detailed) 
   Directory: [[workspace]]                                           
diff --git a/codex-rs/tui/src/status/snapshots/codex_tui__status__tests__status_snapshot_shows_empty_limits_message.snap b/codex-rs/tui/src/status/snapshots/codex_tui__status__tests__status_snapshot_shows_empty_limits_message.snap
index 82ed8fc0..56294638 100644
--- a/codex-rs/tui/src/status/snapshots/codex_tui__status__tests__status_snapshot_shows_empty_limits_message.snap
+++ b/codex-rs/tui/src/status/snapshots/codex_tui__status__tests__status_snapshot_shows_empty_limits_message.snap
@@ -1,14 +1,13 @@
 ---
 source: tui/src/status/tests.rs
+assertion_line: 335
 expression: sanitized
 ---
 /status
 
 
-  >_ OpenAI Codex (v0.0.0)                                       
+  >_ OpenAI Codex (v0.53.0+bcf30cef+litbcf30cef)                 
                                                                  
- Visit chatgpt.com/codex/settings/usage for up-to-date           
- information on rate limits and credits                          
                                                                  
   Model:            gpt-5-codex (reasoning none, summaries auto) 
   Directory: [[workspace]]                                       
diff --git a/codex-rs/tui/src/status/snapshots/codex_tui__status__tests__status_snapshot_shows_missing_limits_message.snap b/codex-rs/tui/src/status/snapshots/codex_tui__status__tests__status_snapshot_shows_missing_limits_message.snap
index 82ed8fc0..2b08e53a 100644
--- a/codex-rs/tui/src/status/snapshots/codex_tui__status__tests__status_snapshot_shows_missing_limits_message.snap
+++ b/codex-rs/tui/src/status/snapshots/codex_tui__status__tests__status_snapshot_shows_missing_limits_message.snap
@@ -1,14 +1,13 @@
 ---
 source: tui/src/status/tests.rs
+assertion_line: 291
 expression: sanitized
 ---
 /status
 
 
-  >_ OpenAI Codex (v0.0.0)                                       
+  >_ OpenAI Codex (v0.53.0+bcf30cef+litbcf30cef)                 
                                                                  
- Visit chatgpt.com/codex/settings/usage for up-to-date           
- information on rate limits and credits                          
                                                                  
   Model:            gpt-5-codex (reasoning none, summaries auto) 
   Directory: [[workspace]]                                       
diff --git a/codex-rs/tui/src/status/snapshots/codex_tui__status__tests__status_snapshot_shows_stale_limits_message.snap b/codex-rs/tui/src/status/snapshots/codex_tui__status__tests__status_snapshot_shows_stale_limits_message.snap
index bd187197..14390401 100644
--- a/codex-rs/tui/src/status/snapshots/codex_tui__status__tests__status_snapshot_shows_stale_limits_message.snap
+++ b/codex-rs/tui/src/status/snapshots/codex_tui__status__tests__status_snapshot_shows_stale_limits_message.snap
@@ -1,14 +1,13 @@
 ---
 source: tui/src/status/tests.rs
+assertion_line: 388
 expression: sanitized
 ---
 /status
 
 
-  >_ OpenAI Codex (v0.0.0)                                           
+  >_ OpenAI Codex (v0.53.0+bcf30cef+litbcf30cef)                     
                                                                      
- Visit chatgpt.com/codex/settings/usage for up-to-date               
- information on rate limits and credits                              
                                                                      
   Model:            gpt-5-codex (reasoning none, summaries auto)     
   Directory: [[workspace]]                                           
diff --git a/codex-rs/tui/src/status/snapshots/codex_tui__status__tests__status_snapshot_truncates_in_narrow_terminal.snap b/codex-rs/tui/src/status/snapshots/codex_tui__status__tests__status_snapshot_truncates_in_narrow_terminal.snap
index 64e9271d..b758f188 100644
--- a/codex-rs/tui/src/status/snapshots/codex_tui__status__tests__status_snapshot_truncates_in_narrow_terminal.snap
+++ b/codex-rs/tui/src/status/snapshots/codex_tui__status__tests__status_snapshot_truncates_in_narrow_terminal.snap
@@ -1,15 +1,13 @@
 ---
 source: tui/src/status/tests.rs
+assertion_line: 260
 expression: sanitized
 ---
 /status
 
 
-  >_ OpenAI Codex (v0.0.0)                  
+  >_ OpenAI Codex (v0.53.0+bcf30cef+litbcf3 
                                             
- Visit chatgpt.com/codex/settings/usage for 
- up-to-date                                 
- information on rate limits and credits     
                                             
   Model:            gpt-5-codex (reasoning  
   Directory: [[workspace]]                  
diff --git a/codex-rs/tui/src/status/tests.rs b/codex-rs/tui/src/status/tests.rs
index 4ab4a8ea..5820e8ab 100644
--- a/codex-rs/tui/src/status/tests.rs
+++ b/codex-rs/tui/src/status/tests.rs
@@ -7,6 +7,7 @@ use chrono::Utc;
 use codex_core::config::Config;
 use codex_core::config::ConfigOverrides;
 use codex_core::config::ConfigToml;
+use codex_core::model_family::find_family_for_model;
 use codex_core::protocol::RateLimitSnapshot;
 use codex_core::protocol::RateLimitWindow;
 use codex_core::protocol::SandboxPolicy;
@@ -27,6 +28,24 @@ fn test_config(temp_home: &TempDir) -> Config {
     .expect("load config")
 }
 
+fn configure_openai_gpt5(config: &mut Config) {
+    let model = "gpt-5-codex";
+    config.model = model.to_string();
+    if let Some(family) = find_family_for_model(model) {
+        config.model_family = family;
+    }
+
+    let provider_id = "openai";
+    config.model_provider_id = provider_id.to_string();
+    if let Some(info) = config.model_providers.get(provider_id) {
+        config.model_provider = info.clone();
+    }
+
+    if config.model_context_window.is_none() {
+        config.model_context_window = Some(272_000);
+    }
+}
+
 fn render_lines(lines: &[Line<'static>]) -> Vec<String> {
     lines
         .iter()
@@ -72,8 +91,7 @@ fn reset_at_from(captured_at: &chrono::DateTime<chrono::Local>, seconds: i64) ->
 fn status_snapshot_includes_reasoning_details() {
     let temp_home = TempDir::new().expect("temp home");
     let mut config = test_config(&temp_home);
-    config.model = "gpt-5-codex".to_string();
-    config.model_provider_id = "openai".to_string();
+    configure_openai_gpt5(&mut config);
     config.model_reasoning_effort = Some(ReasoningEffort::High);
     config.model_reasoning_summary = ReasoningSummary::Detailed;
     config.sandbox_policy = SandboxPolicy::WorkspaceWrite {
@@ -116,6 +134,7 @@ fn status_snapshot_includes_reasoning_details() {
         &usage,
         Some(&usage),
         &None,
+        None,
         Some(&rate_display),
         captured_at,
     );
@@ -133,8 +152,7 @@ fn status_snapshot_includes_reasoning_details() {
 fn status_snapshot_includes_monthly_limit() {
     let temp_home = TempDir::new().expect("temp home");
     let mut config = test_config(&temp_home);
-    config.model = "gpt-5-codex".to_string();
-    config.model_provider_id = "openai".to_string();
+    configure_openai_gpt5(&mut config);
     config.cwd = PathBuf::from("/workspace/tests");
 
     let usage = TokenUsage {
@@ -164,6 +182,7 @@ fn status_snapshot_includes_monthly_limit() {
         &usage,
         Some(&usage),
         &None,
+        None,
         Some(&rate_display),
         captured_at,
     );
@@ -181,7 +200,7 @@ fn status_snapshot_includes_monthly_limit() {
 fn status_card_token_usage_excludes_cached_tokens() {
     let temp_home = TempDir::new().expect("temp home");
     let mut config = test_config(&temp_home);
-    config.model = "gpt-5-codex".to_string();
+    configure_openai_gpt5(&mut config);
     config.cwd = PathBuf::from("/workspace/tests");
 
     let usage = TokenUsage {
@@ -197,7 +216,7 @@ fn status_card_token_usage_excludes_cached_tokens() {
         .single()
         .expect("timestamp");
 
-    let composite = new_status_output(&config, &usage, Some(&usage), &None, None, now);
+    let composite = new_status_output(&config, &usage, Some(&usage), &None, None, None, now);
     let rendered = render_lines(&composite.display_lines(120));
 
     assert!(
@@ -210,8 +229,7 @@ fn status_card_token_usage_excludes_cached_tokens() {
 fn status_snapshot_truncates_in_narrow_terminal() {
     let temp_home = TempDir::new().expect("temp home");
     let mut config = test_config(&temp_home);
-    config.model = "gpt-5-codex".to_string();
-    config.model_provider_id = "openai".to_string();
+    configure_openai_gpt5(&mut config);
     config.model_reasoning_effort = Some(ReasoningEffort::High);
     config.model_reasoning_summary = ReasoningSummary::Detailed;
     config.cwd = PathBuf::from("/workspace/tests");
@@ -243,6 +261,7 @@ fn status_snapshot_truncates_in_narrow_terminal() {
         &usage,
         Some(&usage),
         &None,
+        None,
         Some(&rate_display),
         captured_at,
     );
@@ -261,7 +280,7 @@ fn status_snapshot_truncates_in_narrow_terminal() {
 fn status_snapshot_shows_missing_limits_message() {
     let temp_home = TempDir::new().expect("temp home");
     let mut config = test_config(&temp_home);
-    config.model = "gpt-5-codex".to_string();
+    configure_openai_gpt5(&mut config);
     config.cwd = PathBuf::from("/workspace/tests");
 
     let usage = TokenUsage {
@@ -277,7 +296,7 @@ fn status_snapshot_shows_missing_limits_message() {
         .single()
         .expect("timestamp");
 
-    let composite = new_status_output(&config, &usage, Some(&usage), &None, None, now);
+    let composite = new_status_output(&config, &usage, Some(&usage), &None, None, None, now);
     let mut rendered_lines = render_lines(&composite.display_lines(80));
     if cfg!(windows) {
         for line in &mut rendered_lines {
@@ -292,7 +311,7 @@ fn status_snapshot_shows_missing_limits_message() {
 fn status_snapshot_shows_empty_limits_message() {
     let temp_home = TempDir::new().expect("temp home");
     let mut config = test_config(&temp_home);
-    config.model = "gpt-5-codex".to_string();
+    configure_openai_gpt5(&mut config);
     config.cwd = PathBuf::from("/workspace/tests");
 
     let usage = TokenUsage {
@@ -318,6 +337,7 @@ fn status_snapshot_shows_empty_limits_message() {
         &usage,
         Some(&usage),
         &None,
+        None,
         Some(&rate_display),
         captured_at,
     );
@@ -335,7 +355,7 @@ fn status_snapshot_shows_empty_limits_message() {
 fn status_snapshot_shows_stale_limits_message() {
     let temp_home = TempDir::new().expect("temp home");
     let mut config = test_config(&temp_home);
-    config.model = "gpt-5-codex".to_string();
+    configure_openai_gpt5(&mut config);
     config.cwd = PathBuf::from("/workspace/tests");
 
     let usage = TokenUsage {
@@ -370,6 +390,7 @@ fn status_snapshot_shows_stale_limits_message() {
         &usage,
         Some(&usage),
         &None,
+        None,
         Some(&rate_display),
         now,
     );
@@ -409,7 +430,15 @@ fn status_context_window_uses_last_usage() {
         .single()
         .expect("timestamp");
 
-    let composite = new_status_output(&config, &total_usage, Some(&last_usage), &None, None, now);
+    let composite = new_status_output(
+        &config,
+        &total_usage,
+        Some(&last_usage),
+        &None,
+        None,
+        None,
+        now,
+    );
     let rendered_lines = render_lines(&composite.display_lines(80));
     let context_line = rendered_lines
         .into_iter()
